{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99249, 390) (27830, 390) (99249,) (27830,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import scipy.io as spio\n",
    "\n",
    "data = spio.loadmat('dataset.mat')\n",
    "X_train, X_test = data['X'], data['X_test']\n",
    "y_train, y_test = data['y'].T.flatten(), data['Y_test'].T.flatten()\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.30833877\n",
      "Iteration 2, loss = 4.94550564\n",
      "Iteration 3, loss = 4.67135395\n",
      "Iteration 4, loss = 4.48199153\n",
      "Iteration 5, loss = 4.34161513\n",
      "Iteration 6, loss = 4.23079414\n",
      "Iteration 7, loss = 4.14262169\n",
      "Iteration 8, loss = 4.05727388\n",
      "Iteration 9, loss = 3.98346643\n",
      "Iteration 10, loss = 3.89711786\n",
      "Iteration 11, loss = 3.80660727\n",
      "Iteration 12, loss = 3.71571091\n",
      "Iteration 13, loss = 3.64071654\n",
      "Iteration 14, loss = 3.57328298\n",
      "Iteration 15, loss = 3.51376386\n",
      "Iteration 16, loss = 3.46078241\n",
      "Iteration 17, loss = 3.41299290\n",
      "Iteration 18, loss = 3.34970521\n",
      "Iteration 19, loss = 3.28147723\n",
      "Iteration 20, loss = 3.22386904\n",
      "Iteration 21, loss = 3.14600620\n",
      "Iteration 22, loss = 3.08725624\n",
      "Iteration 23, loss = 3.04080643\n",
      "Iteration 24, loss = 2.98962870\n",
      "Iteration 25, loss = 2.95534034\n",
      "Iteration 26, loss = 2.90633164\n",
      "Iteration 27, loss = 2.87482854\n",
      "Iteration 28, loss = 2.83658025\n",
      "Iteration 29, loss = 2.80953229\n",
      "Iteration 30, loss = 2.78934692\n",
      "Iteration 31, loss = 2.74987768\n",
      "Iteration 32, loss = 2.71881984\n",
      "Iteration 33, loss = 2.69815743\n",
      "Iteration 34, loss = 2.67608685\n",
      "Iteration 35, loss = 2.65690448\n",
      "Iteration 36, loss = 2.62611895\n",
      "Iteration 37, loss = 2.61506766\n",
      "Iteration 38, loss = 2.59475707\n",
      "Iteration 39, loss = 2.57019653\n",
      "Iteration 40, loss = 2.55452154\n",
      "Iteration 41, loss = 2.52380972\n",
      "Iteration 42, loss = 2.53509670\n",
      "Iteration 43, loss = 2.50920535\n",
      "Iteration 44, loss = 2.46354636\n",
      "Iteration 45, loss = 2.47618339\n",
      "Iteration 46, loss = 2.46296230\n",
      "Iteration 47, loss = 2.44498204\n",
      "Iteration 48, loss = 2.43267069\n",
      "Iteration 49, loss = 2.41135887\n",
      "Iteration 50, loss = 2.40627209\n",
      "Iteration 51, loss = 2.38738329\n",
      "Iteration 52, loss = 2.38132204\n",
      "Iteration 53, loss = 2.36749091\n",
      "Iteration 54, loss = 2.35379800\n",
      "Iteration 55, loss = 2.34282395\n",
      "Iteration 56, loss = 2.34198859\n",
      "Iteration 57, loss = 2.31840193\n",
      "Iteration 58, loss = 2.30221237\n",
      "Iteration 59, loss = 2.30759276\n",
      "Iteration 60, loss = 2.27294170\n",
      "Iteration 61, loss = 2.30017014\n",
      "Iteration 62, loss = 2.29500102\n",
      "Iteration 63, loss = 2.27000107\n",
      "Iteration 64, loss = 2.27584830\n",
      "Iteration 65, loss = 2.21825974\n",
      "Iteration 66, loss = 2.25483389\n",
      "Iteration 67, loss = 2.22997046\n",
      "Iteration 68, loss = 2.20412009\n",
      "Iteration 69, loss = 2.23952253\n",
      "Iteration 70, loss = 2.18346196\n",
      "Iteration 71, loss = 2.20502615\n",
      "Iteration 72, loss = 2.18023621\n",
      "Iteration 73, loss = 2.19787004\n",
      "Iteration 74, loss = 2.16718407\n",
      "Iteration 75, loss = 2.15951420\n",
      "Iteration 76, loss = 2.17681616\n",
      "Iteration 77, loss = 2.14166236\n",
      "Iteration 78, loss = 2.12521628\n",
      "Iteration 79, loss = 2.15184140\n",
      "Iteration 80, loss = 2.16242905\n",
      "Iteration 81, loss = 2.12715560\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Training set score: 0.404095\n",
      "Test set score: 0.130363\n"
     ]
    }
   ],
   "source": [
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(200,), max_iter=100, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
