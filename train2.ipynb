{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.3.1\n",
      "2.000000 * 3.000000 = 6.000000\n",
      "Loading data...\n",
      "ids {'MJDG0': 111, 'MDPB0': 60, 'MJRP0': 136, 'MEWM0': 78, 'MILB0': 103, 'MDWD0': 67, 'MJAI0': 106, 'MDLR0': 56, 'MJMA0': 125, 'MDLC2': 53, 'MKLR0': 151, 'MMWS0': 184, 'MJJB0': 119, 'MKAG0': 143, 'MARW0': 11, 'MCEW0': 28, 'MDSS0': 64, 'MAEO0': 4, 'MMDM1': 170, 'MKLS1': 152, 'MDSS1': 65, 'MJBG0': 107, 'MMDS0': 171, 'MFMC0': 80, 'MJJJ0': 120, 'MBOM0': 20, 'MPFU0': 191, 'MJAE0': 105, 'MDLC0': 51, 'MJJM0': 121, 'MJKR0': 122, 'MMWS1': 185, 'MFER0': 79, 'MDAS0': 38, 'MFRM0': 81, 'MEDR0': 71, 'MDLH0': 54, 'MGES0': 90, 'MCTH0': 35, 'MJWT0': 140, 'MGAR0': 88, 'MFXS0': 83, 'MHJB0': 98, 'MJWG0': 138, 'MHRM0': 101, 'MCSS0': 34, 'MCDD0': 25, 'MJEB0': 113, 'MDWM0': 69, 'MJDE0': 110, 'MKJO0': 150, 'MPPC0': 196, 'MJXL0': 142, 'MMEB0': 173, 'MDSJ0': 63, 'MDDC0': 43, 'MKAJ0': 145, 'MAJP0': 6, 'MBMA1': 18, 'MDTB0': 66, 'MJPG0': 128, 'MDLM0': 55, 'MDLB0': 50, 'MMAG0': 163, 'MDEF0': 45, 'MPGH0': 192, 'MDBP0': 40, 'MDEM0': 46, 'MCLK0': 30, 'MMBS0': 165, 'MCHL0': 29, 'MDLC1': 52, 'MLJC0': 158, 'MJRH0': 133, 'MKES0': 149, 'MJEB1': 114, 'MJAC0': 104, 'MJPM0': 129, 'MCPM0': 32, 'MDHL0': 47, 'MCAE0': 22, 'MMEA0': 172, 'MPRK0': 199, 'MMCC0': 166, 'MJLS0': 124, 'MEAL0': 70, 'MKRG0': 154, 'MJMM0': 127, 'MBGT0': 15, 'MKLW0': 153, 'MAFM0': 5, 'MDMA0': 57, 'MJRA0': 131, 'MJDC0': 109, 'MCEF0': 27, 'MEGJ0': 73, 'MHMR0': 100, 'MMDB0': 167, 'MJRG0': 132, 'MMDG0': 168, 'MGXP0': 95, 'MBBR0': 12, 'MGAK0': 87, 'MCDC0': 24, 'MMGC0': 174, 'MJXA0': 141, 'MHXL0': 102, 'MBCG0': 13, 'MDCM0': 42, 'MHBS0': 96, 'MGSH0': 93, 'MCDR0': 26, 'MESJ0': 77, 'MARC0': 10, 'MFWK0': 82, 'MMLM0': 178, 'MMDM0': 169, 'MLSH0': 161, 'MGAG0': 86, 'MPGR0': 193, 'MDBB1': 39, 'MGRP0': 92, 'MEJS0': 75, 'MJRK0': 135, 'MGAW0': 89, 'MEFG0': 72, 'MBMA0': 17, 'MPEB0': 190, 'MDPS0': 61, 'MDNS0': 59, 'MGAF0': 85, 'MJEE0': 115, 'MKAM0': 146, 'MDCD0': 41, 'MDED0': 44, 'MLEL0': 157, 'MFXV0': 84, 'MAPV0': 9, 'MJMD0': 126, 'MMXS0': 186, 'MKAH0': 144, 'MBJV0': 16, 'MMJB1': 177, 'MJHI0': 118, 'MCRE0': 33, 'MMVP0': 182, 'MJDA0': 108, 'MPGR1': 194, 'MCTM0': 36, 'MCLM0': 31, 'MDRD0': 62, 'MMAA0': 162, 'MPMB0': 195, 'MHMG0': 99, 'MGSL0': 94, 'MHIT0': 97, 'MMSM0': 181, 'MAEB0': 3, 'MDMT0': 58, 'MJWS0': 139, 'MPAR0': 189, 'MJLB0': 123, 'MPRB0': 197, 'MESG0': 76, 'MKXL0': 155, 'MDJM0': 48, 'MMPM0': 179, 'MJDM0': 112, 'MCAL0': 23, 'MAKR0': 8, 'MJFH0': 116, 'MKDB0': 147, 'MLJH0': 159, 'MGRL0': 91, 'MMWB0': 183, 'MDKS0': 49, 'MAKB0': 7, 'MCXM0': 37, 'MADD0': 2, 'MJFR0': 117, 'MKDD0': 148, 'MADC0': 1, 'MEJL0': 74, 'MPRD0': 198, 'MMRP0': 180, 'MBML0': 19, 'MMGK0': 176, 'MNET0': 187, 'MMGG0': 175, 'MLBC0': 156, 'MMAM0': 164, 'MJRH1': 134, 'MDWH0': 68, 'MBSB0': 21, 'MABC0': 0, 'MNTW0': 188, 'MBEF0': 14, 'MLNS0': 160, 'MJSR0': 137, 'MJPM1': 130}\n",
      "\n",
      "# Train : 66325\n",
      "# Eval  : 22882\n",
      "# Test  : 23212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "# Add whatever you want\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Test tensorflow\n",
    "print('TensorFlow version: ' + tf.__version__)\n",
    "a = tf.constant(2.0)\n",
    "b = tf.constant(3.0)\n",
    "c = a * b\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run([a, b, c])\n",
    "print('%f * %f = %f' % (result[0], result[1], result[2]))\n",
    "sess.close()\n",
    "\n",
    "# load data\n",
    "TRAIN_FOLDER = '../data/normalized/train'\n",
    "TEST_FOLDER = '../data/normalized/test'\n",
    "ID_FILE = '../data/normalized/id.json'\n",
    "NUM_SPEAKERS = 200\n",
    "\n",
    "def get_attributes(fname):\n",
    "    attr = fname.split('.')[0].split('-')\n",
    "    dialect = attr[0]\n",
    "    gender = attr[1][0]\n",
    "    speaker_id = attr[1]\n",
    "    sentence_type = attr[2][:2]\n",
    "    return dialect, gender, speaker_id, sentence_type\n",
    "\n",
    "def load_files(train):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for speaker_id, flist in train.items():\n",
    "        for fname in flist:\n",
    "            filedata = np.load(fname)\n",
    "            for segment in filedata:\n",
    "                for x in segment:\n",
    "                    X.append(x) # dim(x) => (390,)\n",
    "                    Y.append(speaker_id)\n",
    "    return np.array(X).astype(np.float32), np.array(Y)\n",
    "    \n",
    "\n",
    "def load_train_data():\n",
    "    import json\n",
    "    with open(ID_FILE, 'r') as f_json:\n",
    "        ids = json.load(f_json)\n",
    "    print('ids', ids)\n",
    "    \n",
    "    \n",
    "    files = [f for f in os.listdir(TRAIN_FOLDER) if os.path.isfile(os.path.join(TRAIN_FOLDER, f))]\n",
    "\n",
    "    # split train val data\n",
    "    train = {}\n",
    "    val = {}\n",
    "    \n",
    "    added = {}\n",
    "    for file in files:\n",
    "        dialect, gender, speaker_id, sentence_type = get_attributes(file)\n",
    "        file_path = os.path.join(TRAIN_FOLDER, file)\n",
    "        if speaker_id not in ids:\n",
    "            print('ERROR:', speaker_id, 'not found in ids')\n",
    "        speaker_id = ids[speaker_id]\n",
    "        val_set = added.setdefault(speaker_id, {})\n",
    "        if sentence_type not in val_set:\n",
    "            val.setdefault(speaker_id, []).append(file_path)\n",
    "            val_set[sentence_type] = True\n",
    "        else:\n",
    "            train.setdefault(speaker_id, []).append(file_path)\n",
    "    # test data    \n",
    "    test = {}\n",
    "    files = [f for f in os.listdir(TEST_FOLDER) if os.path.isfile(os.path.join(TEST_FOLDER, f))]\n",
    "    for file in files:\n",
    "        dialect, gender, speaker_id, sentence_type = get_attributes(file)\n",
    "        file_path = os.path.join(TEST_FOLDER, file)\n",
    "        if speaker_id not in ids:\n",
    "            print('ERROR:', speaker_id, 'not found in ids')\n",
    "        speaker_id = ids[speaker_id]\n",
    "        test.setdefault(speaker_id, []).append(file_path)\n",
    "    \n",
    "    # load data\n",
    "    # input_path = os.path.join(TRAIN_FOLDER, file)\n",
    "    X_train, Y_train = load_files(train)\n",
    "    X_val, Y_val = load_files(val)\n",
    "    X_test, Y_test = load_files(test)\n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    "    \n",
    "print('Loading data...')\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = load_train_data()\n",
    "\n",
    "print()\n",
    "print('# Train :', len(Y_train))\n",
    "print('# Eval  :', len(Y_val))\n",
    "print('# Test  :', len(Y_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_nodes = 390\n",
    "hidden_nodes = 200\n",
    "output_nodes = NUM_SPEAKERS\n",
    "batch_size = 128\n",
    "beta = 0.0001 # lambda for regularization\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, [None, 390])\n",
    "    labels = tf.placeholder(tf.int64, [None])\n",
    "    tf_train_labels = tf.one_hot(labels, output_nodes)\n",
    "    tf_valid_dataset = tf.constant(X_val)\n",
    "    tf_test_dataset = tf.constant(X_test)\n",
    "\n",
    "    # Variables.\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([input_nodes, hidden_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, output_nodes]))\n",
    "    biases_2 = tf.Variable(tf.zeros([output_nodes]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    sigmoid_layer = tf.nn.sigmoid(logits_1)\n",
    "    logits_2 = tf.matmul(sigmoid_layer, weights_2) + biases_2\n",
    "    # Normal loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_2, labels=tf_train_labels))\n",
    "    # Loss function with L2 Regularization with beta=0.01\n",
    "    regularizers = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
    "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 1e-4\n",
    "\n",
    "    # tf.train.exponential_decay(learning_rate, global_step, decay_steps, \n",
    "    # decay_rate, staircase=False, name=None)\n",
    "    # staircase: Boolean. If True decay the learning rate at discrete intervals\n",
    "\n",
    "    # decay every 500 steps with a base of 0.96\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                                    global_step, 500, 0.96,\n",
    "                                                    staircase=True)\n",
    "\n",
    "    # Passing global_step to minimize() will increment it at each step.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    # Predictions for validation \n",
    "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "    sigmoid_layer = tf.nn.sigmoid(logits_1)\n",
    "    logits_2 = tf.matmul(sigmoid_layer, weights_2) + biases_2\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    # Predictions for test\n",
    "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "    sigmoid_layer = tf.nn.sigmoid(logits_1)\n",
    "    logits_2 = tf.matmul(sigmoid_layer, weights_2) + biases_2\n",
    "    \n",
    "    test_prediction =  tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "===== > train for epoch 0\n",
      "iteration (50): loss = 476.294, accuracy = 0.000\n",
      "iteration (100): loss = 471.762, accuracy = 0.781\n",
      "iteration (150): loss = 466.807, accuracy = 0.781\n",
      "iteration (200): loss = 461.129, accuracy = 1.562\n",
      "iteration (250): loss = 458.204, accuracy = 0.000\n",
      "iteration (300): loss = 453.277, accuracy = 1.562\n",
      "iteration (350): loss = 450.058, accuracy = 0.000\n",
      "iteration (400): loss = 445.014, accuracy = 0.000\n",
      "iteration (450): loss = 441.025, accuracy = 0.000\n",
      "iteration (500): loss = 437.211, accuracy = 0.000\n",
      "-  epoch 0: validation accuracy = 0.516\n",
      "\n",
      "===== > train for epoch 1\n",
      "iteration (550): loss = 433.528, accuracy = 0.781\n",
      "iteration (600): loss = 429.964, accuracy = 0.781\n",
      "iteration (650): loss = 424.921, accuracy = 1.562\n",
      "iteration (700): loss = 421.877, accuracy = 1.562\n",
      "iteration (750): loss = 418.707, accuracy = 0.000\n",
      "iteration (800): loss = 414.090, accuracy = 0.000\n",
      "iteration (850): loss = 410.261, accuracy = 0.000\n",
      "iteration (900): loss = 405.389, accuracy = 1.562\n",
      "iteration (950): loss = 403.185, accuracy = 0.000\n",
      "iteration (1000): loss = 399.893, accuracy = 1.562\n",
      "-  epoch 1: validation accuracy = 0.568\n",
      "\n",
      "===== > train for epoch 2\n",
      "iteration (1050): loss = 397.196, accuracy = 0.781\n",
      "iteration (1100): loss = 392.292, accuracy = 0.781\n",
      "iteration (1150): loss = 390.205, accuracy = 1.562\n",
      "iteration (1200): loss = 386.685, accuracy = 0.000\n",
      "iteration (1250): loss = 384.043, accuracy = 0.781\n",
      "iteration (1300): loss = 380.315, accuracy = 0.000\n",
      "iteration (1350): loss = 378.391, accuracy = 0.781\n",
      "iteration (1400): loss = 374.487, accuracy = 0.000\n",
      "iteration (1450): loss = 371.507, accuracy = 0.781\n",
      "iteration (1500): loss = 367.857, accuracy = 0.781\n",
      "iteration (1550): loss = 366.513, accuracy = 0.000\n",
      "-  epoch 2: validation accuracy = 0.555\n",
      "\n",
      "===== > train for epoch 3\n",
      "iteration (1600): loss = 362.172, accuracy = 1.562\n",
      "iteration (1650): loss = 359.219, accuracy = 0.000\n",
      "iteration (1700): loss = 355.903, accuracy = 1.562\n",
      "iteration (1750): loss = 353.704, accuracy = 1.562\n",
      "iteration (1800): loss = 351.075, accuracy = 0.781\n",
      "iteration (1850): loss = 348.566, accuracy = 0.000\n",
      "iteration (1900): loss = 345.052, accuracy = 0.000\n",
      "iteration (1950): loss = 342.506, accuracy = 0.000\n",
      "iteration (2000): loss = 340.175, accuracy = 0.781\n",
      "iteration (2050): loss = 337.403, accuracy = 0.000\n",
      "-  epoch 3: validation accuracy = 0.586\n",
      "\n",
      "===== > train for epoch 4\n",
      "iteration (2100): loss = 335.215, accuracy = 1.562\n",
      "iteration (2150): loss = 332.315, accuracy = 0.000\n",
      "iteration (2200): loss = 330.777, accuracy = 1.562\n",
      "iteration (2250): loss = 327.741, accuracy = 0.781\n",
      "iteration (2300): loss = 324.749, accuracy = 0.781\n",
      "iteration (2350): loss = 322.685, accuracy = 0.781\n",
      "iteration (2400): loss = 319.769, accuracy = 0.000\n",
      "iteration (2450): loss = 317.413, accuracy = 0.781\n",
      "iteration (2500): loss = 315.340, accuracy = 0.781\n",
      "iteration (2550): loss = 312.279, accuracy = 1.562\n",
      "-  epoch 4: validation accuracy = 0.551\n",
      "\n",
      "===== > train for epoch 5\n",
      "iteration (2600): loss = 310.194, accuracy = 1.562\n",
      "iteration (2650): loss = 307.867, accuracy = 0.000\n",
      "iteration (2700): loss = 305.991, accuracy = 0.781\n",
      "iteration (2750): loss = 303.052, accuracy = 0.000\n",
      "iteration (2800): loss = 301.255, accuracy = 0.000\n",
      "iteration (2850): loss = 298.948, accuracy = 0.781\n",
      "iteration (2900): loss = 296.489, accuracy = 0.000\n",
      "iteration (2950): loss = 294.094, accuracy = 0.781\n",
      "iteration (3000): loss = 292.087, accuracy = 0.781\n",
      "iteration (3050): loss = 290.272, accuracy = 0.781\n",
      "iteration (3100): loss = 288.093, accuracy = 0.781\n",
      "-  epoch 5: validation accuracy = 0.564\n",
      "\n",
      "===== > train for epoch 6\n",
      "iteration (3150): loss = 285.843, accuracy = 0.000\n",
      "iteration (3200): loss = 283.620, accuracy = 1.562\n",
      "iteration (3250): loss = 281.661, accuracy = 0.781\n",
      "iteration (3300): loss = 279.869, accuracy = 0.000\n",
      "iteration (3350): loss = 277.895, accuracy = 1.562\n",
      "iteration (3400): loss = 275.523, accuracy = 0.000\n",
      "iteration (3450): loss = 273.912, accuracy = 0.000\n",
      "iteration (3500): loss = 271.858, accuracy = 0.000\n",
      "iteration (3550): loss = 269.762, accuracy = 0.781\n",
      "iteration (3600): loss = 267.882, accuracy = 1.562\n",
      "-  epoch 6: validation accuracy = 0.656\n",
      "\n",
      "===== > train for epoch 7\n",
      "iteration (3650): loss = 265.923, accuracy = 0.000\n",
      "iteration (3700): loss = 264.109, accuracy = 0.000\n",
      "iteration (3750): loss = 262.168, accuracy = 0.000\n",
      "iteration (3800): loss = 260.729, accuracy = 2.344\n",
      "iteration (3850): loss = 258.394, accuracy = 1.562\n",
      "iteration (3900): loss = 256.653, accuracy = 1.562\n",
      "iteration (3950): loss = 254.684, accuracy = 0.000\n",
      "iteration (4000): loss = 252.986, accuracy = 0.781\n",
      "iteration (4050): loss = 251.459, accuracy = 1.562\n",
      "iteration (4100): loss = 249.701, accuracy = 0.000\n",
      "-  epoch 7: validation accuracy = 0.647\n",
      "\n",
      "===== > train for epoch 8\n",
      "iteration (4150): loss = 247.926, accuracy = 0.000\n",
      "iteration (4200): loss = 246.130, accuracy = 0.781\n",
      "iteration (4250): loss = 244.460, accuracy = 1.562\n",
      "iteration (4300): loss = 242.936, accuracy = 0.781\n",
      "iteration (4350): loss = 241.245, accuracy = 0.000\n",
      "iteration (4400): loss = 239.580, accuracy = 0.000\n",
      "iteration (4450): loss = 237.741, accuracy = 0.000\n",
      "iteration (4500): loss = 236.000, accuracy = 2.344\n",
      "iteration (4550): loss = 234.907, accuracy = 0.781\n",
      "iteration (4600): loss = 232.942, accuracy = 0.781\n",
      "iteration (4650): loss = 231.572, accuracy = 1.562\n",
      "-  epoch 8: validation accuracy = 0.629\n",
      "\n",
      "===== > train for epoch 9\n",
      "iteration (4700): loss = 229.809, accuracy = 0.781\n",
      "iteration (4750): loss = 228.345, accuracy = 0.781\n",
      "iteration (4800): loss = 226.686, accuracy = 0.781\n",
      "iteration (4850): loss = 225.179, accuracy = 0.781\n",
      "iteration (4900): loss = 223.683, accuracy = 2.344\n",
      "iteration (4950): loss = 222.161, accuracy = 0.781\n",
      "iteration (5000): loss = 220.792, accuracy = 0.000\n",
      "iteration (5050): loss = 219.335, accuracy = 0.000\n",
      "iteration (5100): loss = 217.807, accuracy = 0.781\n",
      "iteration (5150): loss = 216.370, accuracy = 0.781\n",
      "-  epoch 9: validation accuracy = 0.634\n",
      "\n",
      "===== > train for epoch 10\n",
      "iteration (5200): loss = 214.967, accuracy = 0.000\n",
      "iteration (5250): loss = 213.471, accuracy = 0.000\n",
      "iteration (5300): loss = 212.150, accuracy = 0.781\n",
      "iteration (5350): loss = 210.618, accuracy = 1.562\n",
      "iteration (5400): loss = 209.249, accuracy = 1.562\n",
      "iteration (5450): loss = 207.745, accuracy = 0.000\n",
      "iteration (5500): loss = 206.608, accuracy = 0.000\n",
      "iteration (5550): loss = 205.205, accuracy = 0.000\n",
      "iteration (5600): loss = 203.784, accuracy = 2.344\n",
      "iteration (5650): loss = 202.464, accuracy = 0.781\n",
      "-  epoch 10: validation accuracy = 0.642\n",
      "\n",
      "===== > train for epoch 11\n",
      "iteration (5700): loss = 201.192, accuracy = 1.562\n",
      "iteration (5750): loss = 199.955, accuracy = 0.000\n",
      "iteration (5800): loss = 198.666, accuracy = 1.562\n",
      "iteration (5850): loss = 197.252, accuracy = 0.781\n",
      "iteration (5900): loss = 195.901, accuracy = 2.344\n",
      "iteration (5950): loss = 194.728, accuracy = 2.344\n",
      "iteration (6000): loss = 193.678, accuracy = 0.781\n",
      "iteration (6050): loss = 192.361, accuracy = 0.781\n",
      "iteration (6100): loss = 191.157, accuracy = 3.125\n",
      "iteration (6150): loss = 189.955, accuracy = 0.781\n",
      "iteration (6200): loss = 188.643, accuracy = 0.000\n",
      "-  epoch 11: validation accuracy = 0.625\n",
      "\n",
      "===== > train for epoch 12\n",
      "iteration (6250): loss = 187.550, accuracy = 0.781\n",
      "iteration (6300): loss = 186.083, accuracy = 2.344\n",
      "iteration (6350): loss = 185.198, accuracy = 0.000\n",
      "iteration (6400): loss = 183.925, accuracy = 0.000\n",
      "iteration (6450): loss = 182.955, accuracy = 0.000\n",
      "iteration (6500): loss = 181.580, accuracy = 1.562\n",
      "iteration (6550): loss = 180.468, accuracy = 1.562\n",
      "iteration (6600): loss = 179.513, accuracy = 0.781\n",
      "iteration (6650): loss = 178.283, accuracy = 1.562\n",
      "iteration (6700): loss = 177.187, accuracy = 0.000\n",
      "-  epoch 12: validation accuracy = 0.638\n",
      "\n",
      "===== > train for epoch 13\n",
      "iteration (6750): loss = 176.178, accuracy = 2.344\n",
      "iteration (6800): loss = 174.912, accuracy = 1.562\n",
      "iteration (6850): loss = 173.914, accuracy = 0.781\n",
      "iteration (6900): loss = 172.868, accuracy = 0.781\n",
      "iteration (6950): loss = 171.828, accuracy = 0.781\n",
      "iteration (7000): loss = 170.729, accuracy = 0.000\n",
      "iteration (7050): loss = 169.629, accuracy = 0.000\n",
      "iteration (7100): loss = 168.741, accuracy = 0.781\n",
      "iteration (7150): loss = 167.664, accuracy = 0.000\n",
      "iteration (7200): loss = 166.819, accuracy = 1.562\n",
      "iteration (7250): loss = 165.742, accuracy = 0.000\n",
      "-  epoch 13: validation accuracy = 0.603\n",
      "\n",
      "===== > train for epoch 14\n",
      "iteration (7300): loss = 164.608, accuracy = 0.000\n",
      "iteration (7350): loss = 163.764, accuracy = 0.781\n",
      "iteration (7400): loss = 162.742, accuracy = 0.000\n",
      "iteration (7450): loss = 161.752, accuracy = 0.781\n",
      "iteration (7500): loss = 160.733, accuracy = 1.562\n",
      "iteration (7550): loss = 159.819, accuracy = 0.000\n",
      "iteration (7600): loss = 158.872, accuracy = 1.562\n",
      "iteration (7650): loss = 157.942, accuracy = 0.781\n",
      "iteration (7700): loss = 156.902, accuracy = 0.781\n",
      "iteration (7750): loss = 156.111, accuracy = 0.000\n",
      "-  epoch 14: validation accuracy = 0.612\n",
      "\n",
      "===== > train for epoch 15\n",
      "iteration (7800): loss = 155.164, accuracy = 0.781\n",
      "iteration (7850): loss = 154.290, accuracy = 0.000\n",
      "iteration (7900): loss = 153.307, accuracy = 2.344\n",
      "iteration (7950): loss = 152.408, accuracy = 1.562\n",
      "iteration (8000): loss = 151.497, accuracy = 3.125\n",
      "iteration (8050): loss = 150.620, accuracy = 0.000\n",
      "iteration (8100): loss = 149.792, accuracy = 1.562\n",
      "iteration (8150): loss = 149.006, accuracy = 0.781\n",
      "iteration (8200): loss = 148.089, accuracy = 0.000\n",
      "iteration (8250): loss = 147.341, accuracy = 0.781\n",
      "-  epoch 15: validation accuracy = 0.621\n",
      "\n",
      "===== > train for epoch 16\n",
      "iteration (8300): loss = 146.470, accuracy = 0.000\n",
      "iteration (8350): loss = 145.578, accuracy = 0.781\n",
      "iteration (8400): loss = 144.781, accuracy = 1.562\n",
      "iteration (8450): loss = 143.788, accuracy = 3.906\n",
      "iteration (8500): loss = 143.107, accuracy = 2.344\n",
      "iteration (8550): loss = 142.367, accuracy = 2.344\n",
      "iteration (8600): loss = 141.458, accuracy = 1.562\n",
      "iteration (8650): loss = 140.705, accuracy = 1.562\n",
      "iteration (8700): loss = 139.871, accuracy = 0.781\n",
      "iteration (8750): loss = 139.219, accuracy = 0.000\n",
      "iteration (8800): loss = 138.389, accuracy = 1.562\n",
      "-  epoch 16: validation accuracy = 0.629\n",
      "\n",
      "===== > train for epoch 17\n",
      "iteration (8850): loss = 137.630, accuracy = 0.000\n",
      "iteration (8900): loss = 136.808, accuracy = 0.000\n",
      "iteration (8950): loss = 136.055, accuracy = 0.781\n",
      "iteration (9000): loss = 135.283, accuracy = 2.344\n",
      "iteration (9050): loss = 134.664, accuracy = 0.781\n",
      "iteration (9100): loss = 133.811, accuracy = 2.344\n",
      "iteration (9150): loss = 133.131, accuracy = 1.562\n",
      "iteration (9200): loss = 132.448, accuracy = 0.781\n",
      "iteration (9250): loss = 131.673, accuracy = 1.562\n",
      "iteration (9300): loss = 130.943, accuracy = 0.000\n",
      "-  epoch 17: validation accuracy = 0.642\n",
      "\n",
      "===== > train for epoch 18\n",
      "iteration (9350): loss = 130.271, accuracy = 0.000\n",
      "iteration (9400): loss = 129.479, accuracy = 2.344\n",
      "iteration (9450): loss = 128.818, accuracy = 0.000\n",
      "iteration (9500): loss = 128.145, accuracy = 3.125\n",
      "iteration (9550): loss = 127.462, accuracy = 0.781\n",
      "iteration (9600): loss = 126.877, accuracy = 0.781\n",
      "iteration (9650): loss = 126.117, accuracy = 2.344\n",
      "iteration (9700): loss = 125.465, accuracy = 0.781\n",
      "iteration (9750): loss = 124.819, accuracy = 0.000\n",
      "iteration (9800): loss = 124.140, accuracy = 0.781\n",
      "-  epoch 18: validation accuracy = 0.677\n",
      "\n",
      "===== > train for epoch 19\n",
      "iteration (9850): loss = 123.477, accuracy = 0.781\n",
      "iteration (9900): loss = 122.721, accuracy = 1.562\n",
      "iteration (9950): loss = 122.161, accuracy = 0.000\n",
      "iteration (10000): loss = 121.543, accuracy = 0.781\n",
      "iteration (10050): loss = 120.962, accuracy = 0.000\n",
      "iteration (10100): loss = 120.326, accuracy = 0.781\n",
      "iteration (10150): loss = 119.655, accuracy = 1.562\n",
      "iteration (10200): loss = 118.997, accuracy = 2.344\n",
      "iteration (10250): loss = 118.436, accuracy = 0.781\n",
      "iteration (10300): loss = 117.928, accuracy = 0.781\n",
      "iteration (10350): loss = 117.245, accuracy = 1.562\n",
      "-  epoch 19: validation accuracy = 0.721\n",
      "\n",
      "===== > train for epoch 20\n",
      "iteration (10400): loss = 116.608, accuracy = 0.781\n",
      "iteration (10450): loss = 115.948, accuracy = 0.000\n",
      "iteration (10500): loss = 115.447, accuracy = 0.000\n",
      "iteration (10550): loss = 114.905, accuracy = 1.562\n",
      "iteration (10600): loss = 114.245, accuracy = 1.562\n",
      "iteration (10650): loss = 113.634, accuracy = 0.000\n",
      "iteration (10700): loss = 113.190, accuracy = 2.344\n",
      "iteration (10750): loss = 112.589, accuracy = 0.781\n",
      "iteration (10800): loss = 112.090, accuracy = 0.000\n",
      "iteration (10850): loss = 111.497, accuracy = 0.000\n",
      "-  epoch 20: validation accuracy = 0.690\n",
      "\n",
      "===== > train for epoch 21\n",
      "iteration (10900): loss = 110.856, accuracy = 1.562\n",
      "iteration (10950): loss = 110.408, accuracy = 1.562\n",
      "iteration (11000): loss = 109.778, accuracy = 1.562\n",
      "iteration (11050): loss = 109.251, accuracy = 2.344\n",
      "iteration (11100): loss = 108.766, accuracy = 0.000\n",
      "iteration (11150): loss = 108.229, accuracy = 1.562\n",
      "iteration (11200): loss = 107.704, accuracy = 0.781\n",
      "iteration (11250): loss = 107.225, accuracy = 0.781\n",
      "iteration (11300): loss = 106.756, accuracy = 1.562\n",
      "iteration (11350): loss = 106.173, accuracy = 2.344\n",
      "-  epoch 21: validation accuracy = 0.695\n",
      "\n",
      "===== > train for epoch 22\n",
      "iteration (11400): loss = 105.576, accuracy = 0.781\n",
      "iteration (11450): loss = 105.093, accuracy = 2.344\n",
      "iteration (11500): loss = 104.623, accuracy = 1.562\n",
      "iteration (11550): loss = 104.101, accuracy = 3.125\n",
      "iteration (11600): loss = 103.641, accuracy = 1.562\n",
      "iteration (11650): loss = 103.144, accuracy = 0.000\n",
      "iteration (11700): loss = 102.655, accuracy = 0.781\n",
      "iteration (11750): loss = 102.140, accuracy = 0.000\n",
      "iteration (11800): loss = 101.643, accuracy = 0.000\n",
      "iteration (11850): loss = 101.172, accuracy = 0.781\n",
      "iteration (11900): loss = 100.715, accuracy = 1.562\n",
      "-  epoch 22: validation accuracy = 0.708\n",
      "\n",
      "===== > train for epoch 23\n",
      "iteration (11950): loss = 100.293, accuracy = 0.000\n",
      "iteration (12000): loss = 99.767, accuracy = 1.562\n",
      "iteration (12050): loss = 99.329, accuracy = 0.781\n",
      "iteration (12100): loss = 98.844, accuracy = 0.000\n",
      "iteration (12150): loss = 98.446, accuracy = 1.562\n",
      "iteration (12200): loss = 97.990, accuracy = 1.562\n",
      "iteration (12250): loss = 97.602, accuracy = 1.562\n",
      "iteration (12300): loss = 97.067, accuracy = 0.781\n",
      "iteration (12350): loss = 96.680, accuracy = 1.562\n",
      "iteration (12400): loss = 96.195, accuracy = 0.000\n",
      "-  epoch 23: validation accuracy = 0.717\n",
      "\n",
      "===== > train for epoch 24\n",
      "iteration (12450): loss = 95.716, accuracy = 1.562\n",
      "iteration (12500): loss = 95.305, accuracy = 0.781\n",
      "iteration (12550): loss = 94.895, accuracy = 2.344\n",
      "iteration (12600): loss = 94.466, accuracy = 0.781\n",
      "iteration (12650): loss = 94.129, accuracy = 0.000\n",
      "iteration (12700): loss = 93.638, accuracy = 0.000\n",
      "iteration (12750): loss = 93.281, accuracy = 0.000\n",
      "iteration (12800): loss = 92.819, accuracy = 0.000\n",
      "iteration (12850): loss = 92.398, accuracy = 0.000\n",
      "iteration (12900): loss = 92.035, accuracy = 0.781\n",
      "iteration (12950): loss = 91.585, accuracy = 0.000\n",
      "-  epoch 24: validation accuracy = 0.717\n",
      "\n",
      "===== > train for epoch 25\n",
      "iteration (13000): loss = 91.219, accuracy = 0.000\n",
      "iteration (13050): loss = 90.853, accuracy = 1.562\n",
      "iteration (13100): loss = 90.482, accuracy = 0.000\n",
      "iteration (13150): loss = 90.010, accuracy = 0.000\n",
      "iteration (13200): loss = 89.624, accuracy = 1.562\n",
      "iteration (13250): loss = 89.251, accuracy = 0.000\n",
      "iteration (13300): loss = 88.880, accuracy = 0.781\n",
      "iteration (13350): loss = 88.493, accuracy = 1.562\n",
      "iteration (13400): loss = 88.142, accuracy = 0.000\n",
      "iteration (13450): loss = 87.753, accuracy = 0.781\n",
      "-  epoch 25: validation accuracy = 0.734\n",
      "\n",
      "===== > train for epoch 26\n",
      "iteration (13500): loss = 87.339, accuracy = 0.000\n",
      "iteration (13550): loss = 86.959, accuracy = 3.906\n",
      "iteration (13600): loss = 86.652, accuracy = 2.344\n",
      "iteration (13650): loss = 86.299, accuracy = 0.781\n",
      "iteration (13700): loss = 85.912, accuracy = 1.562\n",
      "iteration (13750): loss = 85.573, accuracy = 0.781\n",
      "iteration (13800): loss = 85.185, accuracy = 1.562\n",
      "iteration (13850): loss = 84.800, accuracy = 0.781\n",
      "iteration (13900): loss = 84.473, accuracy = 0.000\n",
      "iteration (13950): loss = 84.080, accuracy = 3.125\n",
      "-  epoch 26: validation accuracy = 0.712\n",
      "\n",
      "===== > train for epoch 27\n",
      "iteration (14000): loss = 83.799, accuracy = 1.562\n",
      "iteration (14050): loss = 83.438, accuracy = 0.000\n",
      "iteration (14100): loss = 83.152, accuracy = 0.781\n",
      "iteration (14150): loss = 82.786, accuracy = 0.000\n",
      "iteration (14200): loss = 82.394, accuracy = 0.781\n",
      "iteration (14250): loss = 82.071, accuracy = 1.562\n",
      "iteration (14300): loss = 81.799, accuracy = 0.781\n",
      "iteration (14350): loss = 81.487, accuracy = 0.781\n",
      "iteration (14400): loss = 81.078, accuracy = 2.344\n",
      "iteration (14450): loss = 80.812, accuracy = 0.000\n",
      "iteration (14500): loss = 80.513, accuracy = 0.781\n",
      "-  epoch 27: validation accuracy = 0.756\n",
      "\n",
      "===== > train for epoch 28\n",
      "iteration (14550): loss = 80.147, accuracy = 0.781\n",
      "iteration (14600): loss = 79.838, accuracy = 0.781\n",
      "iteration (14650): loss = 79.506, accuracy = 2.344\n",
      "iteration (14700): loss = 79.246, accuracy = 0.781\n",
      "iteration (14750): loss = 78.920, accuracy = 1.562\n",
      "iteration (14800): loss = 78.595, accuracy = 0.781\n",
      "iteration (14850): loss = 78.267, accuracy = 0.000\n",
      "iteration (14900): loss = 77.949, accuracy = 1.562\n",
      "iteration (14950): loss = 77.722, accuracy = 0.000\n",
      "iteration (15000): loss = 77.365, accuracy = 1.562\n",
      "-  epoch 28: validation accuracy = 0.791\n",
      "\n",
      "===== > train for epoch 29\n",
      "iteration (15050): loss = 77.068, accuracy = 0.000\n",
      "iteration (15100): loss = 76.849, accuracy = 0.000\n",
      "iteration (15150): loss = 76.544, accuracy = 0.000\n",
      "iteration (15200): loss = 76.214, accuracy = 0.000\n",
      "iteration (15250): loss = 75.927, accuracy = 0.000\n",
      "iteration (15300): loss = 75.616, accuracy = 0.781\n",
      "iteration (15350): loss = 75.337, accuracy = 1.562\n",
      "iteration (15400): loss = 75.060, accuracy = 3.125\n",
      "iteration (15450): loss = 74.803, accuracy = 1.562\n",
      "iteration (15500): loss = 74.475, accuracy = 0.781\n",
      "-  epoch 29: validation accuracy = 0.782\n",
      "\n",
      "===== > train for epoch 30\n",
      "iteration (15550): loss = 74.256, accuracy = 0.781\n",
      "iteration (15600): loss = 73.918, accuracy = 0.000\n",
      "iteration (15650): loss = 73.688, accuracy = 0.000\n",
      "iteration (15700): loss = 73.367, accuracy = 2.344\n",
      "iteration (15750): loss = 73.224, accuracy = 1.562\n",
      "iteration (15800): loss = 72.822, accuracy = 1.562\n",
      "iteration (15850): loss = 72.652, accuracy = 0.781\n",
      "iteration (15900): loss = 72.341, accuracy = 0.781\n",
      "iteration (15950): loss = 72.092, accuracy = 1.562\n",
      "iteration (16000): loss = 71.843, accuracy = 0.000\n",
      "iteration (16050): loss = 71.590, accuracy = 0.781\n",
      "-  epoch 30: validation accuracy = 0.813\n",
      "\n",
      "===== > train for epoch 31\n",
      "iteration (16100): loss = 71.364, accuracy = 0.000\n",
      "iteration (16150): loss = 71.060, accuracy = 0.000\n",
      "iteration (16200): loss = 70.786, accuracy = 1.562\n",
      "iteration (16250): loss = 70.563, accuracy = 0.000\n",
      "iteration (16300): loss = 70.301, accuracy = 3.125\n",
      "iteration (16350): loss = 70.017, accuracy = 1.562\n",
      "iteration (16400): loss = 69.832, accuracy = 3.125\n",
      "iteration (16450): loss = 69.596, accuracy = 2.344\n",
      "iteration (16500): loss = 69.306, accuracy = 2.344\n",
      "iteration (16550): loss = 69.071, accuracy = 1.562\n",
      "-  epoch 31: validation accuracy = 0.804\n",
      "\n",
      "===== > train for epoch 32\n",
      "iteration (16600): loss = 68.832, accuracy = 0.000\n",
      "iteration (16650): loss = 68.634, accuracy = 0.000\n",
      "iteration (16700): loss = 68.413, accuracy = 0.000\n",
      "iteration (16750): loss = 68.181, accuracy = 0.781\n",
      "iteration (16800): loss = 67.854, accuracy = 1.562\n",
      "iteration (16850): loss = 67.668, accuracy = 2.344\n",
      "iteration (16900): loss = 67.442, accuracy = 0.000\n",
      "iteration (16950): loss = 67.164, accuracy = 0.000\n",
      "iteration (17000): loss = 66.979, accuracy = 1.562\n",
      "iteration (17050): loss = 66.802, accuracy = 0.781\n",
      "-  epoch 32: validation accuracy = 0.804\n",
      "\n",
      "===== > train for epoch 33\n",
      "iteration (17100): loss = 66.572, accuracy = 2.344\n",
      "iteration (17150): loss = 66.331, accuracy = 1.562\n",
      "iteration (17200): loss = 66.099, accuracy = 0.000\n",
      "iteration (17250): loss = 65.909, accuracy = 0.781\n",
      "iteration (17300): loss = 65.679, accuracy = 1.562\n",
      "iteration (17350): loss = 65.516, accuracy = 2.344\n",
      "iteration (17400): loss = 65.254, accuracy = 1.562\n",
      "iteration (17450): loss = 65.012, accuracy = 2.344\n",
      "iteration (17500): loss = 64.833, accuracy = 0.000\n",
      "iteration (17550): loss = 64.578, accuracy = 3.125\n",
      "iteration (17600): loss = 64.412, accuracy = 3.125\n",
      "-  epoch 33: validation accuracy = 0.843\n",
      "\n",
      "===== > train for epoch 34\n",
      "iteration (17650): loss = 64.190, accuracy = 0.000\n",
      "iteration (17700): loss = 64.010, accuracy = 0.781\n",
      "iteration (17750): loss = 63.783, accuracy = 0.781\n",
      "iteration (17800): loss = 63.589, accuracy = 2.344\n",
      "iteration (17850): loss = 63.387, accuracy = 0.781\n",
      "iteration (17900): loss = 63.197, accuracy = 2.344\n",
      "iteration (17950): loss = 62.992, accuracy = 2.344\n",
      "iteration (18000): loss = 62.808, accuracy = 0.000\n",
      "iteration (18050): loss = 62.585, accuracy = 0.000\n",
      "iteration (18100): loss = 62.400, accuracy = 1.562\n",
      "-  epoch 34: validation accuracy = 0.830\n",
      "\n",
      "===== > train for epoch 35\n",
      "iteration (18150): loss = 62.212, accuracy = 1.562\n",
      "iteration (18200): loss = 62.031, accuracy = 1.562\n",
      "iteration (18250): loss = 61.799, accuracy = 0.781\n",
      "iteration (18300): loss = 61.604, accuracy = 0.000\n",
      "iteration (18350): loss = 61.458, accuracy = 0.000\n",
      "iteration (18400): loss = 61.224, accuracy = 3.125\n",
      "iteration (18450): loss = 61.106, accuracy = 0.000\n",
      "iteration (18500): loss = 60.931, accuracy = 0.781\n",
      "iteration (18550): loss = 60.651, accuracy = 1.562\n",
      "iteration (18600): loss = 60.464, accuracy = 0.781\n",
      "-  epoch 35: validation accuracy = 0.878\n",
      "\n",
      "===== > train for epoch 36\n",
      "iteration (18650): loss = 60.333, accuracy = 0.781\n",
      "iteration (18700): loss = 60.188, accuracy = 1.562\n",
      "iteration (18750): loss = 59.978, accuracy = 2.344\n",
      "iteration (18800): loss = 59.739, accuracy = 2.344\n",
      "iteration (18850): loss = 59.544, accuracy = 0.000\n",
      "iteration (18900): loss = 59.398, accuracy = 3.125\n",
      "iteration (18950): loss = 59.307, accuracy = 1.562\n",
      "iteration (19000): loss = 59.090, accuracy = 1.562\n",
      "iteration (19050): loss = 58.951, accuracy = 0.000\n",
      "iteration (19100): loss = 58.751, accuracy = 0.000\n",
      "iteration (19150): loss = 58.524, accuracy = 2.344\n",
      "-  epoch 36: validation accuracy = 0.896\n",
      "\n",
      "===== > train for epoch 37\n",
      "iteration (19200): loss = 58.453, accuracy = 0.781\n",
      "iteration (19250): loss = 58.149, accuracy = 4.688\n",
      "iteration (19300): loss = 58.074, accuracy = 0.781\n",
      "iteration (19350): loss = 57.867, accuracy = 0.000\n",
      "iteration (19400): loss = 57.783, accuracy = 2.344\n",
      "iteration (19450): loss = 57.585, accuracy = 2.344\n",
      "iteration (19500): loss = 57.389, accuracy = 0.781\n",
      "iteration (19550): loss = 57.288, accuracy = 0.000\n",
      "iteration (19600): loss = 57.077, accuracy = 0.781\n",
      "iteration (19650): loss = 56.954, accuracy = 0.000\n",
      "-  epoch 37: validation accuracy = 0.900\n",
      "\n",
      "===== > train for epoch 38\n",
      "iteration (19700): loss = 56.789, accuracy = 0.781\n",
      "iteration (19750): loss = 56.570, accuracy = 1.562\n",
      "iteration (19800): loss = 56.433, accuracy = 2.344\n",
      "iteration (19850): loss = 56.283, accuracy = 0.781\n",
      "iteration (19900): loss = 56.139, accuracy = 1.562\n",
      "iteration (19950): loss = 55.965, accuracy = 0.000\n",
      "iteration (20000): loss = 55.798, accuracy = 1.562\n",
      "iteration (20050): loss = 55.733, accuracy = 0.781\n",
      "iteration (20100): loss = 55.512, accuracy = 0.000\n",
      "iteration (20150): loss = 55.399, accuracy = 0.000\n",
      "iteration (20200): loss = 55.231, accuracy = 3.906\n",
      "-  epoch 38: validation accuracy = 0.896\n",
      "\n",
      "===== > train for epoch 39\n",
      "iteration (20250): loss = 55.081, accuracy = 0.000\n",
      "iteration (20300): loss = 54.974, accuracy = 0.000\n",
      "iteration (20350): loss = 54.777, accuracy = 2.344\n",
      "iteration (20400): loss = 54.650, accuracy = 1.562\n",
      "iteration (20450): loss = 54.508, accuracy = 0.781\n",
      "iteration (20500): loss = 54.325, accuracy = 0.781\n",
      "iteration (20550): loss = 54.217, accuracy = 3.125\n",
      "iteration (20600): loss = 54.074, accuracy = 0.781\n",
      "iteration (20650): loss = 53.917, accuracy = 0.781\n",
      "iteration (20700): loss = 53.786, accuracy = 0.000\n",
      "-  epoch 39: validation accuracy = 0.926\n",
      "\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 40\n",
    "log_step = 50\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == labels)\n",
    "            / predictions.shape[0])\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "train_dataset = X_train\n",
    "train_labels = Y_train\n",
    "valid_labels = Y_val\n",
    "test_labels = Y_test\n",
    "\n",
    "curr_step = 0\n",
    "num_training = len(train_dataset)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for epoch in range(num_epoch):\n",
    "        print('===== > train for epoch %d' % epoch)\n",
    "        for step in range(num_training // batch_size):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            curr_step += 1\n",
    "            if (curr_step % log_step == 0):\n",
    "                print('iteration (%d): loss = %.3f, accuracy = %.3f' %\n",
    "                        (curr_step, l, accuracy(predictions, batch_labels)))\n",
    "        print('-  epoch %d: validation accuracy = %.3f' % (epoch, accuracy(valid_prediction.eval(), valid_labels)))\n",
    "        print()\n",
    "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
