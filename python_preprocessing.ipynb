{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2289 DR1-MCPM0-SA1-00.wav\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "indir = 'chunks/' # already VAD\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = 'chunks/'\n",
    "flist = [f for f in listdir(indir) if isfile(join(indir, f))]\n",
    "print(len(flist), flist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(fname):\n",
    "    attr = fname.split('.')[0].split('-')\n",
    "    dialect = attr[0]\n",
    "    gender = attr[1][0]\n",
    "    speaker_id = attr[1]\n",
    "    sentence_type = attr[2][:2]\n",
    "    return dialect, gender, speaker_id, sentence_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DR1', 'M', 'MCPM0', 'SA')\n"
     ]
    }
   ],
   "source": [
    "print(get_attributes('DR1-MCPM0-SA1-00.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "\n",
    "for fname in flist:\n",
    "    input_path = indir + fname\n",
    "    y, sr = librosa.load(input_path, sr=None) # set sr=None for orig file sr otherwise it is converted to ~22K\n",
    "\n",
    "    # scaling the maximum of absolute amplitude to 1\n",
    "    processed_data = y/max(abs(y))\n",
    "    \n",
    "    # TODO: calc VAD\n",
    "    \n",
    "    # https://groups.google.com/forum/#!topic/librosa/V4Z1HpTKn8Q\n",
    "    mfcc = librosa.feature.mfcc(y=processed_data, sr=sr, n_mfcc=13, n_fft=(25*sr)//1000, hop_length=(10*sr)//1000)\n",
    "    mfcc[0] = librosa.feature.rmse(processed_data, hop_length=int(0.010*sr), n_fft=int(0.025*sr)) \n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    features = np.vstack([mfcc, mfcc_delta, mfcc_delta2]) \n",
    "    \n",
    "    # split train test\n",
    "    dialect, gender, speaker_id, sentence_type = get_attributes(fname)\n",
    "    if sentence_type == 'SA':\n",
    "        test.setdefault(speaker_id, []).append(features)\n",
    "    else:\n",
    "        train.setdefault(speaker_id, []).append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MADC0', 'MAEB0', 'MAKB0', 'MAKR0', 'MAPV0', 'MARC0', 'MARW0', 'MBEF0', 'MBGT0', 'MBJV0', 'MBMA0', 'MBWP0', 'MCAL0', 'MCDC0', 'MCDD0', 'MCDR0', 'MCEF0', 'MCEW0', 'MCHL0', 'MCLM0', 'MCPM0', 'MCSS0', 'MCTM0', 'MDAC0', 'MDAS0', 'MDBB1', 'MDBP0', 'MDCD0', 'MDDC0', 'MDEF0', 'MDEM0', 'MDHL0', 'MDHS0', 'MDJM0', 'MDLB0', 'MDLC0', 'MDLC2', 'MDLH0', 'MDMA0', 'MDMT0', 'MDNS0', 'MDPK0', 'MDPS0', 'MDSJ0', 'MDSS0', 'MDSS1', 'MDTB0', 'MDWD0', 'MDWH0', 'MDWM0', 'MEDR0', 'MEFG0', 'MEGJ0', 'MESG0', 'MEWM0', 'MFER0', 'MFMC0', 'MFRM0', 'MFWK0', 'MGAF0', 'MGAG0', 'MGES0', 'MGJC0', 'MGRL0', 'MGRP0', 'MGSH0', 'MGXP0', 'MHIT0', 'MHJB0', 'MHMG0', 'MHMR0', 'MHRM0', 'MILB0', 'MJAC0', 'MJAE0', 'MJBG0', 'MJDA0', 'MJDC0', 'MJDE0', 'MJEB0', 'MJEB1', 'MJEE0', 'MJHI0', 'MJJB0', 'MJJJ0', 'MJKR0', 'MJLB0', 'MJLG1', 'MJLS0', 'MJMA0', 'MJMD0', 'MJMM0', 'MJPM0', 'MJPM1', 'MJRH0', 'MJRH1', 'MJRP0', 'MJSR0', 'MJWS0', 'MJWT0', 'MJXL0', 'MKAH0', 'MKAJ0', 'MKAM0', 'MKDT0', 'MKJO0', 'MKLS0', 'MKLS1', 'MKLW0', 'MKXL0', 'MLBC0', 'MLEL0', 'MLJC0', 'MLJH0', 'MLNS0', 'MLSH0', 'MMAA0', 'MMAG0', 'MMAM0', 'MMAR0', 'MMBS0', 'MMDM0', 'MMDS0', 'MMEB0', 'MMGC0', 'MMGG0', 'MMGK0', 'MMJB1', 'MMRP0', 'MMSM0', 'MMXS0', 'MNET0', 'MPEB0', 'MPGH0', 'MPGR0', 'MPPC0', 'MPRB0', 'MPRD0', 'MPRK0', 'MPRT0', 'MPSW0', 'MRAB0', 'MRAB1', 'MRAI0', 'MRBC0', 'MRCG0', 'MRCW0', 'MRDD0', 'MRDS0', 'MREE0', 'MREH1', 'MRFK0', 'MRFL0', 'MRGM0', 'MRGS0', 'MRHL0', 'MRJB1', 'MRJH0', 'MRJM0', 'MRJM1', 'MRJT0', 'MRLJ0', 'MRLR0', 'MRMS0', 'MRSO0', 'MRSP0', 'MRTC0', 'MRTJ0', 'MRWA0', 'MRWS0', 'MSAT0', 'MSFH0', 'MSFV0', 'MSMC0', 'MSMS0', 'MSRG0', 'MSTF0', 'MTAS0', 'MTAT1', 'MTBC0', 'MTDB0', 'MTJG0', 'MTJM0', 'MTJS0', 'MTKP0', 'MTLB0', 'MTPF0', 'MTPG0', 'MTPP0', 'MTQC0', 'MTRC0', 'MTRR0', 'MTRT0', 'MVJH0', 'MWAD0', 'MWAR0', 'MWDK0', 'MWGR0', 'MWSB0', 'MZMB0']\n",
      "{'MGJC0': 62, 'MKJO0': 105, 'MRDS0': 148, 'MRAB0': 141, 'MBEF0': 7, 'MJMM0': 91, 'MKAM0': 103, 'MMGK0': 126, 'MCAL0': 12, 'MREE0': 149, 'MTPG0': 187, 'MTJS0': 183, 'MJMA0': 89, 'MTJG0': 181, 'MDTB0': 46, 'MFMC0': 56, 'MKLS1': 107, 'MRSP0': 165, 'MMSM0': 129, 'MAKR0': 3, 'MJRP0': 96, 'MDDC0': 28, 'MMBS0': 120, 'MPRD0': 137, 'MJAE0': 74, 'MTRC0': 190, 'MLBC0': 110, 'MAEB0': 1, 'MCPM0': 20, 'MWGR0': 197, 'MTAT1': 178, 'MSRG0': 175, 'MTBC0': 179, 'MJEB1': 80, 'MDNS0': 40, 'MTKP0': 184, 'MJEB0': 79, 'MREH1': 150, 'MJPM1': 93, 'MSFV0': 172, 'MFRM0': 57, 'MRGS0': 154, 'MEWM0': 54, 'MJRH1': 95, 'MMRP0': 128, 'MJWT0': 99, 'MDSS1': 45, 'MJDE0': 78, 'MJLG1': 87, 'MVJH0': 193, 'MRJH0': 157, 'MKXL0': 109, 'MHRM0': 71, 'MTPF0': 186, 'MTPP0': 188, 'MMDS0': 122, 'MGSH0': 65, 'MARC0': 5, 'MLNS0': 114, 'MRAI0': 143, 'MRCW0': 146, 'MJPM0': 92, 'MWAR0': 195, 'MWSB0': 198, 'MPRB0': 136, 'MCLM0': 19, 'MSTF0': 176, 'MJXL0': 100, 'MJBG0': 75, 'MPGR0': 134, 'MTAS0': 177, 'MZMB0': 199, 'MPRT0': 139, 'MKDT0': 104, 'MDLC0': 35, 'MDAC0': 23, 'MBJV0': 9, 'MRSO0': 164, 'MRHL0': 155, 'MDWM0': 49, 'MDMA0': 38, 'MTJM0': 182, 'MFWK0': 58, 'MSMS0': 174, 'MARW0': 6, 'MRBC0': 144, 'MJKR0': 85, 'MLEL0': 111, 'MSAT0': 170, 'MSFH0': 171, 'MMJB1': 127, 'MJDA0': 76, 'MRJB1': 156, 'MESG0': 53, 'MWDK0': 196, 'MCSS0': 21, 'MDEM0': 30, 'MAPV0': 4, 'MDBP0': 26, 'MKAH0': 101, 'MDAS0': 24, 'MJWS0': 98, 'MDBB1': 25, 'MTLB0': 185, 'MDLH0': 37, 'MMAG0': 117, 'MHIT0': 67, 'MLJC0': 112, 'MCEW0': 17, 'MPRK0': 138, 'MGRP0': 64, 'MADC0': 0, 'MDSS0': 44, 'MRWS0': 169, 'MRWA0': 168, 'MCHL0': 18, 'MCEF0': 16, 'MEGJ0': 52, 'MJHI0': 82, 'MILB0': 72, 'MJJB0': 83, 'MRTJ0': 167, 'MBGT0': 8, 'MJLB0': 86, 'MPSW0': 140, 'MDHL0': 31, 'MLJH0': 113, 'MRMS0': 163, 'MMEB0': 123, 'MDSJ0': 43, 'MTRT0': 192, 'MJSR0': 97, 'MFER0': 55, 'MGAF0': 59, 'MMAA0': 116, 'MDLC2': 36, 'MCTM0': 22, 'MBMA0': 10, 'MDHS0': 32, 'MEDR0': 50, 'MDCD0': 27, 'MEFG0': 51, 'MDEF0': 29, 'MCDD0': 14, 'MDMT0': 39, 'MPGH0': 133, 'MBWP0': 11, 'MRLR0': 162, 'MNET0': 131, 'MJLS0': 88, 'MGRL0': 63, 'MTQC0': 189, 'MMDM0': 121, 'MJEE0': 81, 'MLSH0': 115, 'MMGG0': 125, 'MRJM0': 158, 'MDJM0': 33, 'MRLJ0': 161, 'MJRH0': 94, 'MKAJ0': 102, 'MHMG0': 69, 'MRDD0': 147, 'MMGC0': 124, 'MRFL0': 152, 'MJAC0': 73, 'MPEB0': 132, 'MGAG0': 60, 'MWAD0': 194, 'MDWD0': 47, 'MJMD0': 90, 'MRAB1': 142, 'MMAR0': 119, 'MMXS0': 130, 'MTDB0': 180, 'MHJB0': 68, 'MKLW0': 108, 'MRJM1': 159, 'MRJT0': 160, 'MPPC0': 135, 'MRTC0': 166, 'MMAM0': 118, 'MJJJ0': 84, 'MAKB0': 2, 'MSMC0': 173, 'MRCG0': 145, 'MDLB0': 34, 'MRGM0': 153, 'MHMR0': 70, 'MRFK0': 151, 'MGXP0': 66, 'MDPK0': 41, 'MKLS0': 106, 'MJDC0': 77, 'MDPS0': 42, 'MDWH0': 48, 'MTRR0': 191, 'MCDR0': 15, 'MGES0': 61, 'MCDC0': 13}\n"
     ]
    }
   ],
   "source": [
    "ids = list(test.keys())\n",
    "ids.sort()\n",
    "print(ids)\n",
    "\n",
    "idx = {}\n",
    "for i in range(len(ids)):\n",
    "    idx[ids[i]] = i # TODO: for MATLAB set i+1 (i.e 1 to 200)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(x, win_size=10, hop_size=3):\n",
    "    r, c = x.shape\n",
    "    y = []\n",
    "    for i in range(0, c, hop_size):\n",
    "        if i + win_size > c:\n",
    "            break\n",
    "        y.append(x[:, i:i + win_size].T.flatten())\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "# mvn of test\n",
    "for speaker_id, feature_list in test.items():\n",
    "    speaker_id = idx[speaker_id]\n",
    "    for features in feature_list:\n",
    "        features = (features - features.mean())/features.std()\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            X_test.append(frame)\n",
    "            Y_test.append(speaker_id)\n",
    "            \n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "# smvn of train\n",
    "for speaker_id, feature_list in train.items():\n",
    "    speaker_id = idx[speaker_id]\n",
    "    \n",
    "    # calc speaker level mean and std\n",
    "    data = []\n",
    "    for features in feature_list:\n",
    "        frames = features.T\n",
    "        for frame in frames:\n",
    "            data.append(frame)\n",
    "    data = np.array(data)\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    \n",
    "    # speaker level normalize\n",
    "    for features in feature_list:\n",
    "        features = (features - mean)/std\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            X_train.append(frame)\n",
    "            Y_train.append(speaker_id)\n",
    "            \n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137520, 390) (36091, 390)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137520,) (36091,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, Y_train = shuffle(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import scipy.io as spio\n",
    "#spio.savemat('dataset.mat', dict(X=X_train, y=Y_train, X_test=X_test, Y_test=Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.80127068\n",
      "Iteration 2, loss = 3.98026314\n",
      "Iteration 3, loss = 3.48712526\n",
      "Iteration 4, loss = 3.18047052\n",
      "Iteration 5, loss = 2.95978835\n",
      "Iteration 6, loss = 2.78143188\n",
      "Iteration 7, loss = 2.63496096\n",
      "Iteration 8, loss = 2.50725815\n",
      "Iteration 9, loss = 2.39880453\n",
      "Iteration 10, loss = 2.30204915\n",
      "Iteration 11, loss = 2.21552836\n",
      "Iteration 12, loss = 2.13971171\n",
      "Iteration 13, loss = 2.06951789\n",
      "Iteration 14, loss = 2.00561584\n",
      "Iteration 15, loss = 1.94839300\n",
      "Iteration 16, loss = 1.89491911\n",
      "Iteration 17, loss = 1.84475175\n",
      "Iteration 18, loss = 1.79838973\n",
      "Iteration 19, loss = 1.75584822\n",
      "Iteration 20, loss = 1.71518395\n",
      "Iteration 21, loss = 1.67821373\n",
      "Iteration 22, loss = 1.64476982\n",
      "Iteration 23, loss = 1.61129456\n",
      "Iteration 24, loss = 1.58070895\n",
      "Iteration 25, loss = 1.55154822\n",
      "Iteration 26, loss = 1.52238422\n",
      "Iteration 27, loss = 1.49775597\n",
      "Iteration 28, loss = 1.47268344\n",
      "Iteration 29, loss = 1.44909761\n",
      "Iteration 30, loss = 1.42657174\n",
      "Iteration 31, loss = 1.40567778\n",
      "Iteration 32, loss = 1.38146926\n",
      "Iteration 33, loss = 1.36376681\n",
      "Iteration 34, loss = 1.34518149\n",
      "Iteration 35, loss = 1.32599198\n",
      "Iteration 36, loss = 1.30929454\n",
      "Iteration 37, loss = 1.29331371\n",
      "Iteration 38, loss = 1.27689363\n",
      "Iteration 39, loss = 1.26289227\n",
      "Iteration 40, loss = 1.24708079\n",
      "Iteration 41, loss = 1.23369426\n",
      "Iteration 42, loss = 1.21948922\n",
      "Iteration 43, loss = 1.20740779\n",
      "Iteration 44, loss = 1.19266841\n",
      "Iteration 45, loss = 1.17975324\n",
      "Iteration 46, loss = 1.16997214\n",
      "Iteration 47, loss = 1.15701530\n",
      "Iteration 48, loss = 1.14589528\n",
      "Iteration 49, loss = 1.13500076\n",
      "Iteration 50, loss = 1.12385270\n",
      "Iteration 51, loss = 1.11353038\n",
      "Iteration 52, loss = 1.10495753\n",
      "Iteration 53, loss = 1.09548695\n",
      "Iteration 54, loss = 1.08498170\n",
      "Iteration 55, loss = 1.07622699\n",
      "Iteration 56, loss = 1.06584557\n",
      "Iteration 57, loss = 1.05838727\n",
      "Iteration 58, loss = 1.05051916\n",
      "Iteration 59, loss = 1.04049338\n",
      "Iteration 60, loss = 1.03290473\n",
      "Iteration 61, loss = 1.02603009\n",
      "Iteration 62, loss = 1.01702031\n",
      "Iteration 63, loss = 1.00972423\n",
      "Iteration 64, loss = 1.00307845\n",
      "Iteration 65, loss = 0.99626393\n",
      "Iteration 66, loss = 0.98598038\n",
      "Iteration 67, loss = 0.98065264\n",
      "Iteration 68, loss = 0.97460430\n",
      "Iteration 69, loss = 0.96750201\n",
      "Iteration 70, loss = 0.96012603\n",
      "Iteration 71, loss = 0.95463989\n",
      "Iteration 72, loss = 0.94737790\n",
      "Iteration 73, loss = 0.94076721\n",
      "Iteration 74, loss = 0.93561798\n",
      "Iteration 75, loss = 0.92958307\n",
      "Iteration 76, loss = 0.92279975\n",
      "Iteration 77, loss = 0.91938487\n",
      "Iteration 78, loss = 0.91110231\n",
      "Iteration 79, loss = 0.90632767\n",
      "Iteration 80, loss = 0.90237364\n",
      "Iteration 81, loss = 0.89745500\n",
      "Iteration 82, loss = 0.89199890\n",
      "Iteration 83, loss = 0.88568482\n",
      "Iteration 84, loss = 0.88224190\n",
      "Iteration 85, loss = 0.87418007\n",
      "Iteration 86, loss = 0.87069738\n",
      "Iteration 87, loss = 0.86555305\n",
      "Iteration 88, loss = 0.86090319\n",
      "Iteration 89, loss = 0.85629248\n",
      "Iteration 90, loss = 0.85080765\n",
      "Iteration 91, loss = 0.84721972\n",
      "Iteration 92, loss = 0.84145029\n",
      "Iteration 93, loss = 0.83714156\n",
      "Iteration 94, loss = 0.83478565\n",
      "Iteration 95, loss = 0.82879807\n",
      "Iteration 96, loss = 0.82603718\n",
      "Iteration 97, loss = 0.82123774\n",
      "Iteration 98, loss = 0.81775628\n",
      "Iteration 99, loss = 0.81233919\n",
      "Iteration 100, loss = 0.80744836\n",
      "Iteration 101, loss = 0.80478114\n",
      "Iteration 102, loss = 0.79964613\n",
      "Iteration 103, loss = 0.79610981\n",
      "Iteration 104, loss = 0.79387763\n",
      "Iteration 105, loss = 0.78845527\n",
      "Iteration 106, loss = 0.78337204\n",
      "Iteration 107, loss = 0.78024473\n",
      "Iteration 108, loss = 0.77742274\n",
      "Iteration 109, loss = 0.77530389\n",
      "Iteration 110, loss = 0.76999958\n",
      "Iteration 111, loss = 0.76510069\n",
      "Iteration 112, loss = 0.76465179\n",
      "Iteration 113, loss = 0.76027168\n",
      "Iteration 114, loss = 0.75726132\n",
      "Iteration 115, loss = 0.75376144\n",
      "Iteration 116, loss = 0.74947782\n",
      "Iteration 117, loss = 0.74432698\n",
      "Iteration 118, loss = 0.74381042\n",
      "Iteration 119, loss = 0.74137586\n",
      "Iteration 120, loss = 0.73784842\n",
      "Iteration 121, loss = 0.73216513\n",
      "Iteration 122, loss = 0.73159651\n",
      "Iteration 123, loss = 0.72664737\n",
      "Iteration 124, loss = 0.72573898\n",
      "Iteration 125, loss = 0.72324154\n",
      "Iteration 126, loss = 0.72006759\n",
      "Iteration 127, loss = 0.71618393\n",
      "Iteration 128, loss = 0.71193644\n",
      "Iteration 129, loss = 0.70988841\n",
      "Iteration 130, loss = 0.70726704\n",
      "Iteration 131, loss = 0.70431496\n",
      "Iteration 132, loss = 0.70301590\n",
      "Iteration 133, loss = 0.70056209\n",
      "Iteration 134, loss = 0.69622553\n",
      "Iteration 135, loss = 0.69280184\n",
      "Iteration 136, loss = 0.69037388\n",
      "Iteration 137, loss = 0.68842217\n",
      "Iteration 138, loss = 0.68790980\n",
      "Iteration 139, loss = 0.68279316\n",
      "Iteration 140, loss = 0.68055589\n",
      "Iteration 141, loss = 0.67858676\n",
      "Iteration 142, loss = 0.67482063\n",
      "Iteration 143, loss = 0.67249231\n",
      "Iteration 144, loss = 0.66977899\n",
      "Iteration 145, loss = 0.66800221\n",
      "Iteration 146, loss = 0.66482623\n",
      "Iteration 147, loss = 0.66269408\n",
      "Iteration 148, loss = 0.66076994\n",
      "Iteration 149, loss = 0.65806417\n",
      "Iteration 150, loss = 0.65736312\n",
      "Iteration 151, loss = 0.65484711\n",
      "Iteration 152, loss = 0.65195495\n",
      "Iteration 153, loss = 0.64940750\n",
      "Iteration 154, loss = 0.64583952\n",
      "Iteration 155, loss = 0.64598316\n",
      "Iteration 156, loss = 0.64278630\n",
      "Iteration 157, loss = 0.64046166\n",
      "Iteration 158, loss = 0.63867970\n",
      "Iteration 159, loss = 0.63727767\n",
      "Iteration 160, loss = 0.63416713\n",
      "Iteration 161, loss = 0.63225495\n",
      "Iteration 162, loss = 0.63111855\n",
      "Iteration 163, loss = 0.62782637\n",
      "Iteration 164, loss = 0.62683703\n",
      "Iteration 165, loss = 0.62429654\n",
      "Iteration 166, loss = 0.62277789\n",
      "Iteration 167, loss = 0.61904937\n",
      "Iteration 168, loss = 0.61730358\n",
      "Iteration 169, loss = 0.61462367\n",
      "Iteration 170, loss = 0.61367604\n",
      "Iteration 171, loss = 0.61328023\n",
      "Iteration 172, loss = 0.61085908\n",
      "Iteration 173, loss = 0.60647644\n",
      "Iteration 174, loss = 0.60627073\n",
      "Iteration 175, loss = 0.60533420\n",
      "Iteration 176, loss = 0.60221167\n",
      "Iteration 177, loss = 0.60063336\n",
      "Iteration 178, loss = 0.59860432\n",
      "Iteration 179, loss = 0.59747163\n",
      "Iteration 180, loss = 0.59432597\n",
      "Iteration 181, loss = 0.59296961\n",
      "Iteration 182, loss = 0.59212872\n",
      "Iteration 183, loss = 0.58794972\n",
      "Iteration 184, loss = 0.58851548\n",
      "Iteration 185, loss = 0.58611674\n",
      "Iteration 186, loss = 0.58249982\n",
      "Iteration 187, loss = 0.58361013\n",
      "Iteration 188, loss = 0.58119522\n",
      "Iteration 189, loss = 0.57911482\n",
      "Iteration 190, loss = 0.57647179\n",
      "Iteration 191, loss = 0.57415181\n",
      "Iteration 192, loss = 0.57509412\n",
      "Iteration 193, loss = 0.57322512\n",
      "Iteration 194, loss = 0.57140611\n",
      "Iteration 195, loss = 0.56913047\n",
      "Iteration 196, loss = 0.56758547\n",
      "Iteration 197, loss = 0.56737227\n",
      "Iteration 198, loss = 0.56551725\n",
      "Iteration 199, loss = 0.56263769\n",
      "Iteration 200, loss = 0.56141245\n",
      "Iteration 201, loss = 0.55994857\n",
      "Iteration 202, loss = 0.55960414\n",
      "Iteration 203, loss = 0.55606174\n",
      "Iteration 204, loss = 0.55465701\n",
      "Iteration 205, loss = 0.55218026\n",
      "Iteration 206, loss = 0.55307340\n",
      "Iteration 207, loss = 0.55030986\n",
      "Iteration 208, loss = 0.54907932\n",
      "Iteration 209, loss = 0.54678150\n",
      "Iteration 210, loss = 0.54602496\n",
      "Iteration 211, loss = 0.54413174\n",
      "Iteration 212, loss = 0.54406621\n",
      "Iteration 213, loss = 0.54097262\n",
      "Iteration 214, loss = 0.53960262\n",
      "Iteration 215, loss = 0.53882908\n",
      "Iteration 216, loss = 0.53569812\n",
      "Iteration 217, loss = 0.53592337\n",
      "Iteration 218, loss = 0.53604608\n",
      "Iteration 219, loss = 0.53124501\n",
      "Iteration 220, loss = 0.53165413\n",
      "Iteration 221, loss = 0.53263193\n",
      "Iteration 222, loss = 0.52976149\n",
      "Iteration 223, loss = 0.52575183\n",
      "Iteration 224, loss = 0.52691746\n",
      "Iteration 225, loss = 0.52502311\n",
      "Iteration 226, loss = 0.52517706\n",
      "Iteration 227, loss = 0.52089076\n",
      "Iteration 228, loss = 0.51984697\n",
      "Iteration 229, loss = 0.51996616\n",
      "Iteration 230, loss = 0.51846031\n",
      "Iteration 231, loss = 0.51689725\n",
      "Iteration 232, loss = 0.51709488\n",
      "Iteration 233, loss = 0.51462146\n",
      "Iteration 234, loss = 0.51437094\n",
      "Iteration 235, loss = 0.51106517\n",
      "Iteration 236, loss = 0.51118143\n",
      "Iteration 237, loss = 0.50860658\n",
      "Iteration 238, loss = 0.50899583\n",
      "Iteration 239, loss = 0.50824438\n",
      "Iteration 240, loss = 0.50667287\n",
      "Iteration 241, loss = 0.50464255\n",
      "Iteration 242, loss = 0.50291016\n",
      "Iteration 243, loss = 0.50193874\n",
      "Iteration 244, loss = 0.50209082\n",
      "Iteration 245, loss = 0.49908265\n",
      "Iteration 246, loss = 0.49909365\n",
      "Iteration 247, loss = 0.49673663\n",
      "Iteration 248, loss = 0.49539609\n",
      "Iteration 249, loss = 0.49490764\n",
      "Iteration 250, loss = 0.49418907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pratik varshney\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.855432\n",
      "Test set score: 0.234518\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(200,), max_iter=250, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.01)\n",
    "\n",
    "mlp.fit(X_train, Y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, Y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.42084375\n",
      "Iteration 2, loss = 0.41735282\n",
      "Iteration 3, loss = 0.41710122\n",
      "Iteration 4, loss = 0.41580144\n",
      "Iteration 5, loss = 0.41462085\n",
      "Iteration 6, loss = 0.41449071\n",
      "Iteration 7, loss = 0.41279013\n",
      "Iteration 8, loss = 0.41316098\n",
      "Iteration 9, loss = 0.41211435\n",
      "Iteration 10, loss = 0.41102799\n",
      "Iteration 11, loss = 0.41045000\n",
      "Iteration 12, loss = 0.40972863\n",
      "Iteration 13, loss = 0.40917248\n",
      "Iteration 14, loss = 0.40845507\n",
      "Iteration 15, loss = 0.40751720\n",
      "Iteration 16, loss = 0.40703911\n",
      "Iteration 17, loss = 0.40587769\n",
      "Iteration 18, loss = 0.40584536\n",
      "Iteration 19, loss = 0.40546943\n",
      "Iteration 20, loss = 0.40436155\n",
      "Iteration 21, loss = 0.40371829\n",
      "Iteration 22, loss = 0.40340978\n",
      "Iteration 23, loss = 0.40292178\n",
      "Iteration 24, loss = 0.40235717\n",
      "Iteration 25, loss = 0.40191976\n",
      "Iteration 26, loss = 0.40017925\n",
      "Iteration 27, loss = 0.40077131\n",
      "Iteration 28, loss = 0.39950126\n",
      "Iteration 29, loss = 0.39855091\n",
      "Iteration 30, loss = 0.39896559\n",
      "Iteration 31, loss = 0.39883502\n",
      "Iteration 32, loss = 0.39787596\n",
      "Iteration 33, loss = 0.39692889\n",
      "Iteration 34, loss = 0.39653826\n",
      "Iteration 35, loss = 0.39543971\n",
      "Iteration 36, loss = 0.39547628\n",
      "Iteration 37, loss = 0.39448231\n",
      "Iteration 38, loss = 0.39460445\n",
      "Iteration 39, loss = 0.39427085\n",
      "Iteration 40, loss = 0.39375802\n",
      "Iteration 41, loss = 0.39304946\n",
      "Iteration 42, loss = 0.39237306\n",
      "Iteration 43, loss = 0.39149357\n",
      "Iteration 44, loss = 0.39134169\n",
      "Iteration 45, loss = 0.39169945\n",
      "Iteration 46, loss = 0.39042284\n",
      "Iteration 47, loss = 0.38993241\n",
      "Iteration 48, loss = 0.38886175\n",
      "Iteration 49, loss = 0.38886747\n",
      "Iteration 50, loss = 0.38816747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pratik varshney\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.895797\n",
      "Test set score: 0.237455\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(200,), max_iter=50, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.005, warm_start=True)\n",
    "\n",
    "# _initialize\n",
    "mlp2.n_iter_ = 0\n",
    "mlp2.t_ = 0\n",
    "mlp2.n_outputs_ = mlp.n_outputs_\n",
    "\n",
    "# Compute the number of layers\n",
    "mlp2.n_layers_ = mlp.n_layers_\n",
    "\n",
    "# Output for multi class\n",
    "mlp2.out_activation_ = mlp.out_activation_\n",
    "\n",
    "# Initialize coefficient and intercept layers\n",
    "mlp2.coefs_ = mlp.coefs_\n",
    "mlp2.intercepts_ = mlp.intercepts_\n",
    "\n",
    "# self.coefs_ = []\n",
    "# self.intercepts_ = []\n",
    "\n",
    "# for i in range(self.n_layers_ - 1):\n",
    "#     coef_init, intercept_init = self._init_coef(layer_units[i],\n",
    "#                                                 layer_units[i + 1])\n",
    "#     self.coefs_.append(coef_init)\n",
    "#     self.intercepts_.append(intercept_init)\n",
    "\n",
    "\n",
    "mlp2.loss_curve_ = []\n",
    "mlp2._no_improvement_count = 0\n",
    "if mlp2.early_stopping:\n",
    "    mlp2.validation_scores_ = []\n",
    "    mlp2.best_validation_score_ = -np.inf\n",
    "else:\n",
    "    mlp2.best_loss_ = np.inf\n",
    "\n",
    "mlp2.fit(X_train, Y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, Y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38710545\n",
      "Iteration 2, loss = 0.38660115\n",
      "Iteration 3, loss = 0.38718149\n",
      "Iteration 4, loss = 0.38643883\n",
      "Iteration 5, loss = 0.38527870\n",
      "Iteration 6, loss = 0.38561249\n",
      "Iteration 7, loss = 0.38436126\n",
      "Iteration 8, loss = 0.38478164\n",
      "Iteration 9, loss = 0.38437101\n",
      "Iteration 10, loss = 0.38372001\n",
      "Iteration 11, loss = 0.38309909\n",
      "Iteration 12, loss = 0.38263756\n",
      "Iteration 13, loss = 0.38233116\n",
      "Iteration 14, loss = 0.38170075\n",
      "Iteration 15, loss = 0.38102729\n",
      "Iteration 16, loss = 0.38066330\n",
      "Iteration 17, loss = 0.37970144\n",
      "Iteration 18, loss = 0.37987970\n",
      "Iteration 19, loss = 0.37953173\n",
      "Iteration 20, loss = 0.37848023\n",
      "Iteration 21, loss = 0.37801483\n",
      "Iteration 22, loss = 0.37789394\n",
      "Iteration 23, loss = 0.37761946\n",
      "Iteration 24, loss = 0.37703934\n",
      "Iteration 25, loss = 0.37677159\n",
      "Iteration 26, loss = 0.37496969\n",
      "Iteration 27, loss = 0.37581677\n",
      "Iteration 28, loss = 0.37454357\n",
      "Iteration 29, loss = 0.37367511\n",
      "Iteration 30, loss = 0.37406914\n",
      "Iteration 31, loss = 0.37440652\n",
      "Iteration 32, loss = 0.37340866\n",
      "Iteration 33, loss = 0.37264480\n",
      "Iteration 34, loss = 0.37202525\n",
      "Iteration 35, loss = 0.37126436\n",
      "Iteration 36, loss = 0.37131323\n",
      "Iteration 37, loss = 0.37033673\n",
      "Iteration 38, loss = 0.37055854\n",
      "Iteration 39, loss = 0.37042470\n",
      "Iteration 40, loss = 0.36991678\n",
      "Iteration 41, loss = 0.36926006\n",
      "Iteration 42, loss = 0.36871021\n",
      "Iteration 43, loss = 0.36779066\n",
      "Iteration 44, loss = 0.36771087\n",
      "Iteration 45, loss = 0.36832870\n",
      "Iteration 46, loss = 0.36699848\n",
      "Iteration 47, loss = 0.36661161\n",
      "Iteration 48, loss = 0.36544008\n",
      "Iteration 49, loss = 0.36568946\n",
      "Iteration 50, loss = 0.36514603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pratik varshney\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.902501\n",
      "Test set score: 0.236208\n"
     ]
    }
   ],
   "source": [
    "mlp=mlp2\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(200,), max_iter=50, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.005, warm_start=True, learning_rate='adaptive')\n",
    "\n",
    "# _initialize\n",
    "mlp2.n_iter_ = 0\n",
    "mlp2.t_ = 0\n",
    "mlp2.n_outputs_ = mlp.n_outputs_\n",
    "\n",
    "# Compute the number of layers\n",
    "mlp2.n_layers_ = mlp.n_layers_\n",
    "\n",
    "# Output for multi class\n",
    "mlp2.out_activation_ = mlp.out_activation_\n",
    "\n",
    "# Initialize coefficient and intercept layers\n",
    "mlp2.coefs_ = mlp.coefs_\n",
    "mlp2.intercepts_ = mlp.intercepts_\n",
    "\n",
    "# self.coefs_ = []\n",
    "# self.intercepts_ = []\n",
    "\n",
    "# for i in range(self.n_layers_ - 1):\n",
    "#     coef_init, intercept_init = self._init_coef(layer_units[i],\n",
    "#                                                 layer_units[i + 1])\n",
    "#     self.coefs_.append(coef_init)\n",
    "#     self.intercepts_.append(intercept_init)\n",
    "\n",
    "\n",
    "mlp2.loss_curve_ = []\n",
    "mlp2._no_improvement_count = 0\n",
    "if mlp2.early_stopping:\n",
    "    mlp2.validation_scores_ = []\n",
    "    mlp2.best_validation_score_ = -np.inf\n",
    "else:\n",
    "    mlp2.best_loss_ = np.inf\n",
    "\n",
    "mlp2.fit(X_train, Y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, Y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.36426878\n",
      "Iteration 2, loss = 0.36380004\n",
      "Iteration 3, loss = 0.36429859\n",
      "Iteration 4, loss = 0.36354746\n",
      "Iteration 5, loss = 0.36247448\n",
      "Iteration 6, loss = 0.36287318\n",
      "Iteration 7, loss = 0.36173778\n",
      "Iteration 8, loss = 0.36203876\n",
      "Iteration 9, loss = 0.36174762\n",
      "Iteration 10, loss = 0.36136617\n",
      "Iteration 11, loss = 0.36059942\n",
      "Iteration 12, loss = 0.36029864\n",
      "Iteration 13, loss = 0.36022216\n",
      "Iteration 14, loss = 0.35946373\n",
      "Iteration 15, loss = 0.35896693\n",
      "Iteration 16, loss = 0.35858001\n",
      "Iteration 17, loss = 0.35767423\n",
      "Iteration 18, loss = 0.35805066\n",
      "Iteration 19, loss = 0.35756295\n",
      "Iteration 20, loss = 0.35672987\n",
      "Iteration 21, loss = 0.35616548\n",
      "Iteration 22, loss = 0.35623397\n",
      "Iteration 23, loss = 0.35595185\n",
      "Iteration 24, loss = 0.35545152\n",
      "Iteration 25, loss = 0.35524096\n",
      "Iteration 26, loss = 0.35351674\n",
      "Iteration 27, loss = 0.35446743\n",
      "Iteration 28, loss = 0.35316783\n",
      "Iteration 29, loss = 0.35239027\n",
      "Iteration 30, loss = 0.35269911\n",
      "Iteration 31, loss = 0.35324319\n",
      "Iteration 32, loss = 0.35231710\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.001000\n",
      "Iteration 33, loss = 0.31217694\n",
      "Iteration 34, loss = 0.30752505\n",
      "Iteration 35, loss = 0.30683490\n",
      "Iteration 36, loss = 0.30670219\n",
      "Iteration 37, loss = 0.30636573\n",
      "Iteration 38, loss = 0.30652946\n",
      "Iteration 39, loss = 0.30630021\n",
      "Iteration 40, loss = 0.30606112\n",
      "Iteration 41, loss = 0.30598922\n",
      "Iteration 42, loss = 0.30560204\n",
      "Iteration 43, loss = 0.30554053\n",
      "Iteration 44, loss = 0.30538025\n",
      "Iteration 45, loss = 0.30575397\n",
      "Iteration 46, loss = 0.30525759\n",
      "Iteration 47, loss = 0.30533043\n",
      "Iteration 48, loss = 0.30503748\n",
      "Iteration 49, loss = 0.30499745\n",
      "Iteration 50, loss = 0.30477510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pratik varshney\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.924840\n",
      "Test set score: 0.234324\n"
     ]
    }
   ],
   "source": [
    "mlp=mlp2\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(200,), max_iter=50, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.005, warm_start=True, learning_rate='adaptive')\n",
    "\n",
    "# _initialize\n",
    "mlp2.n_iter_ = 0\n",
    "mlp2.t_ = 0\n",
    "mlp2.n_outputs_ = mlp.n_outputs_\n",
    "\n",
    "# Compute the number of layers\n",
    "mlp2.n_layers_ = mlp.n_layers_\n",
    "\n",
    "# Output for multi class\n",
    "mlp2.out_activation_ = mlp.out_activation_\n",
    "\n",
    "# Initialize coefficient and intercept layers\n",
    "mlp2.coefs_ = mlp.coefs_\n",
    "mlp2.intercepts_ = mlp.intercepts_\n",
    "\n",
    "# self.coefs_ = []\n",
    "# self.intercepts_ = []\n",
    "\n",
    "# for i in range(self.n_layers_ - 1):\n",
    "#     coef_init, intercept_init = self._init_coef(layer_units[i],\n",
    "#                                                 layer_units[i + 1])\n",
    "#     self.coefs_.append(coef_init)\n",
    "#     self.intercepts_.append(intercept_init)\n",
    "\n",
    "\n",
    "mlp2.loss_curve_ = []\n",
    "mlp2._no_improvement_count = 0\n",
    "if mlp2.early_stopping:\n",
    "    mlp2.validation_scores_ = []\n",
    "    mlp2.best_validation_score_ = -np.inf\n",
    "else:\n",
    "    mlp2.best_loss_ = np.inf\n",
    "\n",
    "mlp2.fit(X_train, Y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, Y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.29884172\n",
      "Iteration 2, loss = 0.29871739\n",
      "Iteration 3, loss = 0.29892879\n",
      "Iteration 4, loss = 0.29863202\n",
      "Iteration 5, loss = 0.29831857\n",
      "Iteration 6, loss = 0.29841417\n",
      "Iteration 7, loss = 0.29830743\n",
      "Iteration 8, loss = 0.29849440\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 9, loss = 0.29061435\n",
      "Iteration 10, loss = 0.28961098\n",
      "Iteration 11, loss = 0.28951599\n",
      "Iteration 12, loss = 0.28939727\n",
      "Iteration 13, loss = 0.28950599\n",
      "Iteration 14, loss = 0.28938731\n",
      "Iteration 15, loss = 0.28936947\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 16, loss = 0.28775280\n",
      "Iteration 17, loss = 0.28749348\n",
      "Iteration 18, loss = 0.28745869\n",
      "Iteration 19, loss = 0.28744719\n",
      "Iteration 20, loss = 0.28742563\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 21, loss = 0.28700502\n",
      "Iteration 22, loss = 0.28697422\n",
      "Iteration 23, loss = 0.28696112\n",
      "Iteration 24, loss = 0.28695886\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 25, loss = 0.28685448\n",
      "Iteration 26, loss = 0.28685432\n",
      "Iteration 27, loss = 0.28685296\n",
      "Iteration 28, loss = 0.28685262\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 29, loss = 0.28683055\n",
      "Iteration 30, loss = 0.28683054\n",
      "Iteration 31, loss = 0.28683049\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.929559\n",
      "Test set score: 0.235322\n"
     ]
    }
   ],
   "source": [
    "mlp=mlp2\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(200,), max_iter=50, alpha=1e-5,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.001, warm_start=True, learning_rate='adaptive')\n",
    "\n",
    "# _initialize\n",
    "mlp2.n_iter_ = 0\n",
    "mlp2.t_ = 0\n",
    "mlp2.n_outputs_ = mlp.n_outputs_\n",
    "\n",
    "# Compute the number of layers\n",
    "mlp2.n_layers_ = mlp.n_layers_\n",
    "\n",
    "# Output for multi class\n",
    "mlp2.out_activation_ = mlp.out_activation_\n",
    "\n",
    "# Initialize coefficient and intercept layers\n",
    "mlp2.coefs_ = mlp.coefs_\n",
    "mlp2.intercepts_ = mlp.intercepts_\n",
    "\n",
    "# self.coefs_ = []\n",
    "# self.intercepts_ = []\n",
    "\n",
    "# for i in range(self.n_layers_ - 1):\n",
    "#     coef_init, intercept_init = self._init_coef(layer_units[i],\n",
    "#                                                 layer_units[i + 1])\n",
    "#     self.coefs_.append(coef_init)\n",
    "#     self.intercepts_.append(intercept_init)\n",
    "\n",
    "\n",
    "mlp2.loss_curve_ = []\n",
    "mlp2._no_improvement_count = 0\n",
    "if mlp2.early_stopping:\n",
    "    mlp2.validation_scores_ = []\n",
    "    mlp2.best_validation_score_ = -np.inf\n",
    "else:\n",
    "    mlp2.best_loss_ = np.inf\n",
    "\n",
    "mlp2.fit(X_train, Y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, Y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nesterovs_momentum': True, 'tol': 0.0001, 'hidden_layer_sizes': (200,), 'random_state': 1, 'power_t': 0.5, 'shuffle': True, 'verbose': 10, 'epsilon': 1e-08, 'activation': 'relu', 'batch_size': 'auto', 'alpha': 0.0001, 'beta_1': 0.9, 'max_iter': 300, 'early_stopping': False, 'beta_2': 0.999, 'momentum': 0.9, 'learning_rate_init': 0.005, 'learning_rate': 'constant', 'solver': 'sgd', 'warm_start': True, 'validation_fraction': 0.1}\n"
     ]
    }
   ],
   "source": [
    "mlp.set_params(**{'warm_start': True, 'max_iter': 300})\n",
    "print(mlp.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.65834327\n",
      "Training set score: 0.800945\n",
      "Test set score: 0.249009\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(X_train, Y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, Y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('coefs', mlp2.coefs_)\n",
    "np.save('intercepts', mlp2.intercepts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199]\n",
      "0.49418907089\n",
      "[array([[-0.11212256,  0.16968417, -0.09096973, ..., -0.16198049,\n",
      "         0.06640543,  0.84388988],\n",
      "       [ 0.09123717,  0.07599923, -0.02663023, ..., -0.05712243,\n",
      "         0.11257236,  0.04792394],\n",
      "       [-0.25804324,  0.28772547, -0.17412223, ..., -0.04568428,\n",
      "         0.08411288,  0.27393113],\n",
      "       ..., \n",
      "       [-0.04489176,  0.23786616,  0.01568241, ..., -0.11556246,\n",
      "        -0.04457289,  0.78679717],\n",
      "       [-0.10414749,  0.09744343,  0.10603963, ..., -0.21462791,\n",
      "         0.04596479,  0.84709752],\n",
      "       [ 0.01339073,  0.11155119, -0.01885031, ..., -0.10841772,\n",
      "         0.1008605 ,  0.80057599]]), array([[-0.20234689, -0.53068349,  0.72276338, ...,  0.0636864 ,\n",
      "         0.3591263 , -0.43056686],\n",
      "       [-0.2383202 ,  0.00816391, -0.01899075, ...,  0.06411579,\n",
      "         0.16070081,  0.29673068],\n",
      "       [-0.03550162,  0.27021603,  0.06196519, ...,  0.04811321,\n",
      "         0.01104018, -0.14219624],\n",
      "       ..., \n",
      "       [-0.36521715, -0.05932961,  0.73841134, ...,  0.66690026,\n",
      "         0.10042348, -0.27647816],\n",
      "       [ 0.07502434,  0.15170883,  0.0581903 , ...,  0.20490779,\n",
      "        -0.2541497 ,  0.51318288],\n",
      "       [ 0.08669367, -0.55089655, -0.14894117, ..., -0.41453203,\n",
      "        -0.3387822 , -0.56008376]])]\n",
      "[array([ -1.15238730e+00,  -1.57257876e+00,  -1.06291407e+00,\n",
      "        -8.29222582e-01,  -5.07925480e-01,   1.29997094e-01,\n",
      "         1.97785491e+00,   5.79641473e-01,  -3.16999905e-01,\n",
      "         1.78482023e+00,   1.01994147e+00,  -3.45804480e-03,\n",
      "         1.55259764e+00,  -2.56129394e-01,   6.53762282e-01,\n",
      "         3.84878620e-01,  -2.88132646e-01,   5.66999459e+00,\n",
      "         3.74915574e+00,  -4.95653984e-01,   7.18752430e-01,\n",
      "        -8.65996972e-01,   2.26575128e+00,   3.11149427e-01,\n",
      "         3.75187967e+00,  -1.24407619e+00,   1.15871563e+00,\n",
      "         1.76181093e+00,   3.12679434e+00,   1.52891089e+00,\n",
      "        -3.05668937e-01,   2.90023142e-01,   3.62406161e-02,\n",
      "        -3.08343493e-01,  -1.88813638e-02,   4.01339361e+00,\n",
      "         6.43596271e-01,   1.01349726e+00,  -2.41360398e-01,\n",
      "         1.34859466e+00,   5.52949748e-01,   2.42685824e-01,\n",
      "        -8.41978365e-01,  -3.57629594e+00,  -1.36227714e+00,\n",
      "        -1.82504702e-01,  -4.25946637e-01,   7.16580613e-01,\n",
      "         2.09874972e+00,   9.67053049e-02,  -1.35342803e-01,\n",
      "        -3.33218935e+00,   8.35418749e-01,   3.13969157e+00,\n",
      "        -1.57171236e+00,   1.66594586e-01,   2.04733854e+00,\n",
      "         6.00778791e+00,  -8.80702730e-01,   3.72682306e-01,\n",
      "        -2.63874883e+00,  -2.59557483e-01,  -2.20791783e+00,\n",
      "         6.38111483e+00,  -5.14558306e-01,  -1.41570960e-01,\n",
      "         3.48709113e+00,   8.89937423e-01,   3.99049867e-01,\n",
      "         5.52219511e-01,   1.23991649e+00,   2.43927925e+00,\n",
      "        -8.68728184e-01,   6.37663026e-02,   2.75274615e-01,\n",
      "        -9.52179907e-01,  -3.29825148e-01,  -8.67357185e-01,\n",
      "         1.91583437e+00,   9.01796159e-01,   1.06283086e+00,\n",
      "        -4.66936788e-02,   9.90919806e-01,  -2.34741510e-01,\n",
      "        -1.32531309e+00,  -4.97287961e-01,   1.96970060e-01,\n",
      "         1.48371553e+00,  -8.10309718e-01,   3.68847781e-01,\n",
      "         1.79054971e+00,   4.77828113e+00,   1.13786920e+00,\n",
      "         1.52178568e+00,   1.27769704e-01,   1.54190904e-02,\n",
      "         8.72596641e-01,   8.07785733e-02,  -8.72502776e-01,\n",
      "         4.22669090e-01,   2.16000918e-02,   7.79340853e+00,\n",
      "         1.31711305e+00,   7.22356751e-01,   1.89859167e+00,\n",
      "         2.99890445e-01,   6.25381192e-01,   6.65157558e-01,\n",
      "        -1.01893835e+00,   3.06604228e+00,   3.00818715e-01,\n",
      "        -1.47016841e+00,   1.02608637e+00,   5.80777025e-01,\n",
      "         1.35087603e-01,  -8.35285884e-02,  -5.78999440e-01,\n",
      "        -2.29415224e-01,   1.10988741e-01,   2.41366791e+00,\n",
      "         2.21016239e+00,   3.06041176e-01,  -2.09011658e-01,\n",
      "         6.85834602e-02,  -2.56258238e-01,   8.01430164e+00,\n",
      "        -2.67203975e-01,  -2.35286770e+00,  -5.46076138e-02,\n",
      "         1.60460370e+01,   1.96070781e+00,   1.62816054e+00,\n",
      "         1.68720738e-01,  -5.46983138e-01,   1.27155895e+01,\n",
      "         9.68243255e-01,   1.09215253e+00,   2.09903332e+00,\n",
      "        -7.74594238e-01,   3.56237414e-01,   1.38393240e+00,\n",
      "         1.98128481e-01,   6.77239559e-01,  -4.02625051e-01,\n",
      "         9.00617534e+00,  -1.54748093e+00,   1.10596459e+00,\n",
      "        -4.37283420e-01,   1.18488930e+00,  -4.48377723e-01,\n",
      "        -6.62220733e-01,   9.60072084e-01,   1.50957915e+00,\n",
      "         8.08196843e+00,  -3.14193617e-01,  -2.95303393e+00,\n",
      "         4.12141120e+00,   3.38439490e-01,   1.43087364e+00,\n",
      "         2.94724597e-01,  -1.31518679e-01,  -4.08238986e-01,\n",
      "         2.03225143e+00,   9.41014925e-01,   3.54713866e-01,\n",
      "        -1.66488980e+00,   3.60287239e-01,  -1.02451589e+00,\n",
      "         3.53620249e-01,   4.94834840e-01,   7.64066457e-01,\n",
      "        -8.31428024e-02,  -6.79416825e-01,   1.26990933e+00,\n",
      "         5.75410506e-01,  -1.20347208e+00,  -7.89703824e-01,\n",
      "         4.85497818e-01,  -8.69334152e-01,   1.35458447e+00,\n",
      "         1.90008134e+00,   1.89581569e+00,   3.66053224e-02,\n",
      "        -4.36691528e-01,   8.15466702e-01,  -2.22900987e+00,\n",
      "         6.01191988e-01,   3.47139901e-01,  -1.34264250e+00,\n",
      "         3.94926862e-01,   7.04766608e-01,   2.82700014e+00,\n",
      "        -5.11748570e-01,  -1.08072829e+00,  -9.33047715e-01,\n",
      "         1.55166844e-02,   1.02158840e+00,  -6.06582604e-01,\n",
      "        -2.16971862e-02,  -1.45489078e-02]), array([-0.01181229,  0.417233  ,  0.21112744,  0.14529505,  0.40366236,\n",
      "        0.05903866, -0.02412021, -0.177562  , -0.30463869,  0.00789195,\n",
      "       -0.66116212, -0.56599174,  0.34737229,  0.21767792,  0.16479324,\n",
      "        0.39540201,  0.12877047,  0.01342738, -0.2815421 , -0.30423566,\n",
      "       -0.15023101,  0.31485806, -0.10608827,  0.15788263,  0.13214414,\n",
      "        0.06315831,  0.20491365, -0.56798332, -0.36788716,  0.09227531,\n",
      "       -1.022865  ,  0.03444801,  0.12530524, -0.15179835, -0.01274955,\n",
      "       -0.53166796, -0.24963124,  0.28605883, -0.51268887,  0.33791782,\n",
      "       -0.12902948, -0.01519836, -0.33057106,  0.10015729,  0.41658242,\n",
      "        0.14809734, -0.3105027 , -0.18626804,  0.09335161,  0.03496882,\n",
      "       -0.00363687, -0.81706557,  0.19037612, -0.08060748, -0.0449793 ,\n",
      "       -0.28470136, -0.18952548, -0.29387778,  0.09141575,  0.04659689,\n",
      "        0.04646203,  0.20660143,  0.07349853,  0.09455966,  0.23779961,\n",
      "        0.37778674,  0.18331714,  0.56461195,  0.24193339,  0.18103333,\n",
      "       -0.51281404,  0.25921145,  0.11990054,  0.11788521,  0.29201141,\n",
      "        0.15516245,  0.12336329,  0.36554257,  0.27380021,  0.11957009,\n",
      "        0.02397763, -0.43250874, -0.60379693,  0.23931358, -0.02060176,\n",
      "        0.05410061,  0.1425142 ,  0.06673969, -0.40552357, -0.09980321,\n",
      "       -0.54664898,  0.13060231,  0.40435495, -1.24249156,  0.07071554,\n",
      "       -0.12925891, -0.65573011, -1.01206937,  0.28893848, -0.32496247,\n",
      "       -0.09467457,  0.17912841, -0.11133951,  0.50195151,  0.33258102,\n",
      "        0.1946982 ,  0.37815768,  0.31028316,  0.11609869, -0.29650878,\n",
      "        0.20319925, -0.0215983 , -0.6090836 ,  0.02914436,  0.18355201,\n",
      "        0.29908914,  0.07051593, -0.24578887,  0.31420496,  0.24956823,\n",
      "        0.20938275, -0.58417197,  0.23438534,  0.00796224, -0.16626693,\n",
      "        0.26881072,  0.04577591, -0.07912546,  0.10603966, -0.06431822,\n",
      "       -0.40366068, -0.14668774, -0.07375751, -0.06628612,  0.14317299,\n",
      "       -0.07034029, -0.09651847, -0.11232065,  0.28403595,  0.17255527,\n",
      "        0.27281095,  0.10505877,  0.24831771,  0.35238475,  0.03162183,\n",
      "       -0.98495052, -0.0504379 , -0.40573368,  0.1563027 ,  0.51056245,\n",
      "        0.32368829, -0.18469818, -0.25775272, -0.40537835,  0.43182867,\n",
      "       -0.16570331,  0.23336134,  0.14513479,  0.03359473, -0.12575686,\n",
      "       -1.0239629 , -0.14898168,  0.0708282 ,  0.18852587, -0.13706534,\n",
      "        0.32434363,  0.35597811, -0.10697908,  0.07804698,  0.15090323,\n",
      "        0.04995973, -0.02186085, -0.11751663,  0.20264237, -0.09587558,\n",
      "        0.08697183,  0.11184819,  0.07991453, -0.21286345,  0.05675852,\n",
      "       -0.27328812,  0.17216896,  0.35856758,  0.01001549, -0.34542866,\n",
      "        0.08895333,  0.03714458,  0.32613059,  0.1774819 ,  0.60236829,\n",
      "        0.14684224,  0.1980158 ,  0.22963831,  0.29727285,  0.15125917,\n",
      "       -0.31825402,  0.1328708 , -0.02731997, -0.06099604,  0.10913559])]\n",
      "250\n",
      "3\n",
      "200\n",
      "softmax\n"
     ]
    }
   ],
   "source": [
    "print(mlp.classes_)\n",
    "print(mlp.loss_)\n",
    "print(mlp.coefs_) # weights\n",
    "print(mlp.intercepts_) # biases\n",
    "print(mlp.n_iter_)\n",
    "print(mlp.n_layers_)\n",
    "print(mlp.n_outputs_)\n",
    "print(mlp.out_activation_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.117730\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set score: %f\" % mlp.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as spio\n",
    "spio.savemat('dataset.mat', dict(X=X_train, y=Y_train, X_test=X_test, Y_test=Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_old = X_test\n",
    "Y_test_old = Y_test\n",
    "X_test = []\n",
    "Y_test = []\n",
    "# X_train = []\n",
    "# Y_train = []\n",
    "\n",
    "# smvn of train\n",
    "for speaker_id_str, feature_list in train.items():\n",
    "    speaker_id = idx[speaker_id_str]\n",
    "    \n",
    "    # calc speaker level mean and std\n",
    "    data = []\n",
    "    for features in feature_list:\n",
    "        frames = features.T\n",
    "        for frame in frames:\n",
    "            data.append(frame)\n",
    "    # test data\n",
    "    for features in test[speaker_id_str]:\n",
    "        frames = features.T\n",
    "        for frame in frames:\n",
    "            data.append(frame)\n",
    "    \n",
    "    data = np.array(data)\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    \n",
    "    # speaker level normalize\n",
    "#     for features in feature_list:\n",
    "#         features = (features - mean)/std\n",
    "#         frames = concat(features)\n",
    "#         for frame in frames:\n",
    "#             X_train.append(frame)\n",
    "#             Y_train.append(speaker_id)\n",
    "            \n",
    "    # test\n",
    "    for features in test[speaker_id_str]:\n",
    "        features = (features - mean)/std\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            X_test.append(frame)\n",
    "            Y_test.append(speaker_id)\n",
    "            \n",
    "# X_train = np.array(X_train)\n",
    "# Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.561193\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set score: %f\" % mlp2.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.042033\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "# X_train = []\n",
    "# Y_train = []\n",
    "\n",
    "# global mvn\n",
    "data = []\n",
    "for speaker_id_str, feature_list in train.items():\n",
    "    speaker_id = idx[speaker_id_str]\n",
    "    \n",
    "    # calc speaker level mean and std\n",
    "    \n",
    "    for features in feature_list:\n",
    "        frames = features.T\n",
    "        for frame in frames:\n",
    "            data.append(frame)\n",
    "    # test data\n",
    "    for features in test[speaker_id_str]:\n",
    "        frames = features.T\n",
    "        for frame in frames:\n",
    "            data.append(frame)\n",
    "    \n",
    "data = np.array(data)\n",
    "mean = data.mean()\n",
    "std = data.std()\n",
    "    \n",
    "    # speaker level normalize\n",
    "#     for features in feature_list:\n",
    "#         features = (features - mean)/std\n",
    "#         frames = concat(features)\n",
    "#         for frame in frames:\n",
    "#             X_train.append(frame)\n",
    "#             Y_train.append(speaker_id)\n",
    "            \n",
    "    # test\n",
    "for speaker_id_str, feature_list in test.items():\n",
    "    speaker_id = idx[speaker_id_str]\n",
    "    for features in test[speaker_id_str]:\n",
    "        features = (features - mean)/std\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            X_test.append(frame)\n",
    "            Y_test.append(speaker_id)\n",
    "            \n",
    "# X_train = np.array(X_train)\n",
    "# Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
