{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.3.1\n",
      "2.000000 * 3.000000 = 6.000000\n",
      "Loading data...\n",
      "ids {'MCSS0': 21, 'MDLC2': 36, 'MMSM0': 129, 'MDSS0': 44, 'MTAS0': 177, 'MDBP0': 26, 'MKAM0': 103, 'MDEF0': 29, 'MRTJ0': 167, 'MMJB1': 127, 'MGSH0': 65, 'MAKR0': 3, 'MRCW0': 146, 'MTPP0': 188, 'MMRP0': 128, 'MMAG0': 117, 'MBJV0': 9, 'MJWT0': 99, 'MJEB0': 79, 'MSRG0': 175, 'MRJH0': 157, 'MDPK0': 41, 'MMXS0': 130, 'MTRC0': 190, 'MTQC0': 189, 'MRDS0': 148, 'MTJG0': 181, 'MEFG0': 51, 'MESG0': 53, 'MADC0': 0, 'MPGH0': 133, 'MPRD0': 137, 'MCLM0': 19, 'MPGR0': 134, 'MGXP0': 66, 'MJXL0': 100, 'MJRP0': 96, 'MDAS0': 24, 'MRSO0': 164, 'MLSH0': 115, 'MJJJ0': 84, 'MCEW0': 17, 'MJDC0': 77, 'MWSB0': 198, 'MBGT0': 8, 'MVJH0': 193, 'MKDT0': 104, 'MDLC0': 35, 'MTJM0': 182, 'MJEB1': 80, 'MDMA0': 38, 'MJMA0': 89, 'MDMT0': 39, 'MRSP0': 165, 'MMGG0': 125, 'MRAB0': 141, 'MLNS0': 114, 'MSMS0': 174, 'MRJB1': 156, 'MJPM0': 92, 'MAPV0': 4, 'MCDD0': 14, 'MRLJ0': 161, 'MCTM0': 22, 'MSMC0': 173, 'MTKP0': 184, 'MJMM0': 91, 'MRAB1': 142, 'MTPF0': 186, 'MEGJ0': 52, 'MMGC0': 124, 'MRJM1': 159, 'MRWA0': 168, 'MJLS0': 88, 'MDJM0': 33, 'MHIT0': 67, 'MHRM0': 71, 'MFRM0': 57, 'MMGK0': 126, 'MRGS0': 154, 'MTPG0': 187, 'MRTC0': 166, 'MDHS0': 32, 'MBEF0': 7, 'MRFL0': 152, 'MRJT0': 160, 'MCEF0': 16, 'MSFV0': 172, 'MLJH0': 113, 'MDHL0': 31, 'MDWM0': 49, 'MGRP0': 64, 'MJMD0': 90, 'MKLS1': 107, 'MNET0': 131, 'MAEB0': 1, 'MMEB0': 123, 'MJPM1': 93, 'MGRL0': 63, 'MJDA0': 76, 'MJKR0': 85, 'MTRT0': 192, 'MDLH0': 37, 'MKLS0': 106, 'MAKB0': 2, 'MJWS0': 98, 'MMAR0': 119, 'MDLB0': 34, 'MJRH1': 95, 'MDSS1': 45, 'MJEE0': 81, 'MHMG0': 69, 'MCHL0': 18, 'MARW0': 6, 'MFMC0': 56, 'MWAR0': 195, 'MFWK0': 58, 'MTRR0': 191, 'MCPM0': 20, 'MSAT0': 170, 'MGJC0': 62, 'MSFH0': 171, 'MJJB0': 83, 'MTLB0': 185, 'MREH1': 150, 'MJHI0': 82, 'MDWH0': 48, 'MDPS0': 42, 'MKAJ0': 102, 'MEWM0': 54, 'MDBB1': 25, 'MDTB0': 46, 'MGES0': 61, 'MRGM0': 153, 'MWGR0': 197, 'MJLB0': 86, 'MKLW0': 108, 'MARC0': 5, 'MHJB0': 68, 'MPPC0': 135, 'MPEB0': 132, 'MDSJ0': 43, 'MCDR0': 15, 'MTBC0': 179, 'MTDB0': 180, 'MWDK0': 196, 'MMAM0': 118, 'MMBS0': 120, 'MWAD0': 194, 'MMDS0': 122, 'MDWD0': 47, 'MHMR0': 70, 'MRCG0': 145, 'MSTF0': 176, 'MKXL0': 109, 'MJAC0': 73, 'MILB0': 72, 'MPRT0': 139, 'MCDC0': 13, 'MRMS0': 163, 'MTJS0': 183, 'MRJM0': 158, 'MMDM0': 121, 'MRAI0': 143, 'MBMA0': 10, 'MLBC0': 110, 'MDCD0': 27, 'MJDE0': 78, 'MTAT1': 178, 'MGAF0': 59, 'MREE0': 149, 'MJAE0': 74, 'MDEM0': 30, 'MPSW0': 140, 'MKAH0': 101, 'MPRK0': 138, 'MEDR0': 50, 'MCAL0': 12, 'MJRH0': 94, 'MLJC0': 112, 'MBWP0': 11, 'MRWS0': 169, 'MRDD0': 147, 'MKJO0': 105, 'MLEL0': 111, 'MPRB0': 136, 'MDDC0': 28, 'MGAG0': 60, 'MRLR0': 162, 'MZMB0': 199, 'MRFK0': 151, 'MRBC0': 144, 'MJLG1': 87, 'MJBG0': 75, 'MFER0': 55, 'MDNS0': 40, 'MDAC0': 23, 'MRHL0': 155, 'MJSR0': 97, 'MMAA0': 116}\n",
      "\n",
      "# Train : 67872\n",
      "# Eval  : 23577\n",
      "# Test  : 26009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "# Add whatever you want\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io as spio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Test tensorflow\n",
    "print('TensorFlow version: ' + tf.__version__)\n",
    "a = tf.constant(2.0)\n",
    "b = tf.constant(3.0)\n",
    "c = a * b\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run([a, b, c])\n",
    "print('%f * %f = %f' % (result[0], result[1], result[2]))\n",
    "sess.close()\n",
    "\n",
    "# load data\n",
    "TRAIN_FOLDER = '../data/normalized/train'\n",
    "TEST_FOLDER = '../data/normalized/test'\n",
    "ID_FILE = '../data/normalized/id.json'\n",
    "NUM_SPEAKERS = 200\n",
    "\n",
    "def get_attributes(fname):\n",
    "    attr = fname.split('.')[0].split('-')\n",
    "    dialect = attr[0]\n",
    "    gender = attr[1][0]\n",
    "    speaker_id = attr[1]\n",
    "    sentence_type = attr[2][:2]\n",
    "    return dialect, gender, speaker_id, sentence_type\n",
    "\n",
    "def load_files(train):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for speaker_id, flist in train.items():\n",
    "        for fname in flist:\n",
    "            filedata = spio.loadmat(fname)['data'][0, :]\n",
    "            for segment in filedata:\n",
    "                for x in segment:\n",
    "                    X.append(x) # dim(x) => (390,)\n",
    "                    Y.append(speaker_id)\n",
    "    return X, Y\n",
    "    \n",
    "\n",
    "def load_train_data():\n",
    "    import json\n",
    "    with open(ID_FILE, 'r') as f_json:\n",
    "        ids = json.load(f_json)\n",
    "    print('ids', ids)\n",
    "    \n",
    "    \n",
    "    files = [f for f in os.listdir(TRAIN_FOLDER) if os.path.isfile(os.path.join(TRAIN_FOLDER, f))]\n",
    "\n",
    "    # split train val data\n",
    "    train = {}\n",
    "    val = {}\n",
    "    \n",
    "    added = {}\n",
    "    for file in files:\n",
    "        dialect, gender, speaker_id, sentence_type = get_attributes(file)\n",
    "        file_path = os.path.join(TRAIN_FOLDER, file)\n",
    "        if speaker_id not in ids:\n",
    "            print('ERROR:', speaker_id, 'not found in ids')\n",
    "        speaker_id = ids[speaker_id]\n",
    "        val_set = added.setdefault(speaker_id, {})\n",
    "        if sentence_type not in val_set:\n",
    "            val.setdefault(speaker_id, []).append(file_path)\n",
    "            val_set[sentence_type] = True\n",
    "        else:\n",
    "            train.setdefault(speaker_id, []).append(file_path)\n",
    "    # test data    \n",
    "    test = {}\n",
    "    files = [f for f in os.listdir(TEST_FOLDER) if os.path.isfile(os.path.join(TEST_FOLDER, f))]\n",
    "    for file in files:\n",
    "        dialect, gender, speaker_id, sentence_type = get_attributes(file)\n",
    "        file_path = os.path.join(TEST_FOLDER, file)\n",
    "        if speaker_id not in ids:\n",
    "            print('ERROR:', speaker_id, 'not found in ids')\n",
    "        speaker_id = ids[speaker_id]\n",
    "        test.setdefault(speaker_id, []).append(file_path)\n",
    "    \n",
    "    # load data\n",
    "    # input_path = os.path.join(TRAIN_FOLDER, file)\n",
    "    X_train, Y_train = load_files(train)\n",
    "    X_val, Y_val = load_files(val)\n",
    "    X_test, Y_test = load_files(test)\n",
    "    return np.array(X_train), np.array(Y_train), np.array(X_val), np.array(Y_val), np.array(X_test), np.array(Y_test)\n",
    "    \n",
    "print('Loading data...')\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = load_train_data()\n",
    "\n",
    "print()\n",
    "print('# Train :', len(Y_train))\n",
    "print('# Eval  :', len(Y_val))\n",
    "print('# Test  :', len(Y_test))\n",
    "print()\n",
    "\n",
    "# model\n",
    "def fully_connected(input, num_outputs, act_fn=None):\n",
    "    return tf.contrib.layers.fully_connected(input, num_outputs,\n",
    "                                             activation_fn=act_fn,\n",
    "                                             weights_initializer=tf.random_normal_initializer(0.0, 0.02),\n",
    "                                             weights_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0001)\n",
    "                                            )\n",
    "\n",
    "class BaseModel(object):\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.log_step = 50\n",
    "        self._build_model()\n",
    "\n",
    "    def _model(self):\n",
    "        print('-' * 5 + '  Sample model  ' + '-' * 5)\n",
    "\n",
    "        print('input layer: ' + str(self.X.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc1'):\n",
    "            \n",
    "            self.fc1 = fully_connected(self.X, 390)\n",
    "            self.relu1 = tf.nn.relu(self.fc1)\n",
    "            \n",
    "            print('fc1 layer: ' + str(self.relu1.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc2'):\n",
    "            \n",
    "            self.fc2 = fully_connected(self.relu1, 200)\n",
    "            self.relu2 = tf.nn.relu(self.fc2)\n",
    "            \n",
    "            print('fc2 layer: ' + str(self.relu2.get_shape()))\n",
    "            \n",
    "        with tf.variable_scope('fc3'):\n",
    "            \n",
    "            self.fc3 = fully_connected(self.relu2, 200)\n",
    "            \n",
    "            print('fc3 layer: ' + str(self.fc3.get_shape()))\n",
    "        \n",
    "        # Return the last layer\n",
    "        return self.fc3\n",
    "\n",
    "    def _input_ops(self):\n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, 390])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "        \n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.starter_learning_rate = 4e-4\n",
    "        \n",
    "        # tf.train.exponential_decay(learning_rate, global_step, decay_steps, \n",
    "        # decay_rate, staircase=False, name=None)\n",
    "        # staircase: Boolean. If True decay the learning rate at discrete intervals\n",
    "        \n",
    "        # decay every 500 steps with a base of 0.96\n",
    "        self.learning_rate = tf.train.exponential_decay(self.starter_learning_rate,\n",
    "                                                        self.global_step, 500, 0.96,\n",
    "                                                        staircase=True)\n",
    "        \n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        # Adam optimizer 'self.train_op' that minimizes 'self.loss_op'\n",
    "        \n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.minimize(self.loss_op, global_step=self.global_step)\n",
    "        \n",
    "        \n",
    "    def _loss(self, labels, logits):\n",
    "        # Softmax cross entropy loss 'self.loss_op'\n",
    "        \n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        self.loss_op = tf.reduce_mean(loss)     \n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        # Define input variables\n",
    "        self._input_ops()\n",
    "\n",
    "        # Convert Y to one-hot vector\n",
    "        labels = tf.one_hot(self.Y, NUM_SPEAKERS)\n",
    "\n",
    "        # Build a model and get logits\n",
    "        logits = self._model()\n",
    "\n",
    "        # Compute loss\n",
    "        self._loss(labels, logits)\n",
    "        \n",
    "        # Build optimizer\n",
    "        self._build_optimizer()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predict = tf.argmax(logits, 1)\n",
    "        correct = tf.equal(predict, self.Y)\n",
    "        self.accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    def train(self, sess, X_train, Y_train, X_val, Y_val, num_epoch=40):\n",
    "        # shuffle\n",
    "        from sklearn.utils import shuffle\n",
    "        X_train, Y_train = shuffle(X_train, Y_train)\n",
    "        \n",
    "        # try loading checkpoint\n",
    "        saver = tf.train.Saver()\n",
    "        save_dir = \"tf_models/\"        \n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save_path = os.path.join(save_dir, 'csci-599_proj')\n",
    "        \n",
    "        try:\n",
    "            print(\"Trying to restore last checkpoint ...\")\n",
    "\n",
    "            # Use TensorFlow to find the latest checkpoint - if any.\n",
    "            last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=save_dir)\n",
    "\n",
    "            # Try and load the data in the checkpoint.\n",
    "            saver.restore(sess, save_path=last_chk_path)\n",
    "\n",
    "            # If we get to this point, the checkpoint was successfully loaded.\n",
    "            print(\"Restored checkpoint from:\", last_chk_path)\n",
    "        except:\n",
    "            # If the above failed for some reason, simply\n",
    "            # initialize all the variables for the TensorFlow graph.\n",
    "            print(\"Failed to restore checkpoint. Initializing variables instead.\")\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        num_training = len(Y_train)\n",
    "        print('-' * 5 + '  Start training  ' + '-' * 5)\n",
    "        for epoch in range(num_epoch):\n",
    "            print('train for epoch %d' % epoch)\n",
    "            print('global step: %d' % self.global_step.eval())\n",
    "            for i in range(num_training // self.batch_size):\n",
    "                X_ = X_train[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "                Y_ = Y_train[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                \n",
    "                feed_dict = {\n",
    "                    self.X: X_,\n",
    "                    self.Y: Y_\n",
    "                }                \n",
    "                \n",
    "                fetches = [self.train_op, self.loss_op, self.accuracy_op]\n",
    "\n",
    "                _, loss, accuracy = sess.run(fetches, feed_dict=feed_dict)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('iteration (%d): loss = %.3f, accuracy = %.3f' %\n",
    "                        (step, loss, accuracy))\n",
    "                step += 1\n",
    "            \n",
    "            # save state\n",
    "            model_path = saver.save(sess,\n",
    "                       save_path=save_path,\n",
    "                       global_step=self.global_step)\n",
    "            print(\"Model saved in %s\" % model_path)\n",
    "\n",
    "            #############################################################################\n",
    "            # TODO: Plot training curves                                                #\n",
    "            #############################################################################\n",
    "            # Graph 1. X: epoch, Y: training loss\n",
    "            plt.plot(losses)\n",
    "            plt.title('training loss')\n",
    "            plt.xlabel('iteration')\n",
    "            plt.ylabel('loss')\n",
    "            plt.show()    \n",
    "\n",
    "            # Graph 2. X: epoch, Y: training accuracy\n",
    "            plt.plot(accuracies)\n",
    "            plt.title('training accuracy')\n",
    "            plt.xlabel('iteration')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.show()\n",
    "\n",
    "            # Print validation results\n",
    "            print('validation for epoch %d' % epoch)\n",
    "            val_accuracy = self.evaluate(sess, X_val, Y_val)\n",
    "            print('-  epoch %d: validation accuracy = %.3f' % (epoch, val_accuracy))\n",
    "\n",
    "    def evaluate(self, sess, X_eval, Y_eval):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(X_eval.shape[0] // self.batch_size):\n",
    "            X_ = X_eval[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "            Y_ = Y_eval[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                        \n",
    "            \n",
    "            feed_dict = {\n",
    "                    self.X: X_,\n",
    "                    self.Y: Y_\n",
    "                }\n",
    "            \n",
    "            accuracy = sess.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return eval_accuracy / eval_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = time.time()\n",
    "print(datetime.now())\n",
    "print()\n",
    "\n",
    "# Clear old computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Train our sample model\n",
    "with tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True)) as sess:\n",
    "    with tf.device('/gpu:0'):\n",
    "        model = BaseModel()\n",
    "        model.train(sess, X_train, Y_train, X_val, Y_val, num_epoch=40)\n",
    "        accuracy = model.evaluate(sess, X_test, Y_test)\n",
    "        print('***** test accuracy: %.3f' % accuracy)\n",
    "        \n",
    "print('Done')\n",
    "print(datetime.now())\n",
    "print(\"Total time: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
