{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2289 DR1-MCPM0-SA1-00.wav\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "indir = 'chunks/' # already VAD\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = 'chunks/'\n",
    "flist = [f for f in listdir(indir) if isfile(join(indir, f))]\n",
    "print(len(flist), flist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(fname):\n",
    "    attr = fname.split('.')[0].split('-')\n",
    "    dialect = attr[0]\n",
    "    gender = attr[1][0]\n",
    "    speaker_id = attr[1]\n",
    "    sentence_type = attr[2][:2]\n",
    "    return dialect, gender, speaker_id, sentence_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "\n",
    "for fname in flist:\n",
    "    input_path = indir + fname\n",
    "    y, sr = librosa.load(input_path, sr=None) # set sr=None for orig file sr otherwise it is converted to ~22K\n",
    "\n",
    "    # scaling the maximum of absolute amplitude to 1\n",
    "    processed_data = y/max(abs(y))\n",
    "    \n",
    "    # TODO: calc VAD (already done)\n",
    "    \n",
    "    # https://groups.google.com/forum/#!topic/librosa/V4Z1HpTKn8Q\n",
    "    mfcc = librosa.feature.mfcc(y=processed_data, sr=sr, n_mfcc=13, n_fft=(25*sr)//1000, hop_length=(10*sr)//1000)\n",
    "    mfcc[0] = librosa.feature.rmse(processed_data, hop_length=int(0.010*sr), n_fft=int(0.025*sr)) \n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    features = np.vstack([mfcc, mfcc_delta, mfcc_delta2]) \n",
    "    \n",
    "    # split train test\n",
    "    dialect, gender, speaker_id, sentence_type = get_attributes(fname)\n",
    "    if sentence_type == 'SA':\n",
    "        test.setdefault(speaker_id, []).append(features)\n",
    "    else:\n",
    "        train.setdefault(speaker_id, []).append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MADC0', 'MAEB0', 'MAKB0', 'MAKR0', 'MAPV0', 'MARC0', 'MARW0', 'MBEF0', 'MBGT0', 'MBJV0', 'MBMA0', 'MBWP0', 'MCAL0', 'MCDC0', 'MCDD0', 'MCDR0', 'MCEF0', 'MCEW0', 'MCHL0', 'MCLM0', 'MCPM0', 'MCSS0', 'MCTM0', 'MDAC0', 'MDAS0', 'MDBB1', 'MDBP0', 'MDCD0', 'MDDC0', 'MDEF0', 'MDEM0', 'MDHL0', 'MDHS0', 'MDJM0', 'MDLB0', 'MDLC0', 'MDLC2', 'MDLH0', 'MDMA0', 'MDMT0', 'MDNS0', 'MDPK0', 'MDPS0', 'MDSJ0', 'MDSS0', 'MDSS1', 'MDTB0', 'MDWD0', 'MDWH0', 'MDWM0', 'MEDR0', 'MEFG0', 'MEGJ0', 'MESG0', 'MEWM0', 'MFER0', 'MFMC0', 'MFRM0', 'MFWK0', 'MGAF0', 'MGAG0', 'MGES0', 'MGJC0', 'MGRL0', 'MGRP0', 'MGSH0', 'MGXP0', 'MHIT0', 'MHJB0', 'MHMG0', 'MHMR0', 'MHRM0', 'MILB0', 'MJAC0', 'MJAE0', 'MJBG0', 'MJDA0', 'MJDC0', 'MJDE0', 'MJEB0', 'MJEB1', 'MJEE0', 'MJHI0', 'MJJB0', 'MJJJ0', 'MJKR0', 'MJLB0', 'MJLG1', 'MJLS0', 'MJMA0', 'MJMD0', 'MJMM0', 'MJPM0', 'MJPM1', 'MJRH0', 'MJRH1', 'MJRP0', 'MJSR0', 'MJWS0', 'MJWT0', 'MJXL0', 'MKAH0', 'MKAJ0', 'MKAM0', 'MKDT0', 'MKJO0', 'MKLS0', 'MKLS1', 'MKLW0', 'MKXL0', 'MLBC0', 'MLEL0', 'MLJC0', 'MLJH0', 'MLNS0', 'MLSH0', 'MMAA0', 'MMAG0', 'MMAM0', 'MMAR0', 'MMBS0', 'MMDM0', 'MMDS0', 'MMEB0', 'MMGC0', 'MMGG0', 'MMGK0', 'MMJB1', 'MMRP0', 'MMSM0', 'MMXS0', 'MNET0', 'MPEB0', 'MPGH0', 'MPGR0', 'MPPC0', 'MPRB0', 'MPRD0', 'MPRK0', 'MPRT0', 'MPSW0', 'MRAB0', 'MRAB1', 'MRAI0', 'MRBC0', 'MRCG0', 'MRCW0', 'MRDD0', 'MRDS0', 'MREE0', 'MREH1', 'MRFK0', 'MRFL0', 'MRGM0', 'MRGS0', 'MRHL0', 'MRJB1', 'MRJH0', 'MRJM0', 'MRJM1', 'MRJT0', 'MRLJ0', 'MRLR0', 'MRMS0', 'MRSO0', 'MRSP0', 'MRTC0', 'MRTJ0', 'MRWA0', 'MRWS0', 'MSAT0', 'MSFH0', 'MSFV0', 'MSMC0', 'MSMS0', 'MSRG0', 'MSTF0', 'MTAS0', 'MTAT1', 'MTBC0', 'MTDB0', 'MTJG0', 'MTJM0', 'MTJS0', 'MTKP0', 'MTLB0', 'MTPF0', 'MTPG0', 'MTPP0', 'MTQC0', 'MTRC0', 'MTRR0', 'MTRT0', 'MVJH0', 'MWAD0', 'MWAR0', 'MWDK0', 'MWGR0', 'MWSB0', 'MZMB0']\n",
      "{'MDAS0': 24, 'MSFV0': 172, 'MMRP0': 128, 'MTDB0': 180, 'MWAR0': 195, 'MDLB0': 34, 'MSMS0': 174, 'MTJS0': 183, 'MMAR0': 119, 'MRDD0': 147, 'MRGS0': 154, 'MDWD0': 47, 'MFWK0': 58, 'MEGJ0': 52, 'MRDS0': 148, 'MJAE0': 74, 'MRWS0': 169, 'MGXP0': 66, 'MSAT0': 170, 'MRWA0': 168, 'MREH1': 150, 'MDHS0': 32, 'MBEF0': 7, 'MLJC0': 112, 'MPGR0': 134, 'MGJC0': 62, 'MKLW0': 108, 'MRSO0': 164, 'MRFK0': 151, 'MJMD0': 90, 'MKLS1': 107, 'MRGM0': 153, 'MADC0': 0, 'MJPM0': 92, 'MDNS0': 40, 'MCDR0': 15, 'MBGT0': 8, 'MFRM0': 57, 'MRJM1': 159, 'MDEF0': 29, 'MBJV0': 9, 'MDJM0': 33, 'MAPV0': 4, 'MKAJ0': 102, 'MDBB1': 25, 'MRBC0': 144, 'MARC0': 5, 'MDDC0': 28, 'MRJB1': 156, 'MCTM0': 22, 'MKXL0': 109, 'MPSW0': 140, 'MCSS0': 21, 'MTAT1': 178, 'MRSP0': 165, 'MPPC0': 135, 'MFER0': 55, 'MRTJ0': 167, 'MMBS0': 120, 'MEFG0': 51, 'MJDA0': 76, 'MCAL0': 12, 'MDAC0': 23, 'MMAG0': 117, 'MHRM0': 71, 'MJWT0': 99, 'MHMR0': 70, 'MSFH0': 171, 'MGSH0': 65, 'MRJH0': 157, 'MRMS0': 163, 'MLNS0': 114, 'MBWP0': 11, 'MRCG0': 145, 'MMDM0': 121, 'MRCW0': 146, 'MAEB0': 1, 'MKAH0': 101, 'MKDT0': 104, 'MPRD0': 137, 'MJEB0': 79, 'MRLJ0': 161, 'MESG0': 53, 'MEDR0': 50, 'MCLM0': 19, 'MTJM0': 182, 'MJSR0': 97, 'MLSH0': 115, 'MJBG0': 75, 'MVJH0': 193, 'MDPS0': 42, 'MMJB1': 127, 'MAKR0': 3, 'MRFL0': 152, 'MMSM0': 129, 'MJJB0': 83, 'MGRP0': 64, 'MPRB0': 136, 'MPEB0': 132, 'MDWM0': 49, 'MNET0': 131, 'MJRH0': 94, 'MTLB0': 185, 'MJMA0': 89, 'MTQC0': 189, 'MDMT0': 39, 'MDLH0': 37, 'MRJM0': 158, 'MDSS0': 44, 'MTRT0': 192, 'MCHL0': 18, 'MJEE0': 81, 'MDEM0': 30, 'MJPM1': 93, 'MMAM0': 118, 'MHJB0': 68, 'MMDS0': 122, 'MJXL0': 100, 'MDSJ0': 43, 'MJHI0': 82, 'MTRC0': 190, 'MDWH0': 48, 'MDPK0': 41, 'MRAB0': 141, 'MTBC0': 179, 'MDLC2': 36, 'MWDK0': 196, 'MTPF0': 186, 'MWAD0': 194, 'MILB0': 72, 'MKLS0': 106, 'MSRG0': 175, 'MTRR0': 191, 'MPRT0': 139, 'MJAC0': 73, 'MJDC0': 77, 'MLEL0': 111, 'MRTC0': 166, 'MRJT0': 160, 'MCEW0': 17, 'MRAB1': 142, 'MEWM0': 54, 'MRHL0': 155, 'MHMG0': 69, 'MGRL0': 63, 'MDTB0': 46, 'MCDD0': 14, 'MGES0': 61, 'MMEB0': 123, 'MJMM0': 91, 'MCEF0': 16, 'MTJG0': 181, 'MAKB0': 2, 'MKJO0': 105, 'MDMA0': 38, 'MSTF0': 176, 'MTPG0': 187, 'MDHL0': 31, 'MMXS0': 130, 'MMGG0': 125, 'MDLC0': 35, 'MJLG1': 87, 'MHIT0': 67, 'MTAS0': 177, 'MKAM0': 103, 'MCPM0': 20, 'MDBP0': 26, 'MLJH0': 113, 'MJLB0': 86, 'MTPP0': 188, 'MRLR0': 162, 'MJDE0': 78, 'MLBC0': 110, 'MJJJ0': 84, 'MDSS1': 45, 'MREE0': 149, 'MJKR0': 85, 'MWSB0': 198, 'MMAA0': 116, 'MDCD0': 27, 'MSMC0': 173, 'MJEB1': 80, 'MARW0': 6, 'MRAI0': 143, 'MZMB0': 199, 'MWGR0': 197, 'MJRP0': 96, 'MCDC0': 13, 'MMGK0': 126, 'MGAG0': 60, 'MMGC0': 124, 'MJLS0': 88, 'MPGH0': 133, 'MTKP0': 184, 'MFMC0': 56, 'MPRK0': 138, 'MJRH1': 95, 'MGAF0': 59, 'MBMA0': 10, 'MJWS0': 98}\n"
     ]
    }
   ],
   "source": [
    "ids = list(test.keys())\n",
    "ids.sort()\n",
    "print(ids)\n",
    "\n",
    "idx = {}\n",
    "for i in range(len(ids)):\n",
    "    idx[ids[i]] = i # TODO: for MATLAB set i+1 (i.e 1 to 200)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(x, win_size=10, hop_size=3):\n",
    "    r, c = x.shape\n",
    "    y = []\n",
    "    for i in range(0, c, hop_size):\n",
    "        if i + win_size > c:\n",
    "            break\n",
    "        y.append(x[:, i:i + win_size].T.flatten())\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# mvn of test\n",
    "for speaker_id, feature_list in test.items():\n",
    "    speaker_id = idx[speaker_id]\n",
    "    for features in feature_list:\n",
    "        features = preprocessing.scale(features.T).T\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            X_test.append(frame)\n",
    "            Y_test.append(speaker_id)\n",
    "            \n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "# smvn of train\n",
    "for speaker_id, feature_list in train.items():\n",
    "    speaker_id = idx[speaker_id]\n",
    "    \n",
    "    # calc speaker level mean and std\n",
    "    data = []\n",
    "    for features in feature_list:\n",
    "        frames = features.T\n",
    "        for frame in frames:\n",
    "            data.append(frame)\n",
    "    data = np.array(data)\n",
    "    mean = data.mean(axis=0)\n",
    "    std = data.std(axis=0)\n",
    "    \n",
    "    # speaker level normalize\n",
    "    for features in feature_list:\n",
    "        features = ((features.T - mean)/std).T\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            X_train.append(frame)\n",
    "            Y_train.append(speaker_id)\n",
    "            \n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137520, 390) (36091, 390)\n",
      "(137520,) (36091,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, Y_train = shuffle(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.33479681\n",
      "Iteration 2, loss = 5.02862733\n",
      "Iteration 3, loss = 4.76368955\n",
      "Iteration 4, loss = 4.55939396\n",
      "Iteration 5, loss = 4.38443331\n",
      "Iteration 6, loss = 4.23295709\n",
      "Iteration 7, loss = 4.10110612\n",
      "Iteration 8, loss = 3.98623488\n",
      "Iteration 9, loss = 3.88781922\n",
      "Iteration 10, loss = 3.79958155\n",
      "Iteration 11, loss = 3.72265799\n",
      "Iteration 12, loss = 3.65547699\n",
      "Iteration 13, loss = 3.59621347\n",
      "Iteration 14, loss = 3.54229991\n",
      "Iteration 15, loss = 3.49344509\n",
      "Iteration 16, loss = 3.45086983\n",
      "Iteration 17, loss = 3.41007972\n",
      "Iteration 18, loss = 3.37302821\n",
      "Iteration 19, loss = 3.33829433\n",
      "Iteration 20, loss = 3.30578820\n",
      "Iteration 21, loss = 3.27558155\n",
      "Iteration 22, loss = 3.24771529\n",
      "Iteration 23, loss = 3.22157143\n",
      "Iteration 24, loss = 3.19363001\n",
      "Iteration 25, loss = 3.16971427\n",
      "Iteration 26, loss = 3.14725079\n",
      "Iteration 27, loss = 3.12507760\n",
      "Iteration 28, loss = 3.10336494\n",
      "Iteration 29, loss = 3.08484877\n",
      "Iteration 30, loss = 3.06571020\n",
      "Iteration 31, loss = 3.04344551\n",
      "Iteration 32, loss = 3.02913757\n",
      "Iteration 33, loss = 3.01084367\n",
      "Iteration 34, loss = 2.99278344\n",
      "Iteration 35, loss = 2.97681091\n",
      "Iteration 36, loss = 2.95981811\n",
      "Iteration 37, loss = 2.94434323\n",
      "Iteration 38, loss = 2.93116939\n",
      "Iteration 39, loss = 2.91448053\n",
      "Iteration 40, loss = 2.89895209\n",
      "Iteration 41, loss = 2.88568626\n",
      "Iteration 42, loss = 2.86970533\n",
      "Iteration 43, loss = 2.85619254\n",
      "Iteration 44, loss = 2.84115480\n",
      "Iteration 45, loss = 2.82754459\n",
      "Iteration 46, loss = 2.81394543\n",
      "Iteration 47, loss = 2.79964158\n",
      "Iteration 48, loss = 2.78518780\n",
      "Iteration 49, loss = 2.77181028\n",
      "Iteration 50, loss = 2.75740614\n",
      "Iteration 51, loss = 2.74353267\n",
      "Iteration 52, loss = 2.72718733\n",
      "Iteration 53, loss = 2.71344288\n",
      "Iteration 54, loss = 2.69970710\n",
      "Iteration 55, loss = 2.68526057\n",
      "Iteration 56, loss = 2.66833528\n",
      "Iteration 57, loss = 2.65520414\n",
      "Iteration 58, loss = 2.63977673\n",
      "Iteration 59, loss = 2.62491926\n",
      "Iteration 60, loss = 2.60935331\n",
      "Iteration 61, loss = 2.59545147\n",
      "Iteration 62, loss = 2.58014351\n",
      "Iteration 63, loss = 2.56214209\n",
      "Iteration 64, loss = 2.54705272\n",
      "Iteration 65, loss = 2.53323383\n",
      "Iteration 66, loss = 2.51971543\n",
      "Iteration 67, loss = 2.50170183\n",
      "Iteration 68, loss = 2.48737524\n",
      "Iteration 69, loss = 2.47290197\n",
      "Iteration 70, loss = 2.45524112\n",
      "Iteration 71, loss = 2.44206459\n",
      "Iteration 72, loss = 2.42699114\n",
      "Iteration 73, loss = 2.40979538\n",
      "Iteration 74, loss = 2.39606853\n",
      "Iteration 75, loss = 2.38162466\n",
      "Iteration 76, loss = 2.36561361\n",
      "Iteration 77, loss = 2.35080873\n",
      "Iteration 78, loss = 2.33699551\n",
      "Iteration 79, loss = 2.32070532\n",
      "Iteration 80, loss = 2.30817012\n",
      "Iteration 81, loss = 2.29349023\n",
      "Iteration 82, loss = 2.27757415\n",
      "Iteration 83, loss = 2.26484697\n",
      "Iteration 84, loss = 2.25111341\n",
      "Iteration 85, loss = 2.23589255\n",
      "Iteration 86, loss = 2.22223791\n",
      "Iteration 87, loss = 2.20926127\n",
      "Iteration 88, loss = 2.19599333\n",
      "Iteration 89, loss = 2.18013575\n",
      "Iteration 90, loss = 2.16797987\n",
      "Iteration 91, loss = 2.15315997\n",
      "Iteration 92, loss = 2.14089079\n",
      "Iteration 93, loss = 2.13008448\n",
      "Iteration 94, loss = 2.11583694\n",
      "Iteration 95, loss = 2.10155512\n",
      "Iteration 96, loss = 2.08883249\n",
      "Iteration 97, loss = 2.07764364\n",
      "Iteration 98, loss = 2.06252015\n",
      "Iteration 99, loss = 2.05280352\n",
      "Iteration 100, loss = 2.04038551\n",
      "Iteration 101, loss = 2.02701116\n",
      "Iteration 102, loss = 2.01392576\n",
      "Iteration 103, loss = 2.00156553\n",
      "Iteration 104, loss = 1.99036811\n",
      "Iteration 105, loss = 1.97972234\n",
      "Iteration 106, loss = 1.96616766\n",
      "Iteration 107, loss = 1.95280508\n",
      "Iteration 108, loss = 1.94170504\n",
      "Iteration 109, loss = 1.93099709\n",
      "Iteration 110, loss = 1.91647460\n",
      "Iteration 111, loss = 1.90751515\n",
      "Iteration 112, loss = 1.89269594\n",
      "Iteration 113, loss = 1.88376811\n",
      "Iteration 114, loss = 1.87168999\n",
      "Iteration 115, loss = 1.85912168\n",
      "Iteration 116, loss = 1.84741181\n",
      "Iteration 117, loss = 1.83592962\n",
      "Iteration 118, loss = 1.82386415\n",
      "Iteration 119, loss = 1.81356894\n",
      "Iteration 120, loss = 1.79994521\n",
      "Iteration 121, loss = 1.78940862\n",
      "Iteration 122, loss = 1.77794814\n",
      "Iteration 123, loss = 1.76610968\n",
      "Iteration 124, loss = 1.75244718\n",
      "Iteration 125, loss = 1.74139735\n",
      "Iteration 126, loss = 1.73110070\n",
      "Iteration 127, loss = 1.71678721\n",
      "Iteration 128, loss = 1.70541022\n",
      "Iteration 129, loss = 1.69321626\n",
      "Iteration 130, loss = 1.68081508\n",
      "Iteration 131, loss = 1.67038659\n",
      "Iteration 132, loss = 1.65791564\n",
      "Iteration 133, loss = 1.64593027\n",
      "Iteration 134, loss = 1.63354138\n",
      "Iteration 135, loss = 1.62094884\n",
      "Iteration 136, loss = 1.61026380\n",
      "Iteration 137, loss = 1.59810796\n",
      "Iteration 138, loss = 1.58605571\n",
      "Iteration 139, loss = 1.57456249\n",
      "Iteration 140, loss = 1.56301255\n",
      "Iteration 141, loss = 1.55183834\n",
      "Iteration 142, loss = 1.53865294\n",
      "Iteration 143, loss = 1.52795392\n",
      "Iteration 144, loss = 1.51854323\n",
      "Iteration 145, loss = 1.50542278\n",
      "Iteration 146, loss = 1.49552837\n",
      "Iteration 147, loss = 1.48411789\n",
      "Iteration 148, loss = 1.47194254\n",
      "Iteration 149, loss = 1.46034580\n",
      "Iteration 150, loss = 1.45081420\n",
      "Iteration 151, loss = 1.43969359\n",
      "Iteration 152, loss = 1.42870419\n",
      "Iteration 153, loss = 1.41886526\n",
      "Iteration 154, loss = 1.40674689\n",
      "Iteration 155, loss = 1.39796933\n",
      "Iteration 156, loss = 1.38783469\n",
      "Iteration 157, loss = 1.37822076\n",
      "Iteration 158, loss = 1.36794849\n",
      "Iteration 159, loss = 1.35741993\n",
      "Iteration 160, loss = 1.34556705\n",
      "Iteration 161, loss = 1.33635468\n",
      "Iteration 162, loss = 1.32575804\n",
      "Iteration 163, loss = 1.31777732\n",
      "Iteration 164, loss = 1.30648410\n",
      "Iteration 165, loss = 1.29835493\n",
      "Iteration 166, loss = 1.28869097\n",
      "Iteration 167, loss = 1.27952494\n",
      "Iteration 168, loss = 1.26791250\n",
      "Iteration 169, loss = 1.25967237\n",
      "Iteration 170, loss = 1.24938118\n",
      "Iteration 171, loss = 1.24184117\n",
      "Iteration 172, loss = 1.23148788\n",
      "Iteration 173, loss = 1.22282148\n",
      "Iteration 174, loss = 1.21254758\n",
      "Iteration 175, loss = 1.20547523\n",
      "Iteration 176, loss = 1.19574198\n",
      "Iteration 177, loss = 1.18862169\n",
      "Iteration 178, loss = 1.17652666\n",
      "Iteration 179, loss = 1.16802792\n",
      "Iteration 180, loss = 1.15932653\n",
      "Iteration 181, loss = 1.15198403\n",
      "Iteration 182, loss = 1.14563778\n",
      "Iteration 183, loss = 1.13534195\n",
      "Iteration 184, loss = 1.12680797\n",
      "Iteration 185, loss = 1.11767435\n",
      "Iteration 186, loss = 1.10765620\n",
      "Iteration 187, loss = 1.10058865\n",
      "Iteration 188, loss = 1.09118197\n",
      "Iteration 189, loss = 1.08346479\n",
      "Iteration 190, loss = 1.07502217\n",
      "Iteration 191, loss = 1.06736082\n",
      "Iteration 192, loss = 1.05927502\n",
      "Iteration 193, loss = 1.05004410\n",
      "Iteration 194, loss = 1.04268228\n",
      "Iteration 195, loss = 1.03415417\n",
      "Iteration 196, loss = 1.02646725\n",
      "Iteration 197, loss = 1.01806379\n",
      "Iteration 198, loss = 1.01193686\n",
      "Iteration 199, loss = 1.00244765\n",
      "Iteration 200, loss = 0.99686211\n",
      "Iteration 201, loss = 0.98654618\n",
      "Iteration 202, loss = 0.98055859\n",
      "Iteration 203, loss = 0.97406393\n",
      "Iteration 204, loss = 0.96442997\n",
      "Iteration 205, loss = 0.95635552\n",
      "Iteration 206, loss = 0.95040975\n",
      "Iteration 207, loss = 0.93962543\n",
      "Iteration 208, loss = 0.93353936\n",
      "Iteration 209, loss = 0.92511563\n",
      "Iteration 210, loss = 0.91843503\n",
      "Iteration 211, loss = 0.91198305\n",
      "Iteration 212, loss = 0.90582484\n",
      "Iteration 213, loss = 0.89828076\n",
      "Iteration 214, loss = 0.88921449\n",
      "Iteration 215, loss = 0.88388213\n",
      "Iteration 216, loss = 0.87667475\n",
      "Iteration 217, loss = 0.87000901\n",
      "Iteration 218, loss = 0.86164850\n",
      "Iteration 219, loss = 0.85151275\n",
      "Iteration 220, loss = 0.84730818\n",
      "Iteration 221, loss = 0.84101588\n",
      "Iteration 222, loss = 0.83344583\n",
      "Iteration 223, loss = 0.82633281\n",
      "Iteration 224, loss = 0.82058227\n",
      "Iteration 225, loss = 0.81245190\n",
      "Iteration 226, loss = 0.80542453\n",
      "Iteration 227, loss = 0.79932968\n",
      "Iteration 228, loss = 0.79464951\n",
      "Iteration 229, loss = 0.78686865\n",
      "Iteration 230, loss = 0.77861434\n",
      "Iteration 231, loss = 0.77275702\n",
      "Iteration 232, loss = 0.76694319\n",
      "Iteration 233, loss = 0.75944960\n",
      "Iteration 234, loss = 0.75566354\n",
      "Iteration 235, loss = 0.74700550\n",
      "Iteration 236, loss = 0.73897151\n",
      "Iteration 237, loss = 0.73373692\n",
      "Iteration 238, loss = 0.73027900\n",
      "Iteration 239, loss = 0.72126666\n",
      "Iteration 240, loss = 0.71723119\n",
      "Iteration 241, loss = 0.71101806\n",
      "Iteration 242, loss = 0.70334380\n",
      "Iteration 243, loss = 0.69896130\n",
      "Iteration 244, loss = 0.69324820\n",
      "Iteration 245, loss = 0.68609596\n",
      "Iteration 246, loss = 0.68092189\n",
      "Iteration 247, loss = 0.67489745\n",
      "Iteration 248, loss = 0.66890177\n",
      "Iteration 249, loss = 0.66381326\n",
      "Iteration 250, loss = 0.65782339\n",
      "Iteration 251, loss = 0.65231688\n",
      "Iteration 252, loss = 0.64694045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.64151141\n",
      "Iteration 254, loss = 0.63448638\n",
      "Iteration 255, loss = 0.62910988\n",
      "Iteration 256, loss = 0.62652049\n",
      "Iteration 257, loss = 0.61842762\n",
      "Iteration 258, loss = 0.61333833\n",
      "Iteration 259, loss = 0.60690125\n",
      "Iteration 260, loss = 0.60313495\n",
      "Iteration 261, loss = 0.59728118\n",
      "Iteration 262, loss = 0.59204846\n",
      "Iteration 263, loss = 0.58566073\n",
      "Iteration 264, loss = 0.58086823\n",
      "Iteration 265, loss = 0.57616199\n",
      "Iteration 266, loss = 0.57135042\n",
      "Iteration 267, loss = 0.56530148\n",
      "Iteration 268, loss = 0.56007698\n",
      "Iteration 269, loss = 0.55609134\n",
      "Iteration 270, loss = 0.55149760\n",
      "Iteration 271, loss = 0.54760554\n",
      "Iteration 272, loss = 0.54102459\n",
      "Iteration 273, loss = 0.53677334\n",
      "Iteration 274, loss = 0.53243936\n",
      "Iteration 275, loss = 0.52863505\n",
      "Iteration 276, loss = 0.52289378\n",
      "Iteration 277, loss = 0.51753811\n",
      "Iteration 278, loss = 0.51535562\n",
      "Iteration 279, loss = 0.51028957\n",
      "Iteration 280, loss = 0.50741809\n",
      "Iteration 281, loss = 0.50178407\n",
      "Iteration 282, loss = 0.49587421\n",
      "Iteration 283, loss = 0.49400877\n",
      "Iteration 284, loss = 0.48788224\n",
      "Iteration 285, loss = 0.48298929\n",
      "Iteration 286, loss = 0.48057427\n",
      "Iteration 287, loss = 0.47892163\n",
      "Iteration 288, loss = 0.47176208\n",
      "Iteration 289, loss = 0.46950258\n",
      "Iteration 290, loss = 0.46460597\n",
      "Iteration 291, loss = 0.46148525\n",
      "Iteration 292, loss = 0.45535913\n",
      "Iteration 293, loss = 0.45451471\n",
      "Iteration 294, loss = 0.45069732\n",
      "Iteration 295, loss = 0.44565641\n",
      "Iteration 296, loss = 0.44202220\n",
      "Iteration 297, loss = 0.43861480\n",
      "Iteration 298, loss = 0.43535175\n",
      "Iteration 299, loss = 0.43336290\n",
      "Iteration 300, loss = 0.43009907\n",
      "Iteration 301, loss = 0.42719235\n",
      "Iteration 302, loss = 0.42287007\n",
      "Iteration 303, loss = 0.42006858\n",
      "Iteration 304, loss = 0.41597824\n",
      "Iteration 305, loss = 0.41285715\n",
      "Iteration 306, loss = 0.41035405\n",
      "Iteration 307, loss = 0.40690580\n",
      "Iteration 308, loss = 0.40294977\n",
      "Iteration 309, loss = 0.39909900\n",
      "Iteration 310, loss = 0.39691013\n",
      "Iteration 311, loss = 0.39333374\n",
      "Iteration 312, loss = 0.39102757\n",
      "Iteration 313, loss = 0.38767279\n",
      "Iteration 314, loss = 0.38540228\n",
      "Iteration 315, loss = 0.38259094\n",
      "Iteration 316, loss = 0.37963163\n",
      "Iteration 317, loss = 0.37603466\n",
      "Iteration 318, loss = 0.37360707\n",
      "Iteration 319, loss = 0.37064914\n",
      "Iteration 320, loss = 0.36751554\n",
      "Iteration 321, loss = 0.36701379\n",
      "Iteration 322, loss = 0.36158361\n",
      "Iteration 323, loss = 0.35965394\n",
      "Iteration 324, loss = 0.35781865\n",
      "Iteration 325, loss = 0.35348386\n",
      "Iteration 326, loss = 0.35105594\n",
      "Iteration 327, loss = 0.34924815\n",
      "Iteration 328, loss = 0.34777067\n",
      "Iteration 329, loss = 0.34311340\n",
      "Iteration 330, loss = 0.34289235\n",
      "Iteration 331, loss = 0.33954741\n",
      "Iteration 332, loss = 0.33766812\n",
      "Iteration 333, loss = 0.33520616\n",
      "Iteration 334, loss = 0.33384809\n",
      "Iteration 335, loss = 0.32963518\n",
      "Iteration 336, loss = 0.32913715\n",
      "Iteration 337, loss = 0.32617816\n",
      "Iteration 338, loss = 0.32417800\n",
      "Iteration 339, loss = 0.32029184\n",
      "Iteration 340, loss = 0.31872264\n",
      "Iteration 341, loss = 0.31815645\n",
      "Iteration 342, loss = 0.31446920\n",
      "Iteration 343, loss = 0.31220166\n",
      "Iteration 344, loss = 0.30871821\n",
      "Iteration 345, loss = 0.30840372\n",
      "Iteration 346, loss = 0.30562849\n",
      "Iteration 347, loss = 0.30330969\n",
      "Iteration 348, loss = 0.30121888\n",
      "Iteration 349, loss = 0.29886068\n",
      "Iteration 350, loss = 0.29673548\n",
      "Iteration 351, loss = 0.29496205\n",
      "Iteration 352, loss = 0.29284771\n",
      "Iteration 353, loss = 0.29035384\n",
      "Iteration 354, loss = 0.29011140\n",
      "Iteration 355, loss = 0.28631898\n",
      "Iteration 356, loss = 0.28681620\n",
      "Iteration 357, loss = 0.28273269\n",
      "Iteration 358, loss = 0.28262321\n",
      "Iteration 359, loss = 0.27860029\n",
      "Iteration 360, loss = 0.27874302\n",
      "Iteration 361, loss = 0.27680919\n",
      "Iteration 362, loss = 0.27440565\n",
      "Iteration 363, loss = 0.27348884\n",
      "Iteration 364, loss = 0.27094048\n",
      "Iteration 365, loss = 0.27015939\n",
      "Iteration 366, loss = 0.26799979\n",
      "Iteration 367, loss = 0.26776550\n",
      "Iteration 368, loss = 0.26452304\n",
      "Iteration 369, loss = 0.26597447\n",
      "Iteration 370, loss = 0.26121228\n",
      "Iteration 371, loss = 0.25861537\n",
      "Iteration 372, loss = 0.25752131\n",
      "Iteration 373, loss = 0.25751228\n",
      "Iteration 374, loss = 0.25445369\n",
      "Iteration 375, loss = 0.25319745\n",
      "Iteration 376, loss = 0.25220926\n",
      "Iteration 377, loss = 0.24880699\n",
      "Iteration 378, loss = 0.24820673\n",
      "Iteration 379, loss = 0.24642334\n",
      "Iteration 380, loss = 0.24418504\n",
      "Iteration 381, loss = 0.24241690\n",
      "Iteration 382, loss = 0.24084534\n",
      "Iteration 383, loss = 0.23995359\n",
      "Iteration 384, loss = 0.23597955\n",
      "Iteration 385, loss = 0.23633403\n",
      "Iteration 386, loss = 0.23445247\n",
      "Iteration 387, loss = 0.23329113\n",
      "Iteration 388, loss = 0.23187972\n",
      "Iteration 389, loss = 0.23049820\n",
      "Iteration 390, loss = 0.22855295\n",
      "Iteration 391, loss = 0.22757586\n",
      "Iteration 392, loss = 0.22426665\n",
      "Iteration 393, loss = 0.22474491\n",
      "Iteration 394, loss = 0.22251163\n",
      "Iteration 395, loss = 0.22185753\n",
      "Iteration 396, loss = 0.21941569\n",
      "Iteration 397, loss = 0.21904805\n",
      "Iteration 398, loss = 0.21641764\n",
      "Iteration 399, loss = 0.21582323\n",
      "Iteration 400, loss = 0.21430995\n",
      "Iteration 401, loss = 0.21206559\n",
      "Iteration 402, loss = 0.21153381\n",
      "Iteration 403, loss = 0.20797302\n",
      "Iteration 404, loss = 0.20740849\n",
      "Iteration 405, loss = 0.20786574\n",
      "Iteration 406, loss = 0.20533299\n",
      "Iteration 407, loss = 0.20413607\n",
      "Iteration 408, loss = 0.20173038\n",
      "Iteration 409, loss = 0.20073173\n",
      "Iteration 410, loss = 0.20010207\n",
      "Iteration 411, loss = 0.19762259\n",
      "Iteration 412, loss = 0.19680258\n",
      "Iteration 413, loss = 0.19495919\n",
      "Iteration 414, loss = 0.19302803\n",
      "Iteration 415, loss = 0.19199357\n",
      "Iteration 416, loss = 0.19050791\n",
      "Iteration 417, loss = 0.18846207\n",
      "Iteration 418, loss = 0.18877409\n",
      "Iteration 419, loss = 0.18794865\n",
      "Iteration 420, loss = 0.18583874\n",
      "Iteration 421, loss = 0.18463154\n",
      "Iteration 422, loss = 0.18322095\n",
      "Iteration 423, loss = 0.18160257\n",
      "Iteration 424, loss = 0.18064012\n",
      "Iteration 425, loss = 0.17952479\n",
      "Iteration 426, loss = 0.17753486\n",
      "Iteration 427, loss = 0.17777061\n",
      "Iteration 428, loss = 0.17577325\n",
      "Iteration 429, loss = 0.17623861\n",
      "Iteration 430, loss = 0.17360400\n",
      "Iteration 431, loss = 0.17248067\n",
      "Iteration 432, loss = 0.17088021\n",
      "Iteration 433, loss = 0.17203350\n",
      "Iteration 434, loss = 0.16946615\n",
      "Iteration 435, loss = 0.16744832\n",
      "Iteration 436, loss = 0.16664726\n",
      "Iteration 437, loss = 0.16653202\n",
      "Iteration 438, loss = 0.16429371\n",
      "Iteration 439, loss = 0.16319776\n",
      "Iteration 440, loss = 0.16207143\n",
      "Iteration 441, loss = 0.16209676\n",
      "Iteration 442, loss = 0.16077190\n",
      "Iteration 443, loss = 0.16006659\n",
      "Iteration 444, loss = 0.16031606\n",
      "Iteration 445, loss = 0.15653010\n",
      "Iteration 446, loss = 0.15728740\n",
      "Iteration 447, loss = 0.15594101\n",
      "Iteration 448, loss = 0.15532954\n",
      "Iteration 449, loss = 0.15332594\n",
      "Iteration 450, loss = 0.15355803\n",
      "Iteration 451, loss = 0.15303017\n",
      "Iteration 452, loss = 0.15057412\n",
      "Iteration 453, loss = 0.14972574\n",
      "Iteration 454, loss = 0.14778366\n",
      "Iteration 455, loss = 0.14780242\n",
      "Iteration 456, loss = 0.14809917\n",
      "Iteration 457, loss = 0.14606225\n",
      "Iteration 458, loss = 0.14753366\n",
      "Iteration 459, loss = 0.14457052\n",
      "Iteration 460, loss = 0.14471383\n",
      "Iteration 461, loss = 0.14150010\n",
      "Iteration 462, loss = 0.14191361\n",
      "Iteration 463, loss = 0.14202063\n",
      "Iteration 464, loss = 0.13985464\n",
      "Iteration 465, loss = 0.13986778\n",
      "Iteration 466, loss = 0.13943602\n",
      "Iteration 467, loss = 0.13792255\n",
      "Iteration 468, loss = 0.13699981\n",
      "Iteration 469, loss = 0.13536114\n",
      "Iteration 470, loss = 0.13494782\n",
      "Iteration 471, loss = 0.13446294\n",
      "Iteration 472, loss = 0.13475347\n",
      "Iteration 473, loss = 0.13336183\n",
      "Iteration 474, loss = 0.13333605\n",
      "Iteration 475, loss = 0.13162681\n",
      "Iteration 476, loss = 0.13136187\n",
      "Iteration 477, loss = 0.13008290\n",
      "Iteration 478, loss = 0.12975601\n",
      "Iteration 479, loss = 0.12932185\n",
      "Iteration 480, loss = 0.12862293\n",
      "Iteration 481, loss = 0.12705583\n",
      "Iteration 482, loss = 0.12747662\n",
      "Iteration 483, loss = 0.12517944\n",
      "Iteration 484, loss = 0.12539224\n",
      "Iteration 485, loss = 0.12432152\n",
      "Iteration 486, loss = 0.12508392\n",
      "Iteration 487, loss = 0.12249296\n",
      "Iteration 488, loss = 0.12209383\n",
      "Iteration 489, loss = 0.12024697\n",
      "Iteration 490, loss = 0.12098010\n",
      "Iteration 491, loss = 0.12024236\n",
      "Iteration 492, loss = 0.11984952\n",
      "Iteration 493, loss = 0.11782895\n",
      "Iteration 494, loss = 0.11921657\n",
      "Iteration 495, loss = 0.11875939\n",
      "Iteration 496, loss = 0.11773813\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 497, loss = 0.09041585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 498, loss = 0.08396361\n",
      "Iteration 499, loss = 0.08303653\n",
      "Iteration 500, loss = 0.08254243\n",
      "Iteration 501, loss = 0.08217942\n",
      "Iteration 502, loss = 0.08188091\n",
      "Iteration 503, loss = 0.08169943\n",
      "Iteration 504, loss = 0.08154065\n",
      "Iteration 505, loss = 0.08144026\n",
      "Iteration 506, loss = 0.08125897\n",
      "Iteration 507, loss = 0.08113643\n",
      "Iteration 508, loss = 0.08107832\n",
      "Iteration 509, loss = 0.08096414\n",
      "Iteration 510, loss = 0.08088313\n",
      "Iteration 511, loss = 0.08066396\n",
      "Iteration 512, loss = 0.08053013\n",
      "Iteration 513, loss = 0.08049206\n",
      "Iteration 514, loss = 0.08039910\n",
      "Iteration 515, loss = 0.08032497\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 516, loss = 0.07732643\n",
      "Iteration 517, loss = 0.07692767\n",
      "Iteration 518, loss = 0.07685091\n",
      "Iteration 519, loss = 0.07680810\n",
      "Iteration 520, loss = 0.07680314\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 521, loss = 0.07611699\n",
      "Iteration 522, loss = 0.07607770\n",
      "Iteration 523, loss = 0.07606482\n",
      "Iteration 524, loss = 0.07605717\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 525, loss = 0.07590054\n",
      "Iteration 526, loss = 0.07589707\n",
      "Iteration 527, loss = 0.07589539\n",
      "Iteration 528, loss = 0.07589394\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 529, loss = 0.07586069\n",
      "Iteration 530, loss = 0.07586050\n",
      "Iteration 531, loss = 0.07586020\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 532, loss = 0.07585349\n",
      "Iteration 533, loss = 0.07585343\n",
      "Iteration 534, loss = 0.07585338\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.999026\n",
      "Test set score: 0.057826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.01, learning_rate='adaptive',\n",
    "                    warm_start=True)\n",
    "\n",
    "mlp.fit(X_train, Y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, Y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('coefs_smvn_train_mvn_test', mlp.coefs_)\n",
    "np.save('intercepts_smvn_train_mvn_test', mlp.intercepts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = X_test\n",
    "Y_test2 = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "# smvn of train\n",
    "for speaker_id_str, feature_list in train.items():\n",
    "    data = []\n",
    "    speaker_id = idx[speaker_id_str]\n",
    "    # calc speaker level mean and std\n",
    "    for features in feature_list:\n",
    "        frames = features.T\n",
    "        for frame in frames:\n",
    "            data.append(frame)\n",
    "    data = np.array(data)\n",
    "    mean = data.mean(axis=0)\n",
    "    std = data.std(axis=0)\n",
    "\n",
    "    # s level normalize\n",
    "    for features in test[speaker_id_str]:\n",
    "        features = ((features.T - mean)/std).T\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            X_test.append(frame)\n",
    "            Y_test.append(speaker_id)\n",
    "            \n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.707323\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set score: %f\" % mlp.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# segment acc\n",
    "from scipy import stats\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "# test\n",
    "\n",
    "for speaker_id_str, feature_list in train.items():\n",
    "    data = []\n",
    "    speaker_id = idx[speaker_id_str]\n",
    "    # calc speaker level mean and std\n",
    "    for features in feature_list:\n",
    "        frames = features.T\n",
    "        for frame in frames:\n",
    "            data.append(frame)\n",
    "    data = np.array(data)\n",
    "    mean = data.mean(axis=0)\n",
    "    std = data.std(axis=0)\n",
    "\n",
    "    # s level normalize\n",
    "    for features in test[speaker_id_str]:\n",
    "        features = ((features.T - mean)/std).T\n",
    "        frames = concat(features)\n",
    "        x = []\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            x.append(frame)\n",
    "        pred = stats.mode(mlp.predict(x)).mode[0]\n",
    "        y_true.append(speaker_id)\n",
    "        y_pred.append(pred)\n",
    "print(sum(np.array(y_true) == np.array(y_pred))/len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
