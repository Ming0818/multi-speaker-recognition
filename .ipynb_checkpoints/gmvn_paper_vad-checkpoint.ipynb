{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 DR1/MCPM0/SA1.WAV\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "indir = '../data/TIMIT/TRAIN/'\n",
    "outdir = '../data/preprocessed/train/'\n",
    "\n",
    "flist = [\"DR1/MCPM0/SA1.WAV\", \"DR1/MCPM0/SA2.WAV\", \"DR1/MCPM0/SI1194.WAV\", \"DR1/MCPM0/SI1824.WAV\", \"DR1/MCPM0/SI564.WAV\", \"DR1/MCPM0/SX114.WAV\", \"DR1/MCPM0/SX204.WAV\", \"DR1/MCPM0/SX24.WAV\", \"DR1/MCPM0/SX294.WAV\", \"DR1/MCPM0/SX384.WAV\", \"DR1/MDAC0/SA1.WAV\", \"DR1/MDAC0/SA2.WAV\", \"DR1/MDAC0/SI1261.WAV\", \"DR1/MDAC0/SI1837.WAV\", \"DR1/MDAC0/SI631.WAV\", \"DR1/MDAC0/SX181.WAV\", \"DR1/MDAC0/SX271.WAV\", \"DR1/MDAC0/SX361.WAV\", \"DR1/MDAC0/SX451.WAV\", \"DR1/MDAC0/SX91.WAV\", \"DR1/MDPK0/SA1.WAV\", \"DR1/MDPK0/SA2.WAV\", \"DR1/MDPK0/SI1053.WAV\", \"DR1/MDPK0/SI1683.WAV\", \"DR1/MDPK0/SI552.WAV\", \"DR1/MDPK0/SX153.WAV\", \"DR1/MDPK0/SX243.WAV\", \"DR1/MDPK0/SX333.WAV\", \"DR1/MDPK0/SX423.WAV\", \"DR1/MDPK0/SX63.WAV\", \"DR1/MEDR0/SA1.WAV\", \"DR1/MEDR0/SA2.WAV\", \"DR1/MEDR0/SI1374.WAV\", \"DR1/MEDR0/SI2004.WAV\", \"DR1/MEDR0/SI744.WAV\", \"DR1/MEDR0/SX114.WAV\", \"DR1/MEDR0/SX204.WAV\", \"DR1/MEDR0/SX24.WAV\", \"DR1/MEDR0/SX294.WAV\", \"DR1/MEDR0/SX384.WAV\", \"DR1/MGRL0/SA1.WAV\", \"DR1/MGRL0/SA2.WAV\", \"DR1/MGRL0/SI1497.WAV\", \"DR1/MGRL0/SI2127.WAV\", \"DR1/MGRL0/SI867.WAV\", \"DR1/MGRL0/SX147.WAV\", \"DR1/MGRL0/SX237.WAV\", \"DR1/MGRL0/SX327.WAV\", \"DR1/MGRL0/SX417.WAV\", \"DR1/MGRL0/SX57.WAV\", \"DR1/MJEB1/SA1.WAV\", \"DR1/MJEB1/SA2.WAV\", \"DR1/MJEB1/SI1467.WAV\", \"DR1/MJEB1/SI2097.WAV\", \"DR1/MJEB1/SI837.WAV\", \"DR1/MJEB1/SX117.WAV\", \"DR1/MJEB1/SX207.WAV\", \"DR1/MJEB1/SX27.WAV\", \"DR1/MJEB1/SX297.WAV\", \"DR1/MJEB1/SX387.WAV\", \"DR1/MJWT0/SA1.WAV\", \"DR1/MJWT0/SA2.WAV\", \"DR1/MJWT0/SI1291.WAV\", \"DR1/MJWT0/SI1381.WAV\", \"DR1/MJWT0/SI751.WAV\", \"DR1/MJWT0/SX121.WAV\", \"DR1/MJWT0/SX211.WAV\", \"DR1/MJWT0/SX301.WAV\", \"DR1/MJWT0/SX31.WAV\", \"DR1/MJWT0/SX391.WAV\", \"DR1/MKLS0/SA1.WAV\", \"DR1/MKLS0/SA2.WAV\", \"DR1/MKLS0/SI1437.WAV\", \"DR1/MKLS0/SI1533.WAV\", \"DR1/MKLS0/SI2067.WAV\", \"DR1/MKLS0/SX177.WAV\", \"DR1/MKLS0/SX267.WAV\", \"DR1/MKLS0/SX357.WAV\", \"DR1/MKLS0/SX447.WAV\", \"DR1/MKLS0/SX87.WAV\", \"DR1/MKLW0/SA1.WAV\", \"DR1/MKLW0/SA2.WAV\", \"DR1/MKLW0/SI1571.WAV\", \"DR1/MKLW0/SI1844.WAV\", \"DR1/MKLW0/SI2201.WAV\", \"DR1/MKLW0/SX131.WAV\", \"DR1/MKLW0/SX221.WAV\", \"DR1/MKLW0/SX311.WAV\", \"DR1/MKLW0/SX401.WAV\", \"DR1/MKLW0/SX41.WAV\", \"DR1/MMGG0/SA1.WAV\", \"DR1/MMGG0/SA2.WAV\", \"DR1/MMGG0/SI1079.WAV\", \"DR1/MMGG0/SI1709.WAV\", \"DR1/MMGG0/SI2339.WAV\", \"DR1/MMGG0/SX179.WAV\", \"DR1/MMGG0/SX269.WAV\", \"DR1/MMGG0/SX359.WAV\", \"DR1/MMGG0/SX449.WAV\", \"DR1/MMGG0/SX89.WAV\", \"DR1/MMRP0/SA1.WAV\", \"DR1/MMRP0/SA2.WAV\", \"DR1/MMRP0/SI2034.WAV\", \"DR1/MMRP0/SI717.WAV\", \"DR1/MMRP0/SI774.WAV\", \"DR1/MMRP0/SX144.WAV\", \"DR1/MMRP0/SX234.WAV\", \"DR1/MMRP0/SX324.WAV\", \"DR1/MMRP0/SX414.WAV\", \"DR1/MMRP0/SX54.WAV\", \"DR1/MPGH0/SA1.WAV\", \"DR1/MPGH0/SA2.WAV\", \"DR1/MPGH0/SI1554.WAV\", \"DR1/MPGH0/SI675.WAV\", \"DR1/MPGH0/SI924.WAV\", \"DR1/MPGH0/SX114.WAV\", \"DR1/MPGH0/SX204.WAV\", \"DR1/MPGH0/SX24.WAV\", \"DR1/MPGH0/SX294.WAV\", \"DR1/MPGH0/SX384.WAV\", \"DR1/MPGR0/SA1.WAV\", \"DR1/MPGR0/SA2.WAV\", \"DR1/MPGR0/SI1410.WAV\", \"DR1/MPGR0/SI2040.WAV\", \"DR1/MPGR0/SI780.WAV\", \"DR1/MPGR0/SX150.WAV\", \"DR1/MPGR0/SX240.WAV\", \"DR1/MPGR0/SX330.WAV\", \"DR1/MPGR0/SX420.WAV\", \"DR1/MPGR0/SX60.WAV\", \"DR1/MPSW0/SA1.WAV\", \"DR1/MPSW0/SA2.WAV\", \"DR1/MPSW0/SI1067.WAV\", \"DR1/MPSW0/SI1697.WAV\", \"DR1/MPSW0/SI2327.WAV\", \"DR1/MPSW0/SX167.WAV\", \"DR1/MPSW0/SX24.WAV\", \"DR1/MPSW0/SX257.WAV\", \"DR1/MPSW0/SX437.WAV\", \"DR1/MPSW0/SX77.WAV\", \"DR1/MRAI0/SA1.WAV\", \"DR1/MRAI0/SA2.WAV\", \"DR1/MRAI0/SI1954.WAV\", \"DR1/MRAI0/SI2052.WAV\", \"DR1/MRAI0/SI792.WAV\", \"DR1/MRAI0/SX162.WAV\", \"DR1/MRAI0/SX252.WAV\", \"DR1/MRAI0/SX342.WAV\", \"DR1/MRAI0/SX432.WAV\", \"DR1/MRAI0/SX72.WAV\", \"DR1/MRCG0/SA1.WAV\", \"DR1/MRCG0/SA2.WAV\", \"DR1/MRCG0/SI1428.WAV\", \"DR1/MRCG0/SI2058.WAV\", \"DR1/MRCG0/SI798.WAV\", \"DR1/MRCG0/SX168.WAV\", \"DR1/MRCG0/SX258.WAV\", \"DR1/MRCG0/SX348.WAV\", \"DR1/MRCG0/SX438.WAV\", \"DR1/MRCG0/SX78.WAV\", \"DR1/MRDD0/SA1.WAV\", \"DR1/MRDD0/SA2.WAV\", \"DR1/MRDD0/SI1050.WAV\", \"DR1/MRDD0/SI1680.WAV\", \"DR1/MRDD0/SI2310.WAV\", \"DR1/MRDD0/SX150.WAV\", \"DR1/MRDD0/SX240.WAV\", \"DR1/MRDD0/SX277.WAV\", \"DR1/MRDD0/SX330.WAV\", \"DR1/MRDD0/SX60.WAV\", \"DR1/MRSO0/SA1.WAV\", \"DR1/MRSO0/SA2.WAV\", \"DR1/MRSO0/SI1206.WAV\", \"DR1/MRSO0/SI1659.WAV\", \"DR1/MRSO0/SI2289.WAV\", \"DR1/MRSO0/SX129.WAV\", \"DR1/MRSO0/SX219.WAV\", \"DR1/MRSO0/SX309.WAV\", \"DR1/MRSO0/SX399.WAV\", \"DR1/MRSO0/SX39.WAV\", \"DR1/MRWS0/SA1.WAV\", \"DR1/MRWS0/SA2.WAV\", \"DR1/MRWS0/SI1102.WAV\", \"DR1/MRWS0/SI1732.WAV\", \"DR1/MRWS0/SI472.WAV\", \"DR1/MRWS0/SX112.WAV\", \"DR1/MRWS0/SX202.WAV\", \"DR1/MRWS0/SX22.WAV\", \"DR1/MRWS0/SX292.WAV\", \"DR1/MRWS0/SX382.WAV\", \"DR1/MTJS0/SA1.WAV\", \"DR1/MTJS0/SA2.WAV\", \"DR1/MTJS0/SI1192.WAV\", \"DR1/MTJS0/SI1822.WAV\", \"DR1/MTJS0/SI562.WAV\", \"DR1/MTJS0/SX112.WAV\", \"DR1/MTJS0/SX202.WAV\", \"DR1/MTJS0/SX22.WAV\", \"DR1/MTJS0/SX292.WAV\", \"DR1/MTJS0/SX382.WAV\", \"DR1/MTPF0/SA1.WAV\", \"DR1/MTPF0/SA2.WAV\", \"DR1/MTPF0/SI1235.WAV\", \"DR1/MTPF0/SI1865.WAV\", \"DR1/MTPF0/SI605.WAV\", \"DR1/MTPF0/SX155.WAV\", \"DR1/MTPF0/SX245.WAV\", \"DR1/MTPF0/SX335.WAV\", \"DR1/MTPF0/SX425.WAV\", \"DR1/MTPF0/SX65.WAV\", \"DR1/MTRR0/SA1.WAV\", \"DR1/MTRR0/SA2.WAV\", \"DR1/MTRR0/SI1548.WAV\", \"DR1/MTRR0/SI2178.WAV\", \"DR1/MTRR0/SI918.WAV\", \"DR1/MTRR0/SX108.WAV\", \"DR1/MTRR0/SX18.WAV\", \"DR1/MTRR0/SX198.WAV\", \"DR1/MTRR0/SX288.WAV\", \"DR1/MTRR0/SX378.WAV\", \"DR1/MWAD0/SA1.WAV\", \"DR1/MWAD0/SA2.WAV\", \"DR1/MWAD0/SI1062.WAV\", \"DR1/MWAD0/SI1749.WAV\", \"DR1/MWAD0/SI2322.WAV\", \"DR1/MWAD0/SX162.WAV\", \"DR1/MWAD0/SX252.WAV\", \"DR1/MWAD0/SX342.WAV\", \"DR1/MWAD0/SX432.WAV\", \"DR1/MWAD0/SX72.WAV\", \"DR1/MWAR0/SA1.WAV\", \"DR1/MWAR0/SA2.WAV\", \"DR1/MWAR0/SI1045.WAV\", \"DR1/MWAR0/SI1675.WAV\", \"DR1/MWAR0/SI2305.WAV\", \"DR1/MWAR0/SX145.WAV\", \"DR1/MWAR0/SX235.WAV\", \"DR1/MWAR0/SX325.WAV\", \"DR1/MWAR0/SX415.WAV\", \"DR1/MWAR0/SX55.WAV\", \"DR2/MARC0/SA1.WAV\", \"DR2/MARC0/SA2.WAV\", \"DR2/MARC0/SI1188.WAV\", \"DR2/MARC0/SI1818.WAV\", \"DR2/MARC0/SI558.WAV\", \"DR2/MARC0/SX108.WAV\", \"DR2/MARC0/SX18.WAV\", \"DR2/MARC0/SX198.WAV\", \"DR2/MARC0/SX288.WAV\", \"DR2/MARC0/SX378.WAV\", \"DR2/MBJV0/SA1.WAV\", \"DR2/MBJV0/SA2.WAV\", \"DR2/MBJV0/SI1247.WAV\", \"DR2/MBJV0/SI1877.WAV\", \"DR2/MBJV0/SI617.WAV\", \"DR2/MBJV0/SX167.WAV\", \"DR2/MBJV0/SX257.WAV\", \"DR2/MBJV0/SX347.WAV\", \"DR2/MBJV0/SX437.WAV\", \"DR2/MBJV0/SX77.WAV\", \"DR2/MCEW0/SA1.WAV\", \"DR2/MCEW0/SA2.WAV\", \"DR2/MCEW0/SI1442.WAV\", \"DR2/MCEW0/SI2072.WAV\", \"DR2/MCEW0/SI812.WAV\", \"DR2/MCEW0/SX182.WAV\", \"DR2/MCEW0/SX272.WAV\", \"DR2/MCEW0/SX362.WAV\", \"DR2/MCEW0/SX452.WAV\", \"DR2/MCEW0/SX92.WAV\", \"DR2/MCTM0/SA1.WAV\", \"DR2/MCTM0/SA2.WAV\", \"DR2/MCTM0/SI1350.WAV\", \"DR2/MCTM0/SI1980.WAV\", \"DR2/MCTM0/SI720.WAV\", \"DR2/MCTM0/SX180.WAV\", \"DR2/MCTM0/SX270.WAV\", \"DR2/MCTM0/SX360.WAV\", \"DR2/MCTM0/SX450.WAV\", \"DR2/MCTM0/SX90.WAV\", \"DR2/MDBP0/SA1.WAV\", \"DR2/MDBP0/SA2.WAV\", \"DR2/MDBP0/SI1158.WAV\", \"DR2/MDBP0/SI1788.WAV\", \"DR2/MDBP0/SI528.WAV\", \"DR2/MDBP0/SX168.WAV\", \"DR2/MDBP0/SX258.WAV\", \"DR2/MDBP0/SX348.WAV\", \"DR2/MDBP0/SX438.WAV\", \"DR2/MDBP0/SX78.WAV\", \"DR2/MDEM0/SA1.WAV\", \"DR2/MDEM0/SA2.WAV\", \"DR2/MDEM0/SI1868.WAV\", \"DR2/MDEM0/SI608.WAV\", \"DR2/MDEM0/SI800.WAV\", \"DR2/MDEM0/SX158.WAV\", \"DR2/MDEM0/SX248.WAV\", \"DR2/MDEM0/SX338.WAV\", \"DR2/MDEM0/SX428.WAV\", \"DR2/MDEM0/SX68.WAV\", \"DR2/MDLB0/SA1.WAV\", \"DR2/MDLB0/SA2.WAV\", \"DR2/MDLB0/SI1306.WAV\", \"DR2/MDLB0/SI1936.WAV\", \"DR2/MDLB0/SI676.WAV\", \"DR2/MDLB0/SX136.WAV\", \"DR2/MDLB0/SX226.WAV\", \"DR2/MDLB0/SX316.WAV\", \"DR2/MDLB0/SX406.WAV\", \"DR2/MDLB0/SX46.WAV\", \"DR2/MDLC2/SA1.WAV\", \"DR2/MDLC2/SA2.WAV\", \"DR2/MDLC2/SI1614.WAV\", \"DR2/MDLC2/SI2244.WAV\", \"DR2/MDLC2/SI984.WAV\", \"DR2/MDLC2/SX174.WAV\", \"DR2/MDLC2/SX264.WAV\", \"DR2/MDLC2/SX354.WAV\", \"DR2/MDLC2/SX444.WAV\", \"DR2/MDLC2/SX84.WAV\", \"DR2/MDMT0/SA1.WAV\", \"DR2/MDMT0/SA2.WAV\", \"DR2/MDMT0/SI1832.WAV\", \"DR2/MDMT0/SI2341.WAV\", \"DR2/MDMT0/SI572.WAV\", \"DR2/MDMT0/SX122.WAV\", \"DR2/MDMT0/SX212.WAV\", \"DR2/MDMT0/SX302.WAV\", \"DR2/MDMT0/SX32.WAV\", \"DR2/MDMT0/SX392.WAV\", \"DR2/MDPS0/SA1.WAV\", \"DR2/MDPS0/SA2.WAV\", \"DR2/MDPS0/SI1651.WAV\", \"DR2/MDPS0/SI1979.WAV\", \"DR2/MDPS0/SI719.WAV\", \"DR2/MDPS0/SX179.WAV\", \"DR2/MDPS0/SX269.WAV\", \"DR2/MDPS0/SX359.WAV\", \"DR2/MDPS0/SX449.WAV\", \"DR2/MDPS0/SX89.WAV\", \"DR2/MDSS0/SA1.WAV\", \"DR2/MDSS0/SA2.WAV\", \"DR2/MDSS0/SI1881.WAV\", \"DR2/MDSS0/SI2087.WAV\", \"DR2/MDSS0/SI621.WAV\", \"DR2/MDSS0/SX171.WAV\", \"DR2/MDSS0/SX261.WAV\", \"DR2/MDSS0/SX351.WAV\", \"DR2/MDSS0/SX441.WAV\", \"DR2/MDSS0/SX81.WAV\", \"DR2/MDWD0/SA1.WAV\", \"DR2/MDWD0/SA2.WAV\", \"DR2/MDWD0/SI1260.WAV\", \"DR2/MDWD0/SI1890.WAV\", \"DR2/MDWD0/SI557.WAV\", \"DR2/MDWD0/SX180.WAV\", \"DR2/MDWD0/SX270.WAV\", \"DR2/MDWD0/SX360.WAV\", \"DR2/MDWD0/SX450.WAV\", \"DR2/MDWD0/SX90.WAV\", \"DR2/MEFG0/SA1.WAV\", \"DR2/MEFG0/SA2.WAV\", \"DR2/MEFG0/SI465.WAV\", \"DR2/MEFG0/SI491.WAV\", \"DR2/MEFG0/SI598.WAV\", \"DR2/MEFG0/SX105.WAV\", \"DR2/MEFG0/SX15.WAV\", \"DR2/MEFG0/SX195.WAV\", \"DR2/MEFG0/SX285.WAV\", \"DR2/MEFG0/SX375.WAV\", \"DR2/MHRM0/SA1.WAV\", \"DR2/MHRM0/SA2.WAV\", \"DR2/MHRM0/SI1475.WAV\", \"DR2/MHRM0/SI2218.WAV\", \"DR2/MHRM0/SI958.WAV\", \"DR2/MHRM0/SX148.WAV\", \"DR2/MHRM0/SX238.WAV\", \"DR2/MHRM0/SX328.WAV\", \"DR2/MHRM0/SX418.WAV\", \"DR2/MHRM0/SX58.WAV\", \"DR2/MJAE0/SA1.WAV\", \"DR2/MJAE0/SA2.WAV\", \"DR2/MJAE0/SI1524.WAV\", \"DR2/MJAE0/SI1999.WAV\", \"DR2/MJAE0/SI2154.WAV\", \"DR2/MJAE0/SX174.WAV\", \"DR2/MJAE0/SX264.WAV\", \"DR2/MJAE0/SX354.WAV\", \"DR2/MJAE0/SX444.WAV\", \"DR2/MJAE0/SX84.WAV\", \"DR2/MJBG0/SA1.WAV\", \"DR2/MJBG0/SA2.WAV\", \"DR2/MJBG0/SI1232.WAV\", \"DR2/MJBG0/SI1724.WAV\", \"DR2/MJBG0/SI1862.WAV\", \"DR2/MJBG0/SX152.WAV\", \"DR2/MJBG0/SX242.WAV\", \"DR2/MJBG0/SX332.WAV\", \"DR2/MJBG0/SX422.WAV\", \"DR2/MJBG0/SX62.WAV\", \"DR2/MJDE0/SA1.WAV\", \"DR2/MJDE0/SA2.WAV\", \"DR2/MJDE0/SI1120.WAV\", \"DR2/MJDE0/SI463.WAV\", \"DR2/MJDE0/SI490.WAV\", \"DR2/MJDE0/SX130.WAV\", \"DR2/MJDE0/SX220.WAV\", \"DR2/MJDE0/SX310.WAV\", \"DR2/MJDE0/SX400.WAV\", \"DR2/MJDE0/SX40.WAV\", \"DR2/MJEB0/SA1.WAV\", \"DR2/MJEB0/SA2.WAV\", \"DR2/MJEB0/SI1286.WAV\", \"DR2/MJEB0/SI1916.WAV\", \"DR2/MJEB0/SI656.WAV\", \"DR2/MJEB0/SX170.WAV\", \"DR2/MJEB0/SX206.WAV\", \"DR2/MJEB0/SX26.WAV\", \"DR2/MJEB0/SX296.WAV\", \"DR2/MJEB0/SX386.WAV\", \"DR2/MJHI0/SA1.WAV\", \"DR2/MJHI0/SA2.WAV\", \"DR2/MJHI0/SI1328.WAV\", \"DR2/MJHI0/SI555.WAV\", \"DR2/MJHI0/SI698.WAV\", \"DR2/MJHI0/SX158.WAV\", \"DR2/MJHI0/SX248.WAV\", \"DR2/MJHI0/SX338.WAV\", \"DR2/MJHI0/SX428.WAV\", \"DR2/MJHI0/SX68.WAV\", \"DR2/MJMA0/SA1.WAV\", \"DR2/MJMA0/SA2.WAV\", \"DR2/MJMA0/SI1495.WAV\", \"DR2/MJMA0/SI2125.WAV\", \"DR2/MJMA0/SI865.WAV\", \"DR2/MJMA0/SX145.WAV\", \"DR2/MJMA0/SX235.WAV\", \"DR2/MJMA0/SX325.WAV\", \"DR2/MJMA0/SX415.WAV\", \"DR2/MJMA0/SX55.WAV\", \"DR2/MJMD0/SA1.WAV\", \"DR2/MJMD0/SA2.WAV\", \"DR2/MJMD0/SI1028.WAV\", \"DR2/MJMD0/SI1658.WAV\", \"DR2/MJMD0/SI2288.WAV\", \"DR2/MJMD0/SX128.WAV\", \"DR2/MJMD0/SX218.WAV\", \"DR2/MJMD0/SX308.WAV\", \"DR2/MJMD0/SX38.WAV\", \"DR2/MJMD0/SX398.WAV\", \"DR2/MJPM0/SA1.WAV\", \"DR2/MJPM0/SA2.WAV\", \"DR2/MJPM0/SI1368.WAV\", \"DR2/MJPM0/SI1998.WAV\", \"DR2/MJPM0/SI738.WAV\", \"DR2/MJPM0/SX108.WAV\", \"DR2/MJPM0/SX18.WAV\", \"DR2/MJPM0/SX198.WAV\", \"DR2/MJPM0/SX288.WAV\", \"DR2/MJPM0/SX378.WAV\", \"DR2/MJRP0/SA1.WAV\", \"DR2/MJRP0/SA2.WAV\", \"DR2/MJRP0/SI1835.WAV\", \"DR2/MJRP0/SI1845.WAV\", \"DR2/MJRP0/SI585.WAV\", \"DR2/MJRP0/SX135.WAV\", \"DR2/MJRP0/SX225.WAV\", \"DR2/MJRP0/SX315.WAV\", \"DR2/MJRP0/SX405.WAV\", \"DR2/MJRP0/SX45.WAV\", \"DR2/MKAH0/SA1.WAV\", \"DR2/MKAH0/SA2.WAV\", \"DR2/MKAH0/SI1528.WAV\", \"DR2/MKAH0/SI2158.WAV\", \"DR2/MKAH0/SI898.WAV\", \"DR2/MKAH0/SX178.WAV\", \"DR2/MKAH0/SX268.WAV\", \"DR2/MKAH0/SX358.WAV\", \"DR2/MKAH0/SX448.WAV\", \"DR2/MKAH0/SX88.WAV\", \"DR2/MKAJ0/SA1.WAV\", \"DR2/MKAJ0/SA2.WAV\", \"DR2/MKAJ0/SI1414.WAV\", \"DR2/MKAJ0/SI2044.WAV\", \"DR2/MKAJ0/SI784.WAV\", \"DR2/MKAJ0/SX154.WAV\", \"DR2/MKAJ0/SX244.WAV\", \"DR2/MKAJ0/SX334.WAV\", \"DR2/MKAJ0/SX424.WAV\", \"DR2/MKAJ0/SX64.WAV\", \"DR2/MKDT0/SA1.WAV\", \"DR2/MKDT0/SA2.WAV\", \"DR2/MKDT0/SI2153.WAV\", \"DR2/MKDT0/SI814.WAV\", \"DR2/MKDT0/SI893.WAV\", \"DR2/MKDT0/SX173.WAV\", \"DR2/MKDT0/SX263.WAV\", \"DR2/MKDT0/SX353.WAV\", \"DR2/MKDT0/SX443.WAV\", \"DR2/MKDT0/SX83.WAV\", \"DR2/MKJO0/SA1.WAV\", \"DR2/MKJO0/SA2.WAV\", \"DR2/MKJO0/SI1517.WAV\", \"DR2/MKJO0/SI2147.WAV\", \"DR2/MKJO0/SI887.WAV\", \"DR2/MKJO0/SX167.WAV\", \"DR2/MKJO0/SX257.WAV\", \"DR2/MKJO0/SX424.WAV\", \"DR2/MKJO0/SX437.WAV\", \"DR2/MKJO0/SX77.WAV\", \"DR2/MMAA0/SA1.WAV\", \"DR2/MMAA0/SA2.WAV\", \"DR2/MMAA0/SI1588.WAV\", \"DR2/MMAA0/SI2105.WAV\", \"DR2/MMAA0/SI845.WAV\", \"DR2/MMAA0/SX125.WAV\", \"DR2/MMAA0/SX215.WAV\", \"DR2/MMAA0/SX305.WAV\", \"DR2/MMAA0/SX35.WAV\", \"DR2/MMAA0/SX395.WAV\", \"DR2/MMAG0/SA1.WAV\", \"DR2/MMAG0/SA2.WAV\", \"DR2/MMAG0/SI1126.WAV\", \"DR2/MMAG0/SI1756.WAV\", \"DR2/MMAG0/SI496.WAV\", \"DR2/MMAG0/SX136.WAV\", \"DR2/MMAG0/SX226.WAV\", \"DR2/MMAG0/SX316.WAV\", \"DR2/MMAG0/SX406.WAV\", \"DR2/MMAG0/SX46.WAV\", \"DR2/MMDS0/SA1.WAV\", \"DR2/MMDS0/SA2.WAV\", \"DR2/MMDS0/SI1343.WAV\", \"DR2/MMDS0/SI1973.WAV\", \"DR2/MMDS0/SI713.WAV\", \"DR2/MMDS0/SX173.WAV\", \"DR2/MMDS0/SX263.WAV\", \"DR2/MMDS0/SX353.WAV\", \"DR2/MMDS0/SX443.WAV\", \"DR2/MMDS0/SX83.WAV\", \"DR2/MMGK0/SA1.WAV\", \"DR2/MMGK0/SA2.WAV\", \"DR2/MMGK0/SI1322.WAV\", \"DR2/MMGK0/SI1952.WAV\", \"DR2/MMGK0/SI692.WAV\", \"DR2/MMGK0/SX152.WAV\", \"DR2/MMGK0/SX242.WAV\", \"DR2/MMGK0/SX332.WAV\", \"DR2/MMGK0/SX422.WAV\", \"DR2/MMGK0/SX62.WAV\", \"DR2/MMXS0/SA1.WAV\", \"DR2/MMXS0/SA2.WAV\", \"DR2/MMXS0/SI2136.WAV\", \"DR2/MMXS0/SI629.WAV\", \"DR2/MMXS0/SI876.WAV\", \"DR2/MMXS0/SX156.WAV\", \"DR2/MMXS0/SX246.WAV\", \"DR2/MMXS0/SX336.WAV\", \"DR2/MMXS0/SX426.WAV\", \"DR2/MMXS0/SX66.WAV\", \"DR2/MPPC0/SA1.WAV\", \"DR2/MPPC0/SA2.WAV\", \"DR2/MPPC0/SI1412.WAV\", \"DR2/MPPC0/SI2042.WAV\", \"DR2/MPPC0/SI782.WAV\", \"DR2/MPPC0/SX152.WAV\", \"DR2/MPPC0/SX242.WAV\", \"DR2/MPPC0/SX332.WAV\", \"DR2/MPPC0/SX422.WAV\", \"DR2/MPPC0/SX62.WAV\", \"DR2/MPRB0/SA1.WAV\", \"DR2/MPRB0/SA2.WAV\", \"DR2/MPRB0/SI1205.WAV\", \"DR2/MPRB0/SI1215.WAV\", \"DR2/MPRB0/SI575.WAV\", \"DR2/MPRB0/SX125.WAV\", \"DR2/MPRB0/SX215.WAV\", \"DR2/MPRB0/SX305.WAV\", \"DR2/MPRB0/SX35.WAV\", \"DR2/MPRB0/SX395.WAV\", \"DR2/MRAB0/SA1.WAV\", \"DR2/MRAB0/SA2.WAV\", \"DR2/MRAB0/SI1224.WAV\", \"DR2/MRAB0/SI1854.WAV\", \"DR2/MRAB0/SI594.WAV\", \"DR2/MRAB0/SX144.WAV\", \"DR2/MRAB0/SX234.WAV\", \"DR2/MRAB0/SX324.WAV\", \"DR2/MRAB0/SX414.WAV\", \"DR2/MRAB0/SX54.WAV\", \"DR2/MRCW0/SA1.WAV\", \"DR2/MRCW0/SA2.WAV\", \"DR2/MRCW0/SI1371.WAV\", \"DR2/MRCW0/SI2001.WAV\", \"DR2/MRCW0/SI741.WAV\", \"DR2/MRCW0/SX111.WAV\", \"DR2/MRCW0/SX201.WAV\", \"DR2/MRCW0/SX21.WAV\", \"DR2/MRCW0/SX291.WAV\", \"DR2/MRCW0/SX381.WAV\", \"DR2/MRFK0/SA1.WAV\", \"DR2/MRFK0/SA2.WAV\", \"DR2/MRFK0/SI1076.WAV\", \"DR2/MRFK0/SI1706.WAV\", \"DR2/MRFK0/SI2336.WAV\", \"DR2/MRFK0/SX176.WAV\", \"DR2/MRFK0/SX266.WAV\", \"DR2/MRFK0/SX356.WAV\", \"DR2/MRFK0/SX446.WAV\", \"DR2/MRFK0/SX86.WAV\", \"DR2/MRGS0/SA1.WAV\", \"DR2/MRGS0/SA2.WAV\", \"DR2/MRGS0/SI1356.WAV\", \"DR2/MRGS0/SI1986.WAV\", \"DR2/MRGS0/SI726.WAV\", \"DR2/MRGS0/SX186.WAV\", \"DR2/MRGS0/SX276.WAV\", \"DR2/MRGS0/SX366.WAV\", \"DR2/MRGS0/SX6.WAV\", \"DR2/MRGS0/SX96.WAV\", \"DR2/MRHL0/SA1.WAV\", \"DR2/MRHL0/SA2.WAV\", \"DR2/MRHL0/SI1515.WAV\", \"DR2/MRHL0/SI2145.WAV\", \"DR2/MRHL0/SI885.WAV\", \"DR2/MRHL0/SX165.WAV\", \"DR2/MRHL0/SX255.WAV\", \"DR2/MRHL0/SX345.WAV\", \"DR2/MRHL0/SX435.WAV\", \"DR2/MRHL0/SX75.WAV\", \"DR2/MRJH0/SA1.WAV\", \"DR2/MRJH0/SA2.WAV\", \"DR2/MRJH0/SI1519.WAV\", \"DR2/MRJH0/SI889.WAV\", \"DR2/MRJH0/SI914.WAV\", \"DR2/MRJH0/SX169.WAV\", \"DR2/MRJH0/SX259.WAV\", \"DR2/MRJH0/SX307.WAV\", \"DR2/MRJH0/SX439.WAV\", \"DR2/MRJH0/SX79.WAV\", \"DR2/MRJM0/SA1.WAV\", \"DR2/MRJM0/SA2.WAV\", \"DR2/MRJM0/SI1095.WAV\", \"DR2/MRJM0/SI1228.WAV\", \"DR2/MRJM0/SI1858.WAV\", \"DR2/MRJM0/SX148.WAV\", \"DR2/MRJM0/SX238.WAV\", \"DR2/MRJM0/SX328.WAV\", \"DR2/MRJM0/SX418.WAV\", \"DR2/MRJM0/SX58.WAV\", \"DR2/MRJM1/SA1.WAV\", \"DR2/MRJM1/SA2.WAV\", \"DR2/MRJM1/SI1298.WAV\", \"DR2/MRJM1/SI1928.WAV\", \"DR2/MRJM1/SI668.WAV\", \"DR2/MRJM1/SX128.WAV\", \"DR2/MRJM1/SX218.WAV\", \"DR2/MRJM1/SX308.WAV\", \"DR2/MRJM1/SX38.WAV\", \"DR2/MRJM1/SX398.WAV\", \"DR2/MRJT0/SA1.WAV\", \"DR2/MRJT0/SA2.WAV\", \"DR2/MRJT0/SI1498.WAV\", \"DR2/MRJT0/SI1805.WAV\", \"DR2/MRJT0/SI868.WAV\", \"DR2/MRJT0/SX148.WAV\", \"DR2/MRJT0/SX238.WAV\", \"DR2/MRJT0/SX328.WAV\", \"DR2/MRJT0/SX418.WAV\", \"DR2/MRJT0/SX58.WAV\", \"DR2/MRLJ0/SA1.WAV\", \"DR2/MRLJ0/SA2.WAV\", \"DR2/MRLJ0/SI1420.WAV\", \"DR2/MRLJ0/SI2050.WAV\", \"DR2/MRLJ0/SI790.WAV\", \"DR2/MRLJ0/SX160.WAV\", \"DR2/MRLJ0/SX250.WAV\", \"DR2/MRLJ0/SX340.WAV\", \"DR2/MRLJ0/SX430.WAV\", \"DR2/MRLJ0/SX70.WAV\", \"DR2/MRLR0/SA1.WAV\", \"DR2/MRLR0/SA2.WAV\", \"DR2/MRLR0/SI1196.WAV\", \"DR2/MRLR0/SI1826.WAV\", \"DR2/MRLR0/SI566.WAV\", \"DR2/MRLR0/SX116.WAV\", \"DR2/MRLR0/SX206.WAV\", \"DR2/MRLR0/SX26.WAV\", \"DR2/MRLR0/SX296.WAV\", \"DR2/MRLR0/SX386.WAV\", \"DR2/MRMS0/SA1.WAV\", \"DR2/MRMS0/SA2.WAV\", \"DR2/MRMS0/SI1113.WAV\", \"DR2/MRMS0/SI2057.WAV\", \"DR2/MRMS0/SI2100.WAV\", \"DR2/MRMS0/SX120.WAV\", \"DR2/MRMS0/SX210.WAV\", \"DR2/MRMS0/SX300.WAV\", \"DR2/MRMS0/SX30.WAV\", \"DR2/MRMS0/SX390.WAV\", \"DR2/MSAT0/SA1.WAV\", \"DR2/MSAT0/SA2.WAV\", \"DR2/MSAT0/SI1526.WAV\", \"DR2/MSAT0/SI2156.WAV\", \"DR2/MSAT0/SI896.WAV\", \"DR2/MSAT0/SX176.WAV\", \"DR2/MSAT0/SX266.WAV\", \"DR2/MSAT0/SX356.WAV\", \"DR2/MSAT0/SX446.WAV\", \"DR2/MSAT0/SX86.WAV\", \"DR2/MTAT1/SA1.WAV\", \"DR2/MTAT1/SA2.WAV\", \"DR2/MTAT1/SI1409.WAV\", \"DR2/MTAT1/SI1627.WAV\", \"DR2/MTAT1/SI779.WAV\", \"DR2/MTAT1/SX149.WAV\", \"DR2/MTAT1/SX239.WAV\", \"DR2/MTAT1/SX329.WAV\", \"DR2/MTAT1/SX419.WAV\", \"DR2/MTAT1/SX59.WAV\", \"DR2/MTBC0/SA1.WAV\", \"DR2/MTBC0/SA2.WAV\", \"DR2/MTBC0/SI1173.WAV\", \"DR2/MTBC0/SI1803.WAV\", \"DR2/MTBC0/SI543.WAV\", \"DR2/MTBC0/SX183.WAV\", \"DR2/MTBC0/SX273.WAV\", \"DR2/MTBC0/SX347.WAV\", \"DR2/MTBC0/SX363.WAV\", \"DR2/MTBC0/SX93.WAV\", \"DR2/MTDB0/SA1.WAV\", \"DR2/MTDB0/SA2.WAV\", \"DR2/MTDB0/SI1401.WAV\", \"DR2/MTDB0/SI2031.WAV\", \"DR2/MTDB0/SI771.WAV\", \"DR2/MTDB0/SX141.WAV\", \"DR2/MTDB0/SX231.WAV\", \"DR2/MTDB0/SX321.WAV\", \"DR2/MTDB0/SX411.WAV\", \"DR2/MTDB0/SX51.WAV\", \"DR2/MTJG0/SA1.WAV\", \"DR2/MTJG0/SA2.WAV\", \"DR2/MTJG0/SI1520.WAV\", \"DR2/MTJG0/SI2157.WAV\", \"DR2/MTJG0/SI890.WAV\", \"DR2/MTJG0/SX170.WAV\", \"DR2/MTJG0/SX260.WAV\", \"DR2/MTJG0/SX350.WAV\", \"DR2/MTJG0/SX440.WAV\", \"DR2/MTJG0/SX80.WAV\", \"DR2/MWSB0/SA1.WAV\", \"DR2/MWSB0/SA2.WAV\", \"DR2/MWSB0/SI1626.WAV\", \"DR2/MWSB0/SI2256.WAV\", \"DR2/MWSB0/SI996.WAV\", \"DR2/MWSB0/SX186.WAV\", \"DR2/MWSB0/SX276.WAV\", \"DR2/MWSB0/SX366.WAV\", \"DR2/MWSB0/SX6.WAV\", \"DR2/MWSB0/SX96.WAV\", \"DR2/MZMB0/SA1.WAV\", \"DR2/MZMB0/SA2.WAV\", \"DR2/MZMB0/SI1166.WAV\", \"DR2/MZMB0/SI1796.WAV\", \"DR2/MZMB0/SI536.WAV\", \"DR2/MZMB0/SX176.WAV\", \"DR2/MZMB0/SX266.WAV\", \"DR2/MZMB0/SX356.WAV\", \"DR2/MZMB0/SX446.WAV\", \"DR2/MZMB0/SX86.WAV\", \"DR3/MADC0/SA1.WAV\", \"DR3/MADC0/SA2.WAV\", \"DR3/MADC0/SI1367.WAV\", \"DR3/MADC0/SI1997.WAV\", \"DR3/MADC0/SI737.WAV\", \"DR3/MADC0/SX107.WAV\", \"DR3/MADC0/SX17.WAV\", \"DR3/MADC0/SX197.WAV\", \"DR3/MADC0/SX287.WAV\", \"DR3/MADC0/SX377.WAV\", \"DR3/MAKB0/SA1.WAV\", \"DR3/MAKB0/SA2.WAV\", \"DR3/MAKB0/SI1016.WAV\", \"DR3/MAKB0/SI1646.WAV\", \"DR3/MAKB0/SI2276.WAV\", \"DR3/MAKB0/SX116.WAV\", \"DR3/MAKB0/SX206.WAV\", \"DR3/MAKB0/SX26.WAV\", \"DR3/MAKB0/SX296.WAV\", \"DR3/MAKB0/SX386.WAV\", \"DR3/MAKR0/SA1.WAV\", \"DR3/MAKR0/SA2.WAV\", \"DR3/MAKR0/SI1352.WAV\", \"DR3/MAKR0/SI1982.WAV\", \"DR3/MAKR0/SI722.WAV\", \"DR3/MAKR0/SX182.WAV\", \"DR3/MAKR0/SX272.WAV\", \"DR3/MAKR0/SX362.WAV\", \"DR3/MAKR0/SX452.WAV\", \"DR3/MAKR0/SX92.WAV\", \"DR3/MAPV0/SA1.WAV\", \"DR3/MAPV0/SA2.WAV\", \"DR3/MAPV0/SI1293.WAV\", \"DR3/MAPV0/SI1923.WAV\", \"DR3/MAPV0/SI663.WAV\", \"DR3/MAPV0/SX123.WAV\", \"DR3/MAPV0/SX213.WAV\", \"DR3/MAPV0/SX303.WAV\", \"DR3/MAPV0/SX33.WAV\", \"DR3/MAPV0/SX393.WAV\", \"DR3/MBEF0/SA1.WAV\", \"DR3/MBEF0/SA2.WAV\", \"DR3/MBEF0/SI1281.WAV\", \"DR3/MBEF0/SI1911.WAV\", \"DR3/MBEF0/SI651.WAV\", \"DR3/MBEF0/SX111.WAV\", \"DR3/MBEF0/SX201.WAV\", \"DR3/MBEF0/SX21.WAV\", \"DR3/MBEF0/SX291.WAV\", \"DR3/MBEF0/SX381.WAV\", \"DR3/MCAL0/SA1.WAV\", \"DR3/MCAL0/SA2.WAV\", \"DR3/MCAL0/SI1138.WAV\", \"DR3/MCAL0/SI1768.WAV\", \"DR3/MCAL0/SI508.WAV\", \"DR3/MCAL0/SX148.WAV\", \"DR3/MCAL0/SX238.WAV\", \"DR3/MCAL0/SX328.WAV\", \"DR3/MCAL0/SX418.WAV\", \"DR3/MCAL0/SX58.WAV\", \"DR3/MCDC0/SA1.WAV\", \"DR3/MCDC0/SA2.WAV\", \"DR3/MCDC0/SI1292.WAV\", \"DR3/MCDC0/SI1922.WAV\", \"DR3/MCDC0/SI662.WAV\", \"DR3/MCDC0/SX122.WAV\", \"DR3/MCDC0/SX212.WAV\", \"DR3/MCDC0/SX302.WAV\", \"DR3/MCDC0/SX32.WAV\", \"DR3/MCDC0/SX392.WAV\", \"DR3/MCDD0/SA1.WAV\", \"DR3/MCDD0/SA2.WAV\", \"DR3/MCDD0/SI1513.WAV\", \"DR3/MCDD0/SI2143.WAV\", \"DR3/MCDD0/SI883.WAV\", \"DR3/MCDD0/SX163.WAV\", \"DR3/MCDD0/SX253.WAV\", \"DR3/MCDD0/SX343.WAV\", \"DR3/MCDD0/SX433.WAV\", \"DR3/MCDD0/SX73.WAV\", \"DR3/MCEF0/SA1.WAV\", \"DR3/MCEF0/SA2.WAV\", \"DR3/MCEF0/SI1135.WAV\", \"DR3/MCEF0/SI1765.WAV\", \"DR3/MCEF0/SI842.WAV\", \"DR3/MCEF0/SX145.WAV\", \"DR3/MCEF0/SX235.WAV\", \"DR3/MCEF0/SX325.WAV\", \"DR3/MCEF0/SX415.WAV\", \"DR3/MCEF0/SX55.WAV\", \"DR3/MDBB1/SA1.WAV\", \"DR3/MDBB1/SA2.WAV\", \"DR3/MDBB1/SI1006.WAV\", \"DR3/MDBB1/SI1636.WAV\", \"DR3/MDBB1/SI2056.WAV\", \"DR3/MDBB1/SX106.WAV\", \"DR3/MDBB1/SX16.WAV\", \"DR3/MDBB1/SX196.WAV\", \"DR3/MDBB1/SX286.WAV\", \"DR3/MDBB1/SX376.WAV\", \"DR3/MDDC0/SA1.WAV\", \"DR3/MDDC0/SA2.WAV\", \"DR3/MDDC0/SI1419.WAV\", \"DR3/MDDC0/SI2049.WAV\", \"DR3/MDDC0/SI789.WAV\", \"DR3/MDDC0/SX159.WAV\", \"DR3/MDDC0/SX249.WAV\", \"DR3/MDDC0/SX339.WAV\", \"DR3/MDDC0/SX429.WAV\", \"DR3/MDDC0/SX69.WAV\", \"DR3/MDEF0/SA1.WAV\", \"DR3/MDEF0/SA2.WAV\", \"DR3/MDEF0/SI1123.WAV\", \"DR3/MDEF0/SI1563.WAV\", \"DR3/MDEF0/SI2193.WAV\", \"DR3/MDEF0/SX123.WAV\", \"DR3/MDEF0/SX213.WAV\", \"DR3/MDEF0/SX303.WAV\", \"DR3/MDEF0/SX33.WAV\", \"DR3/MDEF0/SX393.WAV\", \"DR3/MDHS0/SA1.WAV\", \"DR3/MDHS0/SA2.WAV\", \"DR3/MDHS0/SI1530.WAV\", \"DR3/MDHS0/SI2160.WAV\", \"DR3/MDHS0/SI900.WAV\", \"DR3/MDHS0/SX180.WAV\", \"DR3/MDHS0/SX270.WAV\", \"DR3/MDHS0/SX360.WAV\", \"DR3/MDHS0/SX450.WAV\", \"DR3/MDHS0/SX90.WAV\", \"DR3/MDJM0/SA1.WAV\", \"DR3/MDJM0/SA2.WAV\", \"DR3/MDJM0/SI1455.WAV\", \"DR3/MDJM0/SI2085.WAV\", \"DR3/MDJM0/SI825.WAV\", \"DR3/MDJM0/SX105.WAV\", \"DR3/MDJM0/SX15.WAV\", \"DR3/MDJM0/SX195.WAV\", \"DR3/MDJM0/SX285.WAV\", \"DR3/MDJM0/SX375.WAV\", \"DR3/MDLC0/SA1.WAV\", \"DR3/MDLC0/SA2.WAV\", \"DR3/MDLC0/SI1395.WAV\", \"DR3/MDLC0/SI2025.WAV\", \"DR3/MDLC0/SI765.WAV\", \"DR3/MDLC0/SX135.WAV\", \"DR3/MDLC0/SX225.WAV\", \"DR3/MDLC0/SX315.WAV\", \"DR3/MDLC0/SX405.WAV\", \"DR3/MDLC0/SX45.WAV\", \"DR3/MDLH0/SA1.WAV\", \"DR3/MDLH0/SA2.WAV\", \"DR3/MDLH0/SI1960.WAV\", \"DR3/MDLH0/SI574.WAV\", \"DR3/MDLH0/SI700.WAV\", \"DR3/MDLH0/SX160.WAV\", \"DR3/MDLH0/SX250.WAV\", \"DR3/MDLH0/SX340.WAV\", \"DR3/MDLH0/SX430.WAV\", \"DR3/MDLH0/SX70.WAV\", \"DR3/MDNS0/SA1.WAV\", \"DR3/MDNS0/SA2.WAV\", \"DR3/MDNS0/SI1011.WAV\", \"DR3/MDNS0/SI2271.WAV\", \"DR3/MDNS0/SI873.WAV\", \"DR3/MDNS0/SX111.WAV\", \"DR3/MDNS0/SX201.WAV\", \"DR3/MDNS0/SX21.WAV\", \"DR3/MDNS0/SX291.WAV\", \"DR3/MDNS0/SX381.WAV\", \"DR3/MDSS1/SA1.WAV\", \"DR3/MDSS1/SA2.WAV\", \"DR3/MDSS1/SI1327.WAV\", \"DR3/MDSS1/SI1713.WAV\", \"DR3/MDSS1/SI697.WAV\", \"DR3/MDSS1/SX157.WAV\", \"DR3/MDSS1/SX247.WAV\", \"DR3/MDSS1/SX337.WAV\", \"DR3/MDSS1/SX427.WAV\", \"DR3/MDSS1/SX67.WAV\", \"DR3/MDTB0/SA1.WAV\", \"DR3/MDTB0/SA2.WAV\", \"DR3/MDTB0/SI1200.WAV\", \"DR3/MDTB0/SI1830.WAV\", \"DR3/MDTB0/SI570.WAV\", \"DR3/MDTB0/SX120.WAV\", \"DR3/MDTB0/SX210.WAV\", \"DR3/MDTB0/SX300.WAV\", \"DR3/MDTB0/SX321.WAV\", \"DR3/MDTB0/SX390.WAV\", \"DR3/MDWM0/SA1.WAV\", \"DR3/MDWM0/SA2.WAV\", \"DR3/MDWM0/SI1546.WAV\", \"DR3/MDWM0/SI2176.WAV\", \"DR3/MDWM0/SI916.WAV\", \"DR3/MDWM0/SX106.WAV\", \"DR3/MDWM0/SX16.WAV\", \"DR3/MDWM0/SX286.WAV\", \"DR3/MDWM0/SX376.WAV\", \"DR3/MDWM0/SX433.WAV\", \"DR3/MFMC0/SA1.WAV\", \"DR3/MFMC0/SA2.WAV\", \"DR3/MFMC0/SI1132.WAV\", \"DR3/MFMC0/SI1762.WAV\", \"DR3/MFMC0/SI502.WAV\", \"DR3/MFMC0/SX142.WAV\", \"DR3/MFMC0/SX232.WAV\", \"DR3/MFMC0/SX322.WAV\", \"DR3/MFMC0/SX412.WAV\", \"DR3/MFMC0/SX52.WAV\", \"DR3/MGAF0/SA1.WAV\", \"DR3/MGAF0/SA2.WAV\", \"DR3/MGAF0/SI1282.WAV\", \"DR3/MGAF0/SI1912.WAV\", \"DR3/MGAF0/SI652.WAV\", \"DR3/MGAF0/SX112.WAV\", \"DR3/MGAF0/SX202.WAV\", \"DR3/MGAF0/SX22.WAV\", \"DR3/MGAF0/SX292.WAV\", \"DR3/MGAF0/SX382.WAV\", \"DR3/MHJB0/SA1.WAV\", \"DR3/MHJB0/SA2.WAV\", \"DR3/MHJB0/SI1017.WAV\", \"DR3/MHJB0/SI1647.WAV\", \"DR3/MHJB0/SI2277.WAV\", \"DR3/MHJB0/SX117.WAV\", \"DR3/MHJB0/SX207.WAV\", \"DR3/MHJB0/SX27.WAV\", \"DR3/MHJB0/SX297.WAV\", \"DR3/MHJB0/SX387.WAV\", \"DR3/MHMR0/SA1.WAV\", \"DR3/MHMR0/SA2.WAV\", \"DR3/MHMR0/SI1119.WAV\", \"DR3/MHMR0/SI1692.WAV\", \"DR3/MHMR0/SI489.WAV\", \"DR3/MHMR0/SX129.WAV\", \"DR3/MHMR0/SX219.WAV\", \"DR3/MHMR0/SX309.WAV\", \"DR3/MHMR0/SX399.WAV\", \"DR3/MHMR0/SX39.WAV\", \"DR3/MILB0/SA1.WAV\", \"DR3/MILB0/SA2.WAV\", \"DR3/MILB0/SI2163.WAV\", \"DR3/MILB0/SI807.WAV\", \"DR3/MILB0/SI903.WAV\", \"DR3/MILB0/SX183.WAV\", \"DR3/MILB0/SX273.WAV\", \"DR3/MILB0/SX363.WAV\", \"DR3/MILB0/SX3.WAV\", \"DR3/MILB0/SX93.WAV\", \"DR3/MJDA0/SA1.WAV\", \"DR3/MJDA0/SA2.WAV\", \"DR3/MJDA0/SI1031.WAV\", \"DR3/MJDA0/SI1661.WAV\", \"DR3/MJDA0/SI2291.WAV\", \"DR3/MJDA0/SX131.WAV\", \"DR3/MJDA0/SX221.WAV\", \"DR3/MJDA0/SX311.WAV\", \"DR3/MJDA0/SX401.WAV\", \"DR3/MJDA0/SX41.WAV\", \"DR3/MJJB0/SA1.WAV\", \"DR3/MJJB0/SA2.WAV\", \"DR3/MJJB0/SI1139.WAV\", \"DR3/MJJB0/SI1277.WAV\", \"DR3/MJJB0/SI1769.WAV\", \"DR3/MJJB0/SX149.WAV\", \"DR3/MJJB0/SX239.WAV\", \"DR3/MJJB0/SX329.WAV\", \"DR3/MJJB0/SX419.WAV\", \"DR3/MJJB0/SX59.WAV\", \"DR3/MJKR0/SA1.WAV\", \"DR3/MJKR0/SA2.WAV\", \"DR3/MJKR0/SI1201.WAV\", \"DR3/MJKR0/SI1831.WAV\", \"DR3/MJKR0/SI571.WAV\", \"DR3/MJKR0/SX121.WAV\", \"DR3/MJKR0/SX211.WAV\", \"DR3/MJKR0/SX301.WAV\", \"DR3/MJKR0/SX31.WAV\", \"DR3/MJKR0/SX391.WAV\", \"DR3/MJLG1/SA1.WAV\", \"DR3/MJLG1/SA2.WAV\", \"DR3/MJLG1/SI1012.WAV\", \"DR3/MJLG1/SI1642.WAV\", \"DR3/MJLG1/SI2272.WAV\", \"DR3/MJLG1/SX112.WAV\", \"DR3/MJLG1/SX202.WAV\", \"DR3/MJLG1/SX22.WAV\", \"DR3/MJLG1/SX292.WAV\", \"DR3/MJLG1/SX382.WAV\", \"DR3/MJRH1/SA1.WAV\", \"DR3/MJRH1/SA2.WAV\", \"DR3/MJRH1/SI1558.WAV\", \"DR3/MJRH1/SI1774.WAV\", \"DR3/MJRH1/SI514.WAV\", \"DR3/MJRH1/SX154.WAV\", \"DR3/MJRH1/SX244.WAV\", \"DR3/MJRH1/SX334.WAV\", \"DR3/MJRH1/SX424.WAV\", \"DR3/MJRH1/SX64.WAV\", \"DR3/MKLS1/SA1.WAV\", \"DR3/MKLS1/SA2.WAV\", \"DR3/MKLS1/SI1545.WAV\", \"DR3/MKLS1/SI2175.WAV\", \"DR3/MKLS1/SI915.WAV\", \"DR3/MKLS1/SX105.WAV\", \"DR3/MKLS1/SX15.WAV\", \"DR3/MKLS1/SX195.WAV\", \"DR3/MKLS1/SX285.WAV\", \"DR3/MKLS1/SX375.WAV\", \"DR3/MKXL0/SA1.WAV\", \"DR3/MKXL0/SA2.WAV\", \"DR3/MKXL0/SI1185.WAV\", \"DR3/MKXL0/SI1815.WAV\", \"DR3/MKXL0/SI1958.WAV\", \"DR3/MKXL0/SX105.WAV\", \"DR3/MKXL0/SX15.WAV\", \"DR3/MKXL0/SX195.WAV\", \"DR3/MKXL0/SX285.WAV\", \"DR3/MKXL0/SX375.WAV\", \"DR3/MLNS0/SA1.WAV\", \"DR3/MLNS0/SA2.WAV\", \"DR3/MLNS0/SI1407.WAV\", \"DR3/MLNS0/SI2037.WAV\", \"DR3/MLNS0/SI777.WAV\", \"DR3/MLNS0/SX147.WAV\", \"DR3/MLNS0/SX237.WAV\", \"DR3/MLNS0/SX327.WAV\", \"DR3/MLNS0/SX417.WAV\", \"DR3/MLNS0/SX57.WAV\", \"DR3/MMAM0/SA1.WAV\", \"DR3/MMAM0/SA2.WAV\", \"DR3/MMAM0/SI1597.WAV\", \"DR3/MMAM0/SI1668.WAV\", \"DR3/MMAM0/SI2227.WAV\", \"DR3/MMAM0/SX157.WAV\", \"DR3/MMAM0/SX247.WAV\", \"DR3/MMAM0/SX337.WAV\", \"DR3/MMAM0/SX427.WAV\", \"DR3/MMAM0/SX67.WAV\", \"DR3/MMAR0/SA1.WAV\", \"DR3/MMAR0/SA2.WAV\", \"DR3/MMAR0/SI1336.WAV\", \"DR3/MMAR0/SI1966.WAV\", \"DR3/MMAR0/SI706.WAV\", \"DR3/MMAR0/SX166.WAV\", \"DR3/MMAR0/SX256.WAV\", \"DR3/MMAR0/SX346.WAV\", \"DR3/MMAR0/SX436.WAV\", \"DR3/MMAR0/SX76.WAV\", \"DR3/MMEB0/SA1.WAV\", \"DR3/MMEB0/SA2.WAV\", \"DR3/MMEB0/SI1357.WAV\", \"DR3/MMEB0/SI1987.WAV\", \"DR3/MMEB0/SI727.WAV\", \"DR3/MMEB0/SX187.WAV\", \"DR3/MMEB0/SX327.WAV\", \"DR3/MMEB0/SX367.WAV\", \"DR3/MMEB0/SX7.WAV\", \"DR3/MMEB0/SX97.WAV\", \"DR3/MMJB1/SA1.WAV\", \"DR3/MMJB1/SA2.WAV\", \"DR3/MMJB1/SI1408.WAV\", \"DR3/MMJB1/SI2038.WAV\", \"DR3/MMJB1/SI778.WAV\", \"DR3/MMJB1/SX148.WAV\", \"DR3/MMJB1/SX238.WAV\", \"DR3/MMJB1/SX328.WAV\", \"DR3/MMJB1/SX418.WAV\", \"DR3/MMJB1/SX58.WAV\", \"DR3/MMSM0/SA1.WAV\", \"DR3/MMSM0/SA2.WAV\", \"DR3/MMSM0/SI1106.WAV\", \"DR3/MMSM0/SI1736.WAV\", \"DR3/MMSM0/SI476.WAV\", \"DR3/MMSM0/SX116.WAV\", \"DR3/MMSM0/SX206.WAV\", \"DR3/MMSM0/SX26.WAV\", \"DR3/MMSM0/SX296.WAV\", \"DR3/MMSM0/SX386.WAV\", \"DR3/MPRD0/SA1.WAV\", \"DR3/MPRD0/SA2.WAV\", \"DR3/MPRD0/SI1431.WAV\", \"DR3/MPRD0/SI2061.WAV\", \"DR3/MPRD0/SI801.WAV\", \"DR3/MPRD0/SX171.WAV\", \"DR3/MPRD0/SX261.WAV\", \"DR3/MPRD0/SX351.WAV\", \"DR3/MPRD0/SX441.WAV\", \"DR3/MPRD0/SX81.WAV\", \"DR3/MRBC0/SA1.WAV\", \"DR3/MRBC0/SA2.WAV\", \"DR3/MRBC0/SI1665.WAV\", \"DR3/MRBC0/SI1859.WAV\", \"DR3/MRBC0/SI599.WAV\", \"DR3/MRBC0/SX149.WAV\", \"DR3/MRBC0/SX239.WAV\", \"DR3/MRBC0/SX329.WAV\", \"DR3/MRBC0/SX419.WAV\", \"DR3/MRBC0/SX59.WAV\", \"DR3/MRDS0/SA1.WAV\", \"DR3/MRDS0/SA2.WAV\", \"DR3/MRDS0/SI1167.WAV\", \"DR3/MRDS0/SI1797.WAV\", \"DR3/MRDS0/SI537.WAV\", \"DR3/MRDS0/SX177.WAV\", \"DR3/MRDS0/SX267.WAV\", \"DR3/MRDS0/SX357.WAV\", \"DR3/MRDS0/SX447.WAV\", \"DR3/MRDS0/SX87.WAV\", \"DR3/MREE0/SA1.WAV\", \"DR3/MREE0/SA2.WAV\", \"DR3/MREE0/SI1104.WAV\", \"DR3/MREE0/SI1734.WAV\", \"DR3/MREE0/SI1959.WAV\", \"DR3/MREE0/SX114.WAV\", \"DR3/MREE0/SX204.WAV\", \"DR3/MREE0/SX24.WAV\", \"DR3/MREE0/SX294.WAV\", \"DR3/MREE0/SX384.WAV\", \"DR3/MREH1/SA1.WAV\", \"DR3/MREH1/SA2.WAV\", \"DR3/MREH1/SI1599.WAV\", \"DR3/MREH1/SI2229.WAV\", \"DR3/MREH1/SI969.WAV\", \"DR3/MREH1/SX159.WAV\", \"DR3/MREH1/SX249.WAV\", \"DR3/MREH1/SX339.WAV\", \"DR3/MREH1/SX429.WAV\", \"DR3/MREH1/SX69.WAV\", \"DR3/MRJB1/SA1.WAV\", \"DR3/MRJB1/SA2.WAV\", \"DR3/MRJB1/SI1020.WAV\", \"DR3/MRJB1/SI1413.WAV\", \"DR3/MRJB1/SI2021.WAV\", \"DR3/MRJB1/SX120.WAV\", \"DR3/MRJB1/SX210.WAV\", \"DR3/MRJB1/SX300.WAV\", \"DR3/MRJB1/SX30.WAV\", \"DR3/MRJB1/SX390.WAV\", \"DR3/MRTC0/SA1.WAV\", \"DR3/MRTC0/SA2.WAV\", \"DR3/MRTC0/SI1458.WAV\", \"DR3/MRTC0/SI2088.WAV\", \"DR3/MRTC0/SI828.WAV\", \"DR3/MRTC0/SX108.WAV\", \"DR3/MRTC0/SX18.WAV\", \"DR3/MRTC0/SX198.WAV\", \"DR3/MRTC0/SX288.WAV\", \"DR3/MRTC0/SX378.WAV\", \"DR3/MRTJ0/SA1.WAV\", \"DR3/MRTJ0/SA2.WAV\", \"DR3/MRTJ0/SI1551.WAV\", \"DR3/MRTJ0/SI2032.WAV\", \"DR3/MRTJ0/SI772.WAV\", \"DR3/MRTJ0/SX142.WAV\", \"DR3/MRTJ0/SX232.WAV\", \"DR3/MRTJ0/SX322.WAV\", \"DR3/MRTJ0/SX412.WAV\", \"DR3/MRTJ0/SX52.WAV\", \"DR3/MRWA0/SA1.WAV\", \"DR3/MRWA0/SA2.WAV\", \"DR3/MRWA0/SI1603.WAV\", \"DR3/MRWA0/SI2233.WAV\", \"DR3/MRWA0/SI973.WAV\", \"DR3/MRWA0/SX163.WAV\", \"DR3/MRWA0/SX253.WAV\", \"DR3/MRWA0/SX343.WAV\", \"DR3/MRWA0/SX433.WAV\", \"DR3/MRWA0/SX73.WAV\", \"DR3/MSFV0/SA1.WAV\", \"DR3/MSFV0/SA2.WAV\", \"DR3/MSFV0/SI1262.WAV\", \"DR3/MSFV0/SI1892.WAV\", \"DR3/MSFV0/SI632.WAV\", \"DR3/MSFV0/SX182.WAV\", \"DR3/MSFV0/SX272.WAV\", \"DR3/MSFV0/SX362.WAV\", \"DR3/MSFV0/SX452.WAV\", \"DR3/MSFV0/SX92.WAV\", \"DR3/MTJM0/SA1.WAV\", \"DR3/MTJM0/SA2.WAV\", \"DR3/MTJM0/SI1226.WAV\", \"DR3/MTJM0/SI1856.WAV\", \"DR3/MTJM0/SI655.WAV\", \"DR3/MTJM0/SX146.WAV\", \"DR3/MTJM0/SX236.WAV\", \"DR3/MTJM0/SX326.WAV\", \"DR3/MTJM0/SX416.WAV\", \"DR3/MTJM0/SX56.WAV\", \"DR3/MTKP0/SA1.WAV\", \"DR3/MTKP0/SA2.WAV\", \"DR3/MTKP0/SI1023.WAV\", \"DR3/MTKP0/SI2283.WAV\", \"DR3/MTKP0/SI454.WAV\", \"DR3/MTKP0/SX123.WAV\", \"DR3/MTKP0/SX213.WAV\", \"DR3/MTKP0/SX303.WAV\", \"DR3/MTKP0/SX33.WAV\", \"DR3/MTKP0/SX393.WAV\", \"DR3/MTLB0/SA1.WAV\", \"DR3/MTLB0/SA2.WAV\", \"DR3/MTLB0/SI1134.WAV\", \"DR3/MTLB0/SI1764.WAV\", \"DR3/MTLB0/SI504.WAV\", \"DR3/MTLB0/SX144.WAV\", \"DR3/MTLB0/SX234.WAV\", \"DR3/MTLB0/SX324.WAV\", \"DR3/MTLB0/SX414.WAV\", \"DR3/MTLB0/SX54.WAV\", \"DR3/MTPG0/SA1.WAV\", \"DR3/MTPG0/SA2.WAV\", \"DR3/MTPG0/SI1383.WAV\", \"DR3/MTPG0/SI2013.WAV\", \"DR3/MTPG0/SI753.WAV\", \"DR3/MTPG0/SX123.WAV\", \"DR3/MTPG0/SX213.WAV\", \"DR3/MTPG0/SX303.WAV\", \"DR3/MTPG0/SX33.WAV\", \"DR3/MTPG0/SX393.WAV\", \"DR3/MTPP0/SA1.WAV\", \"DR3/MTPP0/SA2.WAV\", \"DR3/MTPP0/SI1508.WAV\", \"DR3/MTPP0/SI2138.WAV\", \"DR3/MTPP0/SI878.WAV\", \"DR3/MTPP0/SX158.WAV\", \"DR3/MTPP0/SX248.WAV\", \"DR3/MTPP0/SX338.WAV\", \"DR3/MTPP0/SX428.WAV\", \"DR3/MTPP0/SX68.WAV\", \"DR3/MVJH0/SA1.WAV\", \"DR3/MVJH0/SA2.WAV\", \"DR3/MVJH0/SI1556.WAV\", \"DR3/MVJH0/SI2186.WAV\", \"DR3/MVJH0/SI926.WAV\", \"DR3/MVJH0/SX116.WAV\", \"DR3/MVJH0/SX206.WAV\", \"DR3/MVJH0/SX26.WAV\", \"DR3/MVJH0/SX296.WAV\", \"DR3/MVJH0/SX386.WAV\", \"DR3/MWDK0/SA1.WAV\", \"DR3/MWDK0/SA2.WAV\", \"DR3/MWDK0/SI1436.WAV\", \"DR3/MWDK0/SI2017.WAV\", \"DR3/MWDK0/SI806.WAV\", \"DR3/MWDK0/SX176.WAV\", \"DR3/MWDK0/SX266.WAV\", \"DR3/MWDK0/SX356.WAV\", \"DR3/MWDK0/SX446.WAV\", \"DR3/MWDK0/SX86.WAV\", \"DR3/MWGR0/SA1.WAV\", \"DR3/MWGR0/SA2.WAV\", \"DR3/MWGR0/SI1606.WAV\", \"DR3/MWGR0/SI2236.WAV\", \"DR3/MWGR0/SI976.WAV\", \"DR3/MWGR0/SX166.WAV\", \"DR3/MWGR0/SX256.WAV\", \"DR3/MWGR0/SX346.WAV\", \"DR3/MWGR0/SX436.WAV\", \"DR3/MWGR0/SX76.WAV\", \"DR4/MAEB0/SA1.WAV\", \"DR4/MAEB0/SA2.WAV\", \"DR4/MAEB0/SI1411.WAV\", \"DR4/MAEB0/SI2250.WAV\", \"DR4/MAEB0/SI990.WAV\", \"DR4/MAEB0/SX180.WAV\", \"DR4/MAEB0/SX270.WAV\", \"DR4/MAEB0/SX360.WAV\", \"DR4/MAEB0/SX450.WAV\", \"DR4/MAEB0/SX90.WAV\", \"DR4/MARW0/SA1.WAV\", \"DR4/MARW0/SA2.WAV\", \"DR4/MARW0/SI1276.WAV\", \"DR4/MARW0/SI1906.WAV\", \"DR4/MARW0/SI646.WAV\", \"DR4/MARW0/SX106.WAV\", \"DR4/MARW0/SX16.WAV\", \"DR4/MARW0/SX286.WAV\", \"DR4/MARW0/SX349.WAV\", \"DR4/MARW0/SX376.WAV\", \"DR4/MBMA0/SA1.WAV\", \"DR4/MBMA0/SA2.WAV\", \"DR4/MBMA0/SI1222.WAV\", \"DR4/MBMA0/SI1852.WAV\", \"DR4/MBMA0/SI592.WAV\", \"DR4/MBMA0/SX142.WAV\", \"DR4/MBMA0/SX232.WAV\", \"DR4/MBMA0/SX322.WAV\", \"DR4/MBMA0/SX412.WAV\", \"DR4/MBMA0/SX52.WAV\", \"DR4/MBWP0/SA1.WAV\", \"DR4/MBWP0/SA2.WAV\", \"DR4/MBWP0/SI1531.WAV\", \"DR4/MBWP0/SI1969.WAV\", \"DR4/MBWP0/SI709.WAV\", \"DR4/MBWP0/SX169.WAV\", \"DR4/MBWP0/SX259.WAV\", \"DR4/MBWP0/SX349.WAV\", \"DR4/MBWP0/SX439.WAV\", \"DR4/MBWP0/SX79.WAV\", \"DR4/MCDR0/SA1.WAV\", \"DR4/MCDR0/SA2.WAV\", \"DR4/MCDR0/SI1154.WAV\", \"DR4/MCDR0/SI1784.WAV\", \"DR4/MCDR0/SI524.WAV\", \"DR4/MCDR0/SX164.WAV\", \"DR4/MCDR0/SX254.WAV\", \"DR4/MCDR0/SX344.WAV\", \"DR4/MCDR0/SX434.WAV\", \"DR4/MCDR0/SX74.WAV\", \"DR4/MCSS0/SA1.WAV\", \"DR4/MCSS0/SA2.WAV\", \"DR4/MCSS0/SI1380.WAV\", \"DR4/MCSS0/SI688.WAV\", \"DR4/MCSS0/SI750.WAV\", \"DR4/MCSS0/SX120.WAV\", \"DR4/MCSS0/SX210.WAV\", \"DR4/MCSS0/SX300.WAV\", \"DR4/MCSS0/SX30.WAV\", \"DR4/MCSS0/SX390.WAV\", \"DR4/MDCD0/SA1.WAV\", \"DR4/MDCD0/SA2.WAV\", \"DR4/MDCD0/SI1415.WAV\", \"DR4/MDCD0/SI2045.WAV\", \"DR4/MDCD0/SI785.WAV\", \"DR4/MDCD0/SX155.WAV\", \"DR4/MDCD0/SX245.WAV\", \"DR4/MDCD0/SX335.WAV\", \"DR4/MDCD0/SX425.WAV\", \"DR4/MDCD0/SX65.WAV\", \"DR4/MDMA0/SA1.WAV\", \"DR4/MDMA0/SA2.WAV\", \"DR4/MDMA0/SI1238.WAV\", \"DR4/MDMA0/SI1430.WAV\", \"DR4/MDMA0/SI2060.WAV\", \"DR4/MDMA0/SX170.WAV\", \"DR4/MDMA0/SX260.WAV\", \"DR4/MDMA0/SX350.WAV\", \"DR4/MDMA0/SX440.WAV\", \"DR4/MDMA0/SX80.WAV\", \"DR4/MESG0/SA1.WAV\", \"DR4/MESG0/SA2.WAV\", \"DR4/MESG0/SI1332.WAV\", \"DR4/MESG0/SI1962.WAV\", \"DR4/MESG0/SI702.WAV\", \"DR4/MESG0/SX162.WAV\", \"DR4/MESG0/SX252.WAV\", \"DR4/MESG0/SX342.WAV\", \"DR4/MESG0/SX432.WAV\", \"DR4/MESG0/SX72.WAV\", \"DR4/MFRM0/SA1.WAV\", \"DR4/MFRM0/SA2.WAV\", \"DR4/MFRM0/SI1155.WAV\", \"DR4/MFRM0/SI1717.WAV\", \"DR4/MFRM0/SI1785.WAV\", \"DR4/MFRM0/SX165.WAV\", \"DR4/MFRM0/SX255.WAV\", \"DR4/MFRM0/SX345.WAV\", \"DR4/MFRM0/SX435.WAV\", \"DR4/MFRM0/SX75.WAV\", \"DR4/MFWK0/SA1.WAV\", \"DR4/MFWK0/SA2.WAV\", \"DR4/MFWK0/SI1249.WAV\", \"DR4/MFWK0/SI1879.WAV\", \"DR4/MFWK0/SI619.WAV\", \"DR4/MFWK0/SX169.WAV\", \"DR4/MFWK0/SX259.WAV\", \"DR4/MFWK0/SX349.WAV\", \"DR4/MFWK0/SX439.WAV\", \"DR4/MFWK0/SX79.WAV\", \"DR4/MGAG0/SA1.WAV\", \"DR4/MGAG0/SA2.WAV\", \"DR4/MGAG0/SI1321.WAV\", \"DR4/MGAG0/SI645.WAV\", \"DR4/MGAG0/SI691.WAV\", \"DR4/MGAG0/SX151.WAV\", \"DR4/MGAG0/SX241.WAV\", \"DR4/MGAG0/SX331.WAV\", \"DR4/MGAG0/SX421.WAV\", \"DR4/MGAG0/SX61.WAV\", \"DR4/MGJC0/SA1.WAV\", \"DR4/MGJC0/SA2.WAV\", \"DR4/MGJC0/SI1256.WAV\", \"DR4/MGJC0/SI1335.WAV\", \"DR4/MGJC0/SI1965.WAV\", \"DR4/MGJC0/SX165.WAV\", \"DR4/MGJC0/SX255.WAV\", \"DR4/MGJC0/SX345.WAV\", \"DR4/MGJC0/SX435.WAV\", \"DR4/MGJC0/SX75.WAV\", \"DR4/MGRP0/SA1.WAV\", \"DR4/MGRP0/SA2.WAV\", \"DR4/MGRP0/SI1317.WAV\", \"DR4/MGRP0/SI1947.WAV\", \"DR4/MGRP0/SI687.WAV\", \"DR4/MGRP0/SX147.WAV\", \"DR4/MGRP0/SX237.WAV\", \"DR4/MGRP0/SX327.WAV\", \"DR4/MGRP0/SX417.WAV\", \"DR4/MGRP0/SX57.WAV\", \"DR4/MGXP0/SA1.WAV\", \"DR4/MGXP0/SA2.WAV\", \"DR4/MGXP0/SI1087.WAV\", \"DR4/MGXP0/SI457.WAV\", \"DR4/MGXP0/SI525.WAV\", \"DR4/MGXP0/SX187.WAV\", \"DR4/MGXP0/SX277.WAV\", \"DR4/MGXP0/SX367.WAV\", \"DR4/MGXP0/SX7.WAV\", \"DR4/MGXP0/SX97.WAV\", \"DR4/MJAC0/SA1.WAV\", \"DR4/MJAC0/SA2.WAV\", \"DR4/MJAC0/SI1331.WAV\", \"DR4/MJAC0/SI2148.WAV\", \"DR4/MJAC0/SI701.WAV\", \"DR4/MJAC0/SX251.WAV\", \"DR4/MJAC0/SX307.WAV\", \"DR4/MJAC0/SX341.WAV\", \"DR4/MJAC0/SX431.WAV\", \"DR4/MJAC0/SX71.WAV\", \"DR4/MJDC0/SA1.WAV\", \"DR4/MJDC0/SA2.WAV\", \"DR4/MJDC0/SI1161.WAV\", \"DR4/MJDC0/SI2165.WAV\", \"DR4/MJDC0/SI531.WAV\", \"DR4/MJDC0/SX171.WAV\", \"DR4/MJDC0/SX261.WAV\", \"DR4/MJDC0/SX351.WAV\", \"DR4/MJDC0/SX441.WAV\", \"DR4/MJDC0/SX81.WAV\", \"DR4/MJEE0/SA1.WAV\", \"DR4/MJEE0/SA2.WAV\", \"DR4/MJEE0/SI1237.WAV\", \"DR4/MJEE0/SI1867.WAV\", \"DR4/MJEE0/SI607.WAV\", \"DR4/MJEE0/SX157.WAV\", \"DR4/MJEE0/SX247.WAV\", \"DR4/MJEE0/SX337.WAV\", \"DR4/MJEE0/SX427.WAV\", \"DR4/MJEE0/SX67.WAV\", \"DR4/MJJJ0/SA1.WAV\", \"DR4/MJJJ0/SA2.WAV\", \"DR4/MJJJ0/SI1163.WAV\", \"DR4/MJJJ0/SI1793.WAV\", \"DR4/MJJJ0/SI533.WAV\", \"DR4/MJJJ0/SX173.WAV\", \"DR4/MJJJ0/SX263.WAV\", \"DR4/MJJJ0/SX353.WAV\", \"DR4/MJJJ0/SX443.WAV\", \"DR4/MJJJ0/SX83.WAV\", \"DR4/MJLB0/SA1.WAV\", \"DR4/MJLB0/SA2.WAV\", \"DR4/MJLB0/SI1616.WAV\", \"DR4/MJLB0/SI2246.WAV\", \"DR4/MJLB0/SI986.WAV\", \"DR4/MJLB0/SX176.WAV\", \"DR4/MJLB0/SX266.WAV\", \"DR4/MJLB0/SX356.WAV\", \"DR4/MJLB0/SX446.WAV\", \"DR4/MJLB0/SX86.WAV\", \"DR4/MJLS0/SA1.WAV\", \"DR4/MJLS0/SA2.WAV\", \"DR4/MJLS0/SI1096.WAV\", \"DR4/MJLS0/SI1726.WAV\", \"DR4/MJLS0/SI466.WAV\", \"DR4/MJLS0/SX106.WAV\", \"DR4/MJLS0/SX16.WAV\", \"DR4/MJLS0/SX196.WAV\", \"DR4/MJLS0/SX286.WAV\", \"DR4/MJLS0/SX376.WAV\", \"DR4/MJMM0/SA1.WAV\", \"DR4/MJMM0/SA2.WAV\", \"DR4/MJMM0/SI1255.WAV\", \"DR4/MJMM0/SI1885.WAV\", \"DR4/MJMM0/SI625.WAV\", \"DR4/MJMM0/SX175.WAV\", \"DR4/MJMM0/SX265.WAV\", \"DR4/MJMM0/SX355.WAV\", \"DR4/MJMM0/SX445.WAV\", \"DR4/MJMM0/SX85.WAV\", \"DR4/MJPM1/SA1.WAV\", \"DR4/MJPM1/SA2.WAV\", \"DR4/MJPM1/SI1897.WAV\", \"DR4/MJPM1/SI2280.WAV\", \"DR4/MJPM1/SI761.WAV\", \"DR4/MJPM1/SX131.WAV\", \"DR4/MJPM1/SX221.WAV\", \"DR4/MJPM1/SX311.WAV\", \"DR4/MJPM1/SX401.WAV\", \"DR4/MJPM1/SX41.WAV\", \"DR4/MJRH0/SA1.WAV\", \"DR4/MJRH0/SA2.WAV\", \"DR4/MJRH0/SI1125.WAV\", \"DR4/MJRH0/SI1755.WAV\", \"DR4/MJRH0/SI1840.WAV\", \"DR4/MJRH0/SX135.WAV\", \"DR4/MJRH0/SX225.WAV\", \"DR4/MJRH0/SX315.WAV\", \"DR4/MJRH0/SX405.WAV\", \"DR4/MJRH0/SX45.WAV\", \"DR4/MJSR0/SA1.WAV\", \"DR4/MJSR0/SA2.WAV\", \"DR4/MJSR0/SI1424.WAV\", \"DR4/MJSR0/SI2054.WAV\", \"DR4/MJSR0/SI794.WAV\", \"DR4/MJSR0/SX164.WAV\", \"DR4/MJSR0/SX254.WAV\", \"DR4/MJSR0/SX344.WAV\", \"DR4/MJSR0/SX434.WAV\", \"DR4/MJSR0/SX74.WAV\", \"DR4/MJWS0/SA1.WAV\", \"DR4/MJWS0/SA2.WAV\", \"DR4/MJWS0/SI1143.WAV\", \"DR4/MJWS0/SI1773.WAV\", \"DR4/MJWS0/SI513.WAV\", \"DR4/MJWS0/SX153.WAV\", \"DR4/MJWS0/SX243.WAV\", \"DR4/MJWS0/SX333.WAV\", \"DR4/MJWS0/SX423.WAV\", \"DR4/MJWS0/SX63.WAV\", \"DR4/MJXL0/SA1.WAV\", \"DR4/MJXL0/SA2.WAV\", \"DR4/MJXL0/SI1172.WAV\", \"DR4/MJXL0/SI1795.WAV\", \"DR4/MJXL0/SI542.WAV\", \"DR4/MJXL0/SX182.WAV\", \"DR4/MJXL0/SX272.WAV\", \"DR4/MJXL0/SX362.WAV\", \"DR4/MJXL0/SX452.WAV\", \"DR4/MJXL0/SX92.WAV\", \"DR4/MKAM0/SA1.WAV\", \"DR4/MKAM0/SA2.WAV\", \"DR4/MKAM0/SI1250.WAV\", \"DR4/MKAM0/SI1316.WAV\", \"DR4/MKAM0/SI1465.WAV\", \"DR4/MKAM0/SX146.WAV\", \"DR4/MKAM0/SX236.WAV\", \"DR4/MKAM0/SX326.WAV\", \"DR4/MKAM0/SX416.WAV\", \"DR4/MKAM0/SX56.WAV\", \"DR4/MLBC0/SA1.WAV\", \"DR4/MLBC0/SA2.WAV\", \"DR4/MLBC0/SI1239.WAV\", \"DR4/MLBC0/SI1869.WAV\", \"DR4/MLBC0/SI609.WAV\", \"DR4/MLBC0/SX159.WAV\", \"DR4/MLBC0/SX249.WAV\", \"DR4/MLBC0/SX339.WAV\", \"DR4/MLBC0/SX429.WAV\", \"DR4/MLBC0/SX69.WAV\", \"DR4/MLEL0/SA1.WAV\", \"DR4/MLEL0/SA2.WAV\", \"DR4/MLEL0/SI1246.WAV\", \"DR4/MLEL0/SI1876.WAV\", \"DR4/MLEL0/SI616.WAV\", \"DR4/MLEL0/SX166.WAV\", \"DR4/MLEL0/SX256.WAV\", \"DR4/MLEL0/SX346.WAV\", \"DR4/MLEL0/SX436.WAV\", \"DR4/MLEL0/SX76.WAV\", \"DR4/MLJC0/SA1.WAV\", \"DR4/MLJC0/SA2.WAV\", \"DR4/MLJC0/SI1225.WAV\", \"DR4/MLJC0/SI1855.WAV\", \"DR4/MLJC0/SI595.WAV\", \"DR4/MLJC0/SX145.WAV\", \"DR4/MLJC0/SX235.WAV\", \"DR4/MLJC0/SX325.WAV\", \"DR4/MLJC0/SX415.WAV\", \"DR4/MLJC0/SX55.WAV\", \"DR4/MLJH0/SA1.WAV\", \"DR4/MLJH0/SA2.WAV\", \"DR4/MLJH0/SI1324.WAV\", \"DR4/MLJH0/SI1422.WAV\", \"DR4/MLJH0/SI694.WAV\", \"DR4/MLJH0/SX154.WAV\", \"DR4/MLJH0/SX244.WAV\", \"DR4/MLJH0/SX334.WAV\", \"DR4/MLJH0/SX424.WAV\", \"DR4/MLJH0/SX64.WAV\", \"DR4/MLSH0/SA1.WAV\", \"DR4/MLSH0/SA2.WAV\", \"DR4/MLSH0/SI1417.WAV\", \"DR4/MLSH0/SI2047.WAV\", \"DR4/MLSH0/SI787.WAV\", \"DR4/MLSH0/SX157.WAV\", \"DR4/MLSH0/SX247.WAV\", \"DR4/MLSH0/SX337.WAV\", \"DR4/MLSH0/SX427.WAV\", \"DR4/MLSH0/SX67.WAV\", \"DR4/MMBS0/SA1.WAV\", \"DR4/MMBS0/SA2.WAV\", \"DR4/MMBS0/SI1151.WAV\", \"DR4/MMBS0/SI1781.WAV\", \"DR4/MMBS0/SI521.WAV\", \"DR4/MMBS0/SX161.WAV\", \"DR4/MMBS0/SX251.WAV\", \"DR4/MMBS0/SX341.WAV\", \"DR4/MMBS0/SX431.WAV\", \"DR4/MMBS0/SX71.WAV\", \"DR4/MMDM0/SA1.WAV\", \"DR4/MMDM0/SA2.WAV\", \"DR4/MMDM0/SI1311.WAV\", \"DR4/MMDM0/SI1941.WAV\", \"DR4/MMDM0/SI681.WAV\", \"DR4/MMDM0/SX141.WAV\", \"DR4/MMDM0/SX231.WAV\", \"DR4/MMDM0/SX321.WAV\", \"DR4/MMDM0/SX411.WAV\", \"DR4/MMDM0/SX51.WAV\", \"DR4/MMGC0/SA1.WAV\", \"DR4/MMGC0/SA2.WAV\", \"DR4/MMGC0/SI1305.WAV\", \"DR4/MMGC0/SI1935.WAV\", \"DR4/MMGC0/SI2184.WAV\", \"DR4/MMGC0/SX135.WAV\", \"DR4/MMGC0/SX225.WAV\", \"DR4/MMGC0/SX315.WAV\", \"DR4/MMGC0/SX405.WAV\", \"DR4/MMGC0/SX45.WAV\", \"DR4/MNET0/SA1.WAV\", \"DR4/MNET0/SA2.WAV\", \"DR4/MNET0/SI1446.WAV\", \"DR4/MNET0/SI2076.WAV\", \"DR4/MNET0/SI816.WAV\", \"DR4/MNET0/SX186.WAV\", \"DR4/MNET0/SX276.WAV\", \"DR4/MNET0/SX366.WAV\", \"DR4/MNET0/SX6.WAV\", \"DR4/MNET0/SX96.WAV\", \"DR4/MPEB0/SA1.WAV\", \"DR4/MPEB0/SA2.WAV\", \"DR4/MPEB0/SI1034.WAV\", \"DR4/MPEB0/SI1860.WAV\", \"DR4/MPEB0/SI600.WAV\", \"DR4/MPEB0/SX150.WAV\", \"DR4/MPEB0/SX240.WAV\", \"DR4/MPEB0/SX330.WAV\", \"DR4/MPEB0/SX420.WAV\", \"DR4/MPEB0/SX60.WAV\", \"DR4/MPRK0/SA1.WAV\", \"DR4/MPRK0/SA2.WAV\", \"DR4/MPRK0/SI1097.WAV\", \"DR4/MPRK0/SI1727.WAV\", \"DR4/MPRK0/SI467.WAV\", \"DR4/MPRK0/SX107.WAV\", \"DR4/MPRK0/SX17.WAV\", \"DR4/MPRK0/SX197.WAV\", \"DR4/MPRK0/SX287.WAV\", \"DR4/MPRK0/SX377.WAV\", \"DR4/MPRT0/SA1.WAV\", \"DR4/MPRT0/SA2.WAV\", \"DR4/MPRT0/SI1210.WAV\", \"DR4/MPRT0/SI495.WAV\", \"DR4/MPRT0/SI580.WAV\", \"DR4/MPRT0/SX130.WAV\", \"DR4/MPRT0/SX220.WAV\", \"DR4/MPRT0/SX310.WAV\", \"DR4/MPRT0/SX400.WAV\", \"DR4/MPRT0/SX40.WAV\", \"DR4/MRAB1/SA1.WAV\", \"DR4/MRAB1/SA2.WAV\", \"DR4/MRAB1/SI1478.WAV\", \"DR4/MRAB1/SI2108.WAV\", \"DR4/MRAB1/SI848.WAV\", \"DR4/MRAB1/SX128.WAV\", \"DR4/MRAB1/SX218.WAV\", \"DR4/MRAB1/SX308.WAV\", \"DR4/MRAB1/SX38.WAV\", \"DR4/MRAB1/SX398.WAV\", \"DR4/MRFL0/SA1.WAV\", \"DR4/MRFL0/SA2.WAV\", \"DR4/MRFL0/SI1156.WAV\", \"DR4/MRFL0/SI1786.WAV\", \"DR4/MRFL0/SI526.WAV\", \"DR4/MRFL0/SX166.WAV\", \"DR4/MRFL0/SX256.WAV\", \"DR4/MRFL0/SX346.WAV\", \"DR4/MRFL0/SX436.WAV\", \"DR4/MRFL0/SX76.WAV\", \"DR4/MRGM0/SA1.WAV\", \"DR4/MRGM0/SA2.WAV\", \"DR4/MRGM0/SI1162.WAV\", \"DR4/MRGM0/SI1792.WAV\", \"DR4/MRGM0/SI532.WAV\", \"DR4/MRGM0/SX172.WAV\", \"DR4/MRGM0/SX262.WAV\", \"DR4/MRGM0/SX416.WAV\", \"DR4/MRGM0/SX442.WAV\", \"DR4/MRGM0/SX82.WAV\", \"DR4/MRSP0/SA1.WAV\", \"DR4/MRSP0/SA2.WAV\", \"DR4/MRSP0/SI1429.WAV\", \"DR4/MRSP0/SI2059.WAV\", \"DR4/MRSP0/SI799.WAV\", \"DR4/MRSP0/SX169.WAV\", \"DR4/MRSP0/SX196.WAV\", \"DR4/MRSP0/SX259.WAV\", \"DR4/MRSP0/SX439.WAV\", \"DR4/MRSP0/SX79.WAV\", \"DR4/MSFH0/SA1.WAV\", \"DR4/MSFH0/SA2.WAV\", \"DR4/MSFH0/SI1216.WAV\", \"DR4/MSFH0/SI1738.WAV\", \"DR4/MSFH0/SI586.WAV\", \"DR4/MSFH0/SX136.WAV\", \"DR4/MSFH0/SX226.WAV\", \"DR4/MSFH0/SX316.WAV\", \"DR4/MSFH0/SX406.WAV\", \"DR4/MSFH0/SX46.WAV\", \"DR4/MSMC0/SA1.WAV\", \"DR4/MSMC0/SA2.WAV\", \"DR4/MSMC0/SI1907.WAV\", \"DR4/MSMC0/SI509.WAV\", \"DR4/MSMC0/SI647.WAV\", \"DR4/MSMC0/SX107.WAV\", \"DR4/MSMC0/SX17.WAV\", \"DR4/MSMC0/SX197.WAV\", \"DR4/MSMC0/SX287.WAV\", \"DR4/MSMC0/SX377.WAV\", \"DR4/MSMS0/SA1.WAV\", \"DR4/MSMS0/SA2.WAV\", \"DR4/MSMS0/SI1433.WAV\", \"DR4/MSMS0/SI2063.WAV\", \"DR4/MSMS0/SI803.WAV\", \"DR4/MSMS0/SX173.WAV\", \"DR4/MSMS0/SX263.WAV\", \"DR4/MSMS0/SX353.WAV\", \"DR4/MSMS0/SX443.WAV\", \"DR4/MSMS0/SX83.WAV\", \"DR4/MSRG0/SA1.WAV\", \"DR4/MSRG0/SA2.WAV\", \"DR4/MSRG0/SI1221.WAV\", \"DR4/MSRG0/SI1851.WAV\", \"DR4/MSRG0/SI591.WAV\", \"DR4/MSRG0/SX141.WAV\", \"DR4/MSRG0/SX231.WAV\", \"DR4/MSRG0/SX321.WAV\", \"DR4/MSRG0/SX411.WAV\", \"DR4/MSRG0/SX51.WAV\", \"DR4/MSTF0/SA1.WAV\", \"DR4/MSTF0/SA2.WAV\", \"DR4/MSTF0/SI1396.WAV\", \"DR4/MSTF0/SI766.WAV\", \"DR4/MSTF0/SI852.WAV\", \"DR4/MSTF0/SX136.WAV\", \"DR4/MSTF0/SX226.WAV\", \"DR4/MSTF0/SX316.WAV\", \"DR4/MSTF0/SX406.WAV\", \"DR4/MSTF0/SX46.WAV\", \"DR4/MTAS0/SA1.WAV\", \"DR4/MTAS0/SA2.WAV\", \"DR4/MTAS0/SI1385.WAV\", \"DR4/MTAS0/SI2015.WAV\", \"DR4/MTAS0/SI755.WAV\", \"DR4/MTAS0/SX125.WAV\", \"DR4/MTAS0/SX215.WAV\", \"DR4/MTAS0/SX305.WAV\", \"DR4/MTAS0/SX35.WAV\", \"DR4/MTAS0/SX395.WAV\", \"DR4/MTQC0/SA1.WAV\", \"DR4/MTQC0/SA2.WAV\", \"DR4/MTQC0/SI1441.WAV\", \"DR4/MTQC0/SI2071.WAV\", \"DR4/MTQC0/SI480.WAV\", \"DR4/MTQC0/SX181.WAV\", \"DR4/MTQC0/SX271.WAV\", \"DR4/MTQC0/SX361.WAV\", \"DR4/MTQC0/SX451.WAV\", \"DR4/MTQC0/SX91.WAV\", \"DR4/MTRC0/SA1.WAV\", \"DR4/MTRC0/SA2.WAV\", \"DR4/MTRC0/SI1623.WAV\", \"DR4/MTRC0/SI589.WAV\", \"DR4/MTRC0/SI993.WAV\", \"DR4/MTRC0/SX170.WAV\", \"DR4/MTRC0/SX183.WAV\", \"DR4/MTRC0/SX273.WAV\", \"DR4/MTRC0/SX363.WAV\", \"DR4/MTRC0/SX93.WAV\", \"DR4/MTRT0/SA1.WAV\", \"DR4/MTRT0/SA2.WAV\", \"DR4/MTRT0/SI1227.WAV\", \"DR4/MTRT0/SI1857.WAV\", \"DR4/MTRT0/SI597.WAV\", \"DR4/MTRT0/SX147.WAV\", \"DR4/MTRT0/SX237.WAV\", \"DR4/MTRT0/SX254.WAV\", \"DR4/MTRT0/SX417.WAV\", \"DR4/MTRT0/SX57.WAV\", \"DR5/MBGT0/SA1.WAV\", \"DR5/MBGT0/SA2.WAV\", \"DR5/MBGT0/SI1341.WAV\", \"DR5/MBGT0/SI1841.WAV\", \"DR5/MBGT0/SI711.WAV\", \"DR5/MBGT0/SX171.WAV\", \"DR5/MBGT0/SX261.WAV\", \"DR5/MBGT0/SX351.WAV\", \"DR5/MBGT0/SX441.WAV\", \"DR5/MBGT0/SX81.WAV\", \"DR5/MCHL0/SA1.WAV\", \"DR5/MCHL0/SA2.WAV\", \"DR5/MCHL0/SI1347.WAV\", \"DR5/MCHL0/SI1404.WAV\", \"DR5/MCHL0/SI1977.WAV\", \"DR5/MCHL0/SX177.WAV\", \"DR5/MCHL0/SX267.WAV\", \"DR5/MCHL0/SX357.WAV\", \"DR5/MCHL0/SX447.WAV\", \"DR5/MCHL0/SX87.WAV\", \"DR5/MCLM0/SA1.WAV\", \"DR5/MCLM0/SA2.WAV\", \"DR5/MCLM0/SI1456.WAV\", \"DR5/MCLM0/SI2086.WAV\", \"DR5/MCLM0/SI826.WAV\", \"DR5/MCLM0/SX106.WAV\", \"DR5/MCLM0/SX16.WAV\", \"DR5/MCLM0/SX196.WAV\", \"DR5/MCLM0/SX286.WAV\", \"DR5/MCLM0/SX376.WAV\", \"DR5/MDAS0/SA1.WAV\", \"DR5/MDAS0/SA2.WAV\", \"DR5/MDAS0/SI1266.WAV\", \"DR5/MDAS0/SI1896.WAV\", \"DR5/MDAS0/SI636.WAV\", \"DR5/MDAS0/SX186.WAV\", \"DR5/MDAS0/SX21.WAV\", \"DR5/MDAS0/SX276.WAV\", \"DR5/MDAS0/SX6.WAV\", \"DR5/MDAS0/SX96.WAV\", \"DR5/MDHL0/SA1.WAV\", \"DR5/MDHL0/SA2.WAV\", \"DR5/MDHL0/SI1439.WAV\", \"DR5/MDHL0/SI2069.WAV\", \"DR5/MDHL0/SI809.WAV\", \"DR5/MDHL0/SX179.WAV\", \"DR5/MDHL0/SX269.WAV\", \"DR5/MDHL0/SX359.WAV\", \"DR5/MDHL0/SX449.WAV\", \"DR5/MDHL0/SX89.WAV\", \"DR5/MDSJ0/SA1.WAV\", \"DR5/MDSJ0/SA2.WAV\", \"DR5/MDSJ0/SI1462.WAV\", \"DR5/MDSJ0/SI2092.WAV\", \"DR5/MDSJ0/SI832.WAV\", \"DR5/MDSJ0/SX112.WAV\", \"DR5/MDSJ0/SX22.WAV\", \"DR5/MDSJ0/SX292.WAV\", \"DR5/MDSJ0/SX382.WAV\", \"DR5/MDSJ0/SX438.WAV\", \"DR5/MDWH0/SA1.WAV\", \"DR5/MDWH0/SA2.WAV\", \"DR5/MDWH0/SI1168.WAV\", \"DR5/MDWH0/SI1925.WAV\", \"DR5/MDWH0/SI665.WAV\", \"DR5/MDWH0/SX125.WAV\", \"DR5/MDWH0/SX215.WAV\", \"DR5/MDWH0/SX305.WAV\", \"DR5/MDWH0/SX35.WAV\", \"DR5/MDWH0/SX395.WAV\", \"DR5/MEGJ0/SA1.WAV\", \"DR5/MEGJ0/SA2.WAV\", \"DR5/MEGJ0/SI1337.WAV\", \"DR5/MEGJ0/SI1967.WAV\", \"DR5/MEGJ0/SI707.WAV\", \"DR5/MEGJ0/SX167.WAV\", \"DR5/MEGJ0/SX257.WAV\", \"DR5/MEGJ0/SX3.WAV\", \"DR5/MEGJ0/SX437.WAV\", \"DR5/MEGJ0/SX77.WAV\", \"DR5/MEWM0/SA1.WAV\", \"DR5/MEWM0/SA2.WAV\", \"DR5/MEWM0/SI1348.WAV\", \"DR5/MEWM0/SI1978.WAV\", \"DR5/MEWM0/SI718.WAV\", \"DR5/MEWM0/SX178.WAV\", \"DR5/MEWM0/SX268.WAV\", \"DR5/MEWM0/SX358.WAV\", \"DR5/MEWM0/SX448.WAV\", \"DR5/MEWM0/SX88.WAV\", \"DR5/MFER0/SA1.WAV\", \"DR5/MFER0/SA2.WAV\", \"DR5/MFER0/SI1492.WAV\", \"DR5/MFER0/SI2122.WAV\", \"DR5/MFER0/SI862.WAV\", \"DR5/MFER0/SX142.WAV\", \"DR5/MFER0/SX232.WAV\", \"DR5/MFER0/SX322.WAV\", \"DR5/MFER0/SX412.WAV\", \"DR5/MFER0/SX52.WAV\", \"DR5/MGES0/SA1.WAV\", \"DR5/MGES0/SA2.WAV\", \"DR5/MGES0/SI1481.WAV\", \"DR5/MGES0/SI2111.WAV\", \"DR5/MGES0/SI851.WAV\", \"DR5/MGES0/SX131.WAV\", \"DR5/MGES0/SX221.WAV\", \"DR5/MGES0/SX311.WAV\", \"DR5/MGES0/SX401.WAV\", \"DR5/MGES0/SX41.WAV\", \"DR5/MGSH0/SA1.WAV\", \"DR5/MGSH0/SA2.WAV\", \"DR5/MGSH0/SI1176.WAV\", \"DR5/MGSH0/SI1806.WAV\", \"DR5/MGSH0/SI546.WAV\", \"DR5/MGSH0/SX127.WAV\", \"DR5/MGSH0/SX186.WAV\", \"DR5/MGSH0/SX276.WAV\", \"DR5/MGSH0/SX6.WAV\", \"DR5/MGSH0/SX96.WAV\", \"DR5/MHIT0/SA1.WAV\", \"DR5/MHIT0/SA2.WAV\", \"DR5/MHIT0/SI1613.WAV\", \"DR5/MHIT0/SI2243.WAV\", \"DR5/MHIT0/SI983.WAV\", \"DR5/MHIT0/SX173.WAV\", \"DR5/MHIT0/SX263.WAV\", \"DR5/MHIT0/SX353.WAV\", \"DR5/MHIT0/SX443.WAV\", \"DR5/MHIT0/SX83.WAV\", \"DR5/MHMG0/SA1.WAV\", \"DR5/MHMG0/SA2.WAV\", \"DR5/MHMG0/SI1365.WAV\", \"DR5/MHMG0/SI1995.WAV\", \"DR5/MHMG0/SI735.WAV\", \"DR5/MHMG0/SX105.WAV\", \"DR5/MHMG0/SX15.WAV\", \"DR5/MHMG0/SX195.WAV\", \"DR5/MHMG0/SX285.WAV\", \"DR5/MHMG0/SX375.WAV\"]\n",
    "\n",
    "print(len(flist), flist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(fname):\n",
    "    attr = fname.split('.')[0].split('/')\n",
    "    dialect = attr[0]\n",
    "    gender = attr[1][0]\n",
    "    speaker_id = attr[1]\n",
    "    sentence_type = attr[2][:2]\n",
    "    return dialect, gender, speaker_id, sentence_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vad\n",
    "import os\n",
    "import shutil\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "\n",
    "def short_term_energy(signal, window_length: int, step: int):\n",
    "\tnum_of_frames = int((signal.size - window_length) / step) + 1\n",
    "\te = np.zeros(num_of_frames)\n",
    "\tfor window_index in range(0, num_of_frames):\n",
    "\t\tcurrent_index = window_index * step\n",
    "\t\twindow = signal[current_index:current_index + window_length]\n",
    "\t\te[window_index] = (1 / window_length) * np.sum(np.square(window))\n",
    "\treturn e\n",
    "\n",
    "\n",
    "def spectral_centroid(signal, window_length: int, step: int, fs: int):\n",
    "\tnum_of_frames = int((len(signal) - window_length) / step) + 1\n",
    "\th = np.hamming(window_length)\n",
    "\tm = np.transpose(((fs / (2 * window_length)) * np.array([i for i in range(1, window_length + 1)])))\n",
    "\tc = np.zeros(num_of_frames)\n",
    "\tfor window_index in range(0, num_of_frames):\n",
    "\t\tcurrent_index = window_index * step\n",
    "\t\twindow = np.multiply(h, (signal[current_index: current_index + window_length]))\n",
    "\t\tfft = np.abs(np.fft.rfft(window, 2 * window_length))\n",
    "\t\tfft = fft[:window_length]\n",
    "\t\tfft = fft / np.max(fft)\n",
    "\t\tc[window_index] = np.sum(np.multiply(m, fft)) / np.sum(fft)\n",
    "\t\tif np.sum(np.square(window)) < 0.010:\n",
    "\t\t\tc[window_index] = 0.0\n",
    "\tc = c / (fs / 2)\n",
    "\treturn c\n",
    "\n",
    "\n",
    "def find_maxima(f, step):\n",
    "\tcount_maxima = 0\n",
    "\tmaxima = np.zeros([2, 0])\n",
    "\tfor i in range(0, len(f) - step):\n",
    "\t\tif i > step:\n",
    "\t\t\tif (np.mean(f[i - step:i]) < f[i]) and (np.mean(f[i + 1: i + step + 1]) < f[i]):\n",
    "\t\t\t\tmaxima = np.hstack((maxima, np.array([[i], [f[i]]])))\n",
    "\t\t\t\tcount_maxima = count_maxima + 1\n",
    "\t\telse:\n",
    "\t\t\tif (np.mean(f[0:i + 1]) <= f[i]) and (np.mean(f[i + 1: i + step + 1]) < f[i]):\n",
    "\t\t\t\tmaxima = np.hstack((maxima, np.array([[i], [f[i]]])))\n",
    "\t\t\t\tcount_maxima = count_maxima + 1\n",
    "\n",
    "\tmaxima_new = np.zeros((2, count_maxima))\n",
    "\tcount_new_maxima = 0\n",
    "\ti = 0\n",
    "\n",
    "\twhile i < count_maxima:\n",
    "\t\ttemp_max = np.array([maxima[0][i]])\n",
    "\t\ttemp_val = np.array([maxima[1][i]])\n",
    "\n",
    "\t\twhile (i < count_maxima - 1) and (maxima[0][i + 1] - temp_max[-1] < step / 2):\n",
    "\t\t\ti = i + 1\n",
    "\t\t\tnp.append(temp_max, maxima[0][i])\n",
    "\t\t\tnp.append(temp_val, maxima[1][i])\n",
    "\n",
    "\t\tmi = temp_val.argmax()\n",
    "\t\tmm = temp_val[mi]\n",
    "\n",
    "\t\tif mm > 0.02 * f.mean():\n",
    "\t\t\tmaxima_new[0][count_new_maxima] = temp_max[mi]\n",
    "\t\t\tmaxima_new[1][count_new_maxima] = f[int(maxima_new[0][count_new_maxima])]\n",
    "\t\t\tcount_new_maxima = count_new_maxima + 1\n",
    "\n",
    "\t\ti = i + 1\n",
    "\n",
    "\tmaxima = maxima_new\n",
    "\tcount_maxima = count_new_maxima\n",
    "\n",
    "\treturn maxima, count_maxima\n",
    "\n",
    "\n",
    "def calc_histogram_boundary_centers(hist_bounds):\n",
    "\tbounds = len(hist_bounds) - 1\n",
    "\tcenters = np.zeros(bounds)\n",
    "\tfor i in range(0, bounds):\n",
    "\t\tcenters[i] = (hist_bounds[i] + hist_bounds[i + 1]) / 2\n",
    "\treturn centers\n",
    "\n",
    "\n",
    "def vad(processed_wav, bit_rate):\n",
    "\t# 50ms window size\n",
    "\twindow_size = 0.050\n",
    "\n",
    "\t# 25ms stride\n",
    "\tstep_size = 0.025\n",
    "\n",
    "\t# Compute short-term energy and spectral centroid of the signal\n",
    "\teor = short_term_energy(processed_wav, int(window_size * bit_rate), int(step_size * bit_rate))\n",
    "\tcor = spectral_centroid(processed_wav, int(window_size * bit_rate), int(step_size * bit_rate), bit_rate)\n",
    "\n",
    "\t# Apply median filtering in the feature sequence twice\n",
    "\tsmoothing_step_size = 7\n",
    "\tE = medfilt(medfilt(eor, [smoothing_step_size]), [smoothing_step_size])\n",
    "\tC = medfilt(medfilt(cor, [smoothing_step_size]), [smoothing_step_size])\n",
    "\n",
    "\t# Get the average values of the smoothed feature sequences\n",
    "\tE_mean = E.mean()\n",
    "\tC_mean = C.mean()\n",
    "\n",
    "\tweight = 5\n",
    "\n",
    "\t# Find energy threshold\n",
    "\thist_e, bounds_e = np.histogram(E, int(np.round(len(E) / 10)))\n",
    "\tx_e = calc_histogram_boundary_centers(bounds_e)\n",
    "\tmaxima_e, count_maxima_e = find_maxima(hist_e, 3)\n",
    "\tif np.size(maxima_e, 1) >= 2:\n",
    "\t\tt_e = (weight * x_e[int(maxima_e[0][0])] + x_e[int(maxima_e[0][1])]) / (weight + 1)\n",
    "\telse:\n",
    "\t\tt_e = E_mean / 2\n",
    "\n",
    "\t# Find spectral centroid threshold\n",
    "\thist_c, bounds_c = np.histogram(C, int(np.round(len(C) / 10)))\n",
    "\tx_c = calc_histogram_boundary_centers(bounds_c)\n",
    "\tmaxima_c, count_maxima_c = find_maxima(hist_c, 3)\n",
    "\tif np.size(maxima_c, 1) >= 2:\n",
    "\t\tt_c = (weight * x_c[int(maxima_c[0][0])] + x_c[int(maxima_c[0][1])]) / (weight + 1)\n",
    "\telse:\n",
    "\t\tt_c = C_mean / 2\n",
    "\n",
    "\t# Thresholding\n",
    "\tflags1 = E >= t_e\n",
    "\tflags2 = C >= t_c\n",
    "\tflags = flags1 & flags2\n",
    "\n",
    "\t# Speech segments detection\n",
    "\tcount = 0\n",
    "\twin = 5\n",
    "\tlimits = np.zeros((0, 2)).astype(int)\n",
    "\twhile count < len(flags):\n",
    "\t\tcountTemp = 1\n",
    "\t\tlimit1 = 0\n",
    "\t\tlimit2 = 0\n",
    "\t\twhile flags[count] == 1 and count < len(flags):\n",
    "\t\t\tif countTemp == 1:\n",
    "\t\t\t\tlimit1 = np.round((count - win + 1) * step_size * bit_rate) + 1\n",
    "\t\t\t\tif limit1 < 1:\n",
    "\t\t\t\t\tlimit1 = 1\n",
    "\t\t\tcount = count + 1\n",
    "\t\t\tcountTemp = countTemp + 1\n",
    "\n",
    "\t\tif countTemp > 1:\n",
    "\t\t\tlimit2 = np.round((count + win + 1) * step_size * bit_rate)\n",
    "\t\t\tif limit2 > len(processed_wav):\n",
    "\t\t\t\tlimit2 = len(processed_wav)\n",
    "\t\t\tlimits = np.append(limits, np.array([[limit1, limit2]]).astype(int), axis=0)\n",
    "\n",
    "\t\tcount = count + 1\n",
    "\n",
    "\t# post process\n",
    "\t# Merge overlapping segments\n",
    "\trun = 1\n",
    "\twhile run == 1:\n",
    "\t\trun = 0\n",
    "\t\tfor i in range(0, limits.shape[0] - 1):\n",
    "\t\t\tif limits[i][1] >= limits[i + 1][0]:\n",
    "\t\t\t\trun = 1\n",
    "\t\t\t\tlimits[i][1] = limits[i + 1][1]\n",
    "\t\t\t\tlimits = np.delete(limits, (i + 1), axis=0)\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t# Get final segments\n",
    "\tsegments = []\n",
    "\tfor i in range(0, limits.shape[0]):\n",
    "\t\tsegments.append(processed_wav[limits[i][0]:limits[i][1] + 1])\n",
    "\n",
    "\treturn segments, bit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "\n",
    "for fname in flist:\n",
    "    input_path = indir + fname\n",
    "    y, sr = librosa.load(input_path, sr=None) # set sr=None for orig file sr otherwise it is converted to ~22K\n",
    "\n",
    "    # scaling the maximum of absolute amplitude to 1\n",
    "    processed_wav = y/max(abs(y))\n",
    "    \n",
    "    # calc VAD\n",
    "    segments, sr = vad(processed_wav, sr)\n",
    "    # merge all segments\n",
    "    if len(segments) > 0:\n",
    "        processed_data = np.hstack(segments)\n",
    "        \n",
    "        # https://groups.google.com/forum/#!topic/librosa/V4Z1HpTKn8Q\n",
    "        mfcc = librosa.feature.mfcc(y=processed_data, sr=sr, n_mfcc=13, n_fft=(25*sr)//1000, hop_length=(10*sr)//1000)\n",
    "        mfcc[0] = librosa.feature.rmse(processed_data, hop_length=int(0.010*sr), n_fft=int(0.025*sr)) \n",
    "        mfcc_delta = librosa.feature.delta(mfcc)\n",
    "        mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "        features = np.vstack([mfcc, mfcc_delta, mfcc_delta2]) \n",
    "\n",
    "        # split train test\n",
    "        dialect, gender, speaker_id, sentence_type = get_attributes(fname)\n",
    "        if sentence_type == 'SA':\n",
    "            test.setdefault(speaker_id, []).append(features)\n",
    "        else:\n",
    "            train.setdefault(speaker_id, []).append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MADC0', 'MAEB0', 'MAKB0', 'MAKR0', 'MAPV0', 'MARC0', 'MARW0', 'MBEF0', 'MBGT0', 'MBJV0', 'MBMA0', 'MBWP0', 'MCAL0', 'MCDC0', 'MCDD0', 'MCDR0', 'MCEF0', 'MCEW0', 'MCHL0', 'MCLM0', 'MCPM0', 'MCSS0', 'MCTM0', 'MDAC0', 'MDAS0', 'MDBB1', 'MDBP0', 'MDCD0', 'MDDC0', 'MDEF0', 'MDEM0', 'MDHL0', 'MDHS0', 'MDJM0', 'MDLB0', 'MDLC0', 'MDLC2', 'MDLH0', 'MDMA0', 'MDMT0', 'MDNS0', 'MDPK0', 'MDPS0', 'MDSJ0', 'MDSS0', 'MDSS1', 'MDTB0', 'MDWD0', 'MDWH0', 'MDWM0', 'MEDR0', 'MEFG0', 'MEGJ0', 'MESG0', 'MEWM0', 'MFER0', 'MFMC0', 'MFRM0', 'MFWK0', 'MGAF0', 'MGAG0', 'MGES0', 'MGJC0', 'MGRL0', 'MGRP0', 'MGSH0', 'MGXP0', 'MHIT0', 'MHJB0', 'MHMG0', 'MHMR0', 'MHRM0', 'MILB0', 'MJAC0', 'MJAE0', 'MJBG0', 'MJDA0', 'MJDC0', 'MJDE0', 'MJEB0', 'MJEB1', 'MJEE0', 'MJHI0', 'MJJB0', 'MJJJ0', 'MJKR0', 'MJLB0', 'MJLG1', 'MJLS0', 'MJMA0', 'MJMD0', 'MJMM0', 'MJPM0', 'MJPM1', 'MJRH0', 'MJRH1', 'MJRP0', 'MJSR0', 'MJWS0', 'MJWT0', 'MJXL0', 'MKAH0', 'MKAJ0', 'MKAM0', 'MKDT0', 'MKJO0', 'MKLS0', 'MKLS1', 'MKLW0', 'MKXL0', 'MLBC0', 'MLEL0', 'MLJC0', 'MLJH0', 'MLNS0', 'MLSH0', 'MMAA0', 'MMAG0', 'MMAM0', 'MMAR0', 'MMBS0', 'MMDM0', 'MMDS0', 'MMEB0', 'MMGC0', 'MMGG0', 'MMGK0', 'MMJB1', 'MMRP0', 'MMSM0', 'MMXS0', 'MNET0', 'MPEB0', 'MPGH0', 'MPGR0', 'MPPC0', 'MPRB0', 'MPRD0', 'MPRK0', 'MPRT0', 'MPSW0', 'MRAB0', 'MRAB1', 'MRAI0', 'MRBC0', 'MRCG0', 'MRCW0', 'MRDD0', 'MRDS0', 'MREE0', 'MREH1', 'MRFK0', 'MRFL0', 'MRGM0', 'MRGS0', 'MRHL0', 'MRJB1', 'MRJH0', 'MRJM0', 'MRJM1', 'MRJT0', 'MRLJ0', 'MRLR0', 'MRMS0', 'MRSO0', 'MRSP0', 'MRTC0', 'MRTJ0', 'MRWA0', 'MRWS0', 'MSAT0', 'MSFH0', 'MSFV0', 'MSMC0', 'MSMS0', 'MSRG0', 'MSTF0', 'MTAS0', 'MTAT1', 'MTBC0', 'MTDB0', 'MTJG0', 'MTJM0', 'MTJS0', 'MTKP0', 'MTLB0', 'MTPF0', 'MTPG0', 'MTPP0', 'MTQC0', 'MTRC0', 'MTRR0', 'MTRT0', 'MVJH0', 'MWAD0', 'MWAR0', 'MWDK0', 'MWGR0', 'MWSB0', 'MZMB0']\n",
      "{'MARC0': 5, 'MSRG0': 175, 'MWGR0': 197, 'MMAR0': 119, 'MKAJ0': 102, 'MJWT0': 99, 'MRHL0': 155, 'MGXP0': 66, 'MDPS0': 42, 'MEDR0': 50, 'MDLC2': 36, 'MTPP0': 188, 'MJMD0': 90, 'MCEF0': 16, 'MJSR0': 97, 'MWAD0': 194, 'MCPM0': 20, 'MKAM0': 103, 'MDLH0': 37, 'MFER0': 55, 'MHIT0': 67, 'MNET0': 131, 'MRJM0': 158, 'MJRP0': 96, 'MHMG0': 69, 'MDSS0': 44, 'MLNS0': 114, 'MWDK0': 196, 'MJJB0': 83, 'MRSP0': 165, 'MJEB1': 80, 'MKLW0': 108, 'MJMA0': 89, 'MWAR0': 195, 'MDWD0': 47, 'MMRP0': 128, 'MAEB0': 1, 'MADC0': 0, 'MAPV0': 4, 'MRJH0': 157, 'MJAE0': 74, 'MDCD0': 27, 'MCHL0': 18, 'MDSS1': 45, 'MMDS0': 122, 'MRSO0': 164, 'MRCW0': 146, 'MPRK0': 138, 'MTJM0': 182, 'MKXL0': 109, 'MJHI0': 82, 'MLSH0': 115, 'MPEB0': 132, 'MPRB0': 136, 'MVJH0': 193, 'MCDD0': 14, 'MTLB0': 185, 'MJDA0': 76, 'MTKP0': 184, 'MRBC0': 144, 'MDEM0': 30, 'MMXS0': 130, 'MJEB0': 79, 'MCDC0': 13, 'MRDD0': 147, 'MSAT0': 170, 'MSMS0': 174, 'MJWS0': 98, 'MTJG0': 181, 'MJLS0': 88, 'MTDB0': 180, 'MRMS0': 163, 'MLJH0': 113, 'MJEE0': 81, 'MCAL0': 12, 'MRGM0': 153, 'MMGC0': 124, 'MGSH0': 65, 'MMAA0': 116, 'MRGS0': 154, 'MRWA0': 168, 'MHRM0': 71, 'MRTJ0': 167, 'MFWK0': 58, 'MDMA0': 38, 'MREH1': 150, 'MJKR0': 85, 'MRJT0': 160, 'MAKB0': 2, 'MCDR0': 15, 'MMJB1': 127, 'MDPK0': 41, 'MRJM1': 159, 'MAKR0': 3, 'MMAM0': 118, 'MCLM0': 19, 'MLEL0': 111, 'MZMB0': 199, 'MTQC0': 189, 'MSFH0': 171, 'MPPC0': 135, 'MSMC0': 173, 'MRLR0': 162, 'MJAC0': 73, 'MHMR0': 70, 'MBWP0': 11, 'MDWH0': 48, 'MDSJ0': 43, 'MJLB0': 86, 'MLJC0': 112, 'MJMM0': 91, 'MRFL0': 152, 'MMSM0': 129, 'MRDS0': 148, 'MSTF0': 176, 'MMBS0': 120, 'MESG0': 53, 'MDAS0': 24, 'MGRL0': 63, 'MDMT0': 39, 'MKJO0': 105, 'MJRH1': 95, 'MBGT0': 8, 'MGAF0': 59, 'MRAI0': 143, 'MCEW0': 17, 'MDWM0': 49, 'MARW0': 6, 'MRFK0': 151, 'MJXL0': 100, 'MJPM1': 93, 'MFRM0': 57, 'MTPF0': 186, 'MJLG1': 87, 'MTBC0': 179, 'MLBC0': 110, 'MDTB0': 46, 'MDHL0': 31, 'MDDC0': 28, 'MTAS0': 177, 'MKLS0': 106, 'MJBG0': 75, 'MTAT1': 178, 'MREE0': 149, 'MSFV0': 172, 'MBEF0': 7, 'MRCG0': 145, 'MDLB0': 34, 'MTJS0': 183, 'MPGR0': 134, 'MRAB1': 142, 'MEWM0': 54, 'MRJB1': 156, 'MJDE0': 78, 'MKDT0': 104, 'MBJV0': 9, 'MDBP0': 26, 'MHJB0': 68, 'MDEF0': 29, 'MDJM0': 33, 'MJDC0': 77, 'MBMA0': 10, 'MEFG0': 51, 'MMAG0': 117, 'MWSB0': 198, 'MGRP0': 64, 'MRAB0': 141, 'MFMC0': 56, 'MILB0': 72, 'MMEB0': 123, 'MDNS0': 40, 'MDLC0': 35, 'MTRR0': 191, 'MJJJ0': 84, 'MPRT0': 139, 'MRTC0': 166, 'MTPG0': 187, 'MPGH0': 133, 'MGES0': 61, 'MMGK0': 126, 'MTRT0': 192, 'MPSW0': 140, 'MCTM0': 22, 'MRLJ0': 161, 'MDBB1': 25, 'MDAC0': 23, 'MRWS0': 169, 'MCSS0': 21, 'MGAG0': 60, 'MGJC0': 62, 'MEGJ0': 52, 'MJPM0': 92, 'MDHS0': 32, 'MKLS1': 107, 'MTRC0': 190, 'MKAH0': 101, 'MJRH0': 94, 'MPRD0': 137, 'MMDM0': 121, 'MMGG0': 125}\n"
     ]
    }
   ],
   "source": [
    "ids = list(test.keys())\n",
    "ids.sort()\n",
    "print(ids)\n",
    "\n",
    "idx = {}\n",
    "for i in range(len(ids)):\n",
    "    idx[ids[i]] = i # TODO: for MATLAB set i+1 (i.e 1 to 200)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(x, win_size=10, hop_size=3):\n",
    "    r, c = x.shape\n",
    "    y = []\n",
    "    for i in range(0, c, hop_size):\n",
    "        if i + win_size > c:\n",
    "            break\n",
    "        y.append(x[:, i:i + win_size].T.flatten())\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95867, 390) (25244, 390)\n",
      "(95867,) (25244,)\n"
     ]
    }
   ],
   "source": [
    "# gmvn\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# test\n",
    "for speaker_id, feature_list in test.items():\n",
    "    speaker_id = idx[speaker_id]\n",
    "    for features in feature_list:\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            X_test.append(frame)\n",
    "            Y_test.append(speaker_id)\n",
    "            \n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# train\n",
    "for speaker_id, feature_list in train.items():\n",
    "    speaker_id = idx[speaker_id]    \n",
    "    for features in feature_list:\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            X_train.append(frame)\n",
    "            Y_train.append(speaker_id)\n",
    "            \n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "# mean var normalize\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled\n"
     ]
    }
   ],
   "source": [
    "# shuffle training data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "print('Shuffled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.83643991\n",
      "Iteration 2, loss = 3.87086831\n",
      "Iteration 3, loss = 3.41896153\n",
      "Iteration 4, loss = 3.14764144\n",
      "Iteration 5, loss = 2.94670809\n",
      "Iteration 6, loss = 2.78694333\n",
      "Iteration 7, loss = 2.65450012\n",
      "Iteration 8, loss = 2.54315300\n",
      "Iteration 9, loss = 2.44855099\n",
      "Iteration 10, loss = 2.36318937\n",
      "Iteration 11, loss = 2.28943572\n",
      "Iteration 12, loss = 2.22171694\n",
      "Iteration 13, loss = 2.16337993\n",
      "Iteration 14, loss = 2.11076753\n",
      "Iteration 15, loss = 2.06058729\n",
      "Iteration 16, loss = 2.01569451\n",
      "Iteration 17, loss = 1.97377020\n",
      "Iteration 18, loss = 1.93493543\n",
      "Iteration 19, loss = 1.90021421\n",
      "Iteration 20, loss = 1.86876984\n",
      "Iteration 21, loss = 1.83788083\n",
      "Iteration 22, loss = 1.80881042\n",
      "Iteration 23, loss = 1.78069823\n",
      "Iteration 24, loss = 1.75632422\n",
      "Iteration 25, loss = 1.73198399\n",
      "Iteration 26, loss = 1.70993466\n",
      "Iteration 27, loss = 1.68700112\n",
      "Iteration 28, loss = 1.66720285\n",
      "Iteration 29, loss = 1.64806531\n",
      "Iteration 30, loss = 1.62919835\n",
      "Iteration 31, loss = 1.61325263\n",
      "Iteration 32, loss = 1.59589634\n",
      "Iteration 33, loss = 1.57880299\n",
      "Iteration 34, loss = 1.56457789\n",
      "Iteration 35, loss = 1.54939428\n",
      "Iteration 36, loss = 1.53412373\n",
      "Iteration 37, loss = 1.52086390\n",
      "Iteration 38, loss = 1.50715526\n",
      "Iteration 39, loss = 1.49250526\n",
      "Iteration 40, loss = 1.48195014\n",
      "Iteration 41, loss = 1.46959180\n",
      "Iteration 42, loss = 1.45826521\n",
      "Iteration 43, loss = 1.44642442\n",
      "Iteration 44, loss = 1.43582282\n",
      "Iteration 45, loss = 1.42481318\n",
      "Iteration 46, loss = 1.41360901\n",
      "Iteration 47, loss = 1.40543653\n",
      "Iteration 48, loss = 1.39316485\n",
      "Iteration 49, loss = 1.38503026\n",
      "Iteration 50, loss = 1.37492706\n",
      "Iteration 51, loss = 1.36657430\n",
      "Iteration 52, loss = 1.35817681\n",
      "Iteration 53, loss = 1.34828313\n",
      "Iteration 54, loss = 1.34158417\n",
      "Iteration 55, loss = 1.33131055\n",
      "Iteration 56, loss = 1.32503578\n",
      "Iteration 57, loss = 1.31780019\n",
      "Iteration 58, loss = 1.30929549\n",
      "Iteration 59, loss = 1.30246698\n",
      "Iteration 60, loss = 1.29434092\n",
      "Iteration 61, loss = 1.28673557\n",
      "Iteration 62, loss = 1.27981512\n",
      "Iteration 63, loss = 1.27543517\n",
      "Iteration 64, loss = 1.26734102\n",
      "Iteration 65, loss = 1.25979134\n",
      "Iteration 66, loss = 1.25375330\n",
      "Iteration 67, loss = 1.24773124\n",
      "Iteration 68, loss = 1.24267798\n",
      "Iteration 69, loss = 1.23734839\n",
      "Iteration 70, loss = 1.22985515\n",
      "Iteration 71, loss = 1.22526595\n",
      "Iteration 72, loss = 1.21975161\n",
      "Iteration 73, loss = 1.21340475\n",
      "Iteration 74, loss = 1.20926229\n",
      "Iteration 75, loss = 1.20379495\n",
      "Iteration 76, loss = 1.19934146\n",
      "Iteration 77, loss = 1.19162830\n",
      "Iteration 78, loss = 1.18764516\n",
      "Iteration 79, loss = 1.18331092\n",
      "Iteration 80, loss = 1.17875658\n",
      "Iteration 81, loss = 1.17386478\n",
      "Iteration 82, loss = 1.16895667\n",
      "Iteration 83, loss = 1.16388202\n",
      "Iteration 84, loss = 1.15970788\n",
      "Iteration 85, loss = 1.15486277\n",
      "Iteration 86, loss = 1.14994222\n",
      "Iteration 87, loss = 1.14525659\n",
      "Iteration 88, loss = 1.14238075\n",
      "Iteration 89, loss = 1.13667793\n",
      "Iteration 90, loss = 1.13317955\n",
      "Iteration 91, loss = 1.13118114\n",
      "Iteration 92, loss = 1.12498761\n",
      "Iteration 93, loss = 1.12212678\n",
      "Iteration 94, loss = 1.11844904\n",
      "Iteration 95, loss = 1.11419217\n",
      "Iteration 96, loss = 1.11082561\n",
      "Iteration 97, loss = 1.10907548\n",
      "Iteration 98, loss = 1.10328628\n",
      "Iteration 99, loss = 1.10157561\n",
      "Iteration 100, loss = 1.09696474\n",
      "Iteration 101, loss = 1.09463494\n",
      "Iteration 102, loss = 1.09095225\n",
      "Iteration 103, loss = 1.08625910\n",
      "Iteration 104, loss = 1.08480358\n",
      "Iteration 105, loss = 1.07758173\n",
      "Iteration 106, loss = 1.07667385\n",
      "Iteration 107, loss = 1.07359722\n",
      "Iteration 108, loss = 1.07054790\n",
      "Iteration 109, loss = 1.06693788\n",
      "Iteration 110, loss = 1.06361802\n",
      "Iteration 111, loss = 1.06160740\n",
      "Iteration 112, loss = 1.05748386\n",
      "Iteration 113, loss = 1.05490203\n",
      "Iteration 114, loss = 1.05123306\n",
      "Iteration 115, loss = 1.04786840\n",
      "Iteration 116, loss = 1.04456918\n",
      "Iteration 117, loss = 1.04149370\n",
      "Iteration 118, loss = 1.03886428\n",
      "Iteration 119, loss = 1.03728356\n",
      "Iteration 120, loss = 1.03467881\n",
      "Iteration 121, loss = 1.03153161\n",
      "Iteration 122, loss = 1.02908957\n",
      "Iteration 123, loss = 1.02752910\n",
      "Iteration 124, loss = 1.02222562\n",
      "Iteration 125, loss = 1.02042111\n",
      "Iteration 126, loss = 1.01948705\n",
      "Iteration 127, loss = 1.01573704\n",
      "Iteration 128, loss = 1.01331020\n",
      "Iteration 129, loss = 1.01201023\n",
      "Iteration 130, loss = 1.00898006\n",
      "Iteration 131, loss = 1.00429335\n",
      "Iteration 132, loss = 1.00140269\n",
      "Iteration 133, loss = 1.00060629\n",
      "Iteration 134, loss = 0.99824585\n",
      "Iteration 135, loss = 0.99457505\n",
      "Iteration 136, loss = 0.99501480\n",
      "Iteration 137, loss = 0.99106745\n",
      "Iteration 138, loss = 0.98745670\n",
      "Iteration 139, loss = 0.98659315\n",
      "Iteration 140, loss = 0.98341655\n",
      "Iteration 141, loss = 0.98153780\n",
      "Iteration 142, loss = 0.98058961\n",
      "Iteration 143, loss = 0.97899942\n",
      "Iteration 144, loss = 0.97595584\n",
      "Iteration 145, loss = 0.97242525\n",
      "Iteration 146, loss = 0.97228147\n",
      "Iteration 147, loss = 0.96886513\n",
      "Iteration 148, loss = 0.96800427\n",
      "Iteration 149, loss = 0.96462341\n",
      "Iteration 150, loss = 0.96293376\n",
      "Iteration 151, loss = 0.96156980\n",
      "Iteration 152, loss = 0.95982125\n",
      "Iteration 153, loss = 0.95876352\n",
      "Iteration 154, loss = 0.95361141\n",
      "Iteration 155, loss = 0.95347086\n",
      "Iteration 156, loss = 0.95116925\n",
      "Iteration 157, loss = 0.94846312\n",
      "Iteration 158, loss = 0.94757155\n",
      "Iteration 159, loss = 0.94605943\n",
      "Iteration 160, loss = 0.94337534\n",
      "Iteration 161, loss = 0.94097601\n",
      "Iteration 162, loss = 0.93870800\n",
      "Iteration 163, loss = 0.93923271\n",
      "Iteration 164, loss = 0.93525821\n",
      "Iteration 165, loss = 0.93317182\n",
      "Iteration 166, loss = 0.93175380\n",
      "Iteration 167, loss = 0.93021221\n",
      "Iteration 168, loss = 0.92911154\n",
      "Iteration 169, loss = 0.92658387\n",
      "Iteration 170, loss = 0.92555324\n",
      "Iteration 171, loss = 0.92294042\n",
      "Iteration 172, loss = 0.92080181\n",
      "Iteration 173, loss = 0.92183362\n",
      "Iteration 174, loss = 0.91693108\n",
      "Iteration 175, loss = 0.91700160\n",
      "Iteration 176, loss = 0.91428732\n",
      "Iteration 177, loss = 0.91209764\n",
      "Iteration 178, loss = 0.91268538\n",
      "Iteration 179, loss = 0.90940398\n",
      "Iteration 180, loss = 0.90935276\n",
      "Iteration 181, loss = 0.90726659\n",
      "Iteration 182, loss = 0.90543809\n",
      "Iteration 183, loss = 0.90381848\n",
      "Iteration 184, loss = 0.90143812\n",
      "Iteration 185, loss = 0.89919010\n",
      "Iteration 186, loss = 0.89711942\n",
      "Iteration 187, loss = 0.89606005\n",
      "Iteration 188, loss = 0.89540772\n",
      "Iteration 189, loss = 0.89310030\n",
      "Iteration 190, loss = 0.89146709\n",
      "Iteration 191, loss = 0.89112402\n",
      "Iteration 192, loss = 0.89099229\n",
      "Iteration 193, loss = 0.88843444\n",
      "Iteration 194, loss = 0.88483482\n",
      "Iteration 195, loss = 0.88355283\n",
      "Iteration 196, loss = 0.88298280\n",
      "Iteration 197, loss = 0.88322845\n",
      "Iteration 198, loss = 0.88122408\n",
      "Iteration 199, loss = 0.87712990\n",
      "Iteration 200, loss = 0.87680625\n",
      "Iteration 201, loss = 0.87664097\n",
      "Iteration 202, loss = 0.87505269\n",
      "Iteration 203, loss = 0.87189400\n",
      "Iteration 204, loss = 0.87211051\n",
      "Iteration 205, loss = 0.87136954\n",
      "Iteration 206, loss = 0.86954374\n",
      "Iteration 207, loss = 0.86724958\n",
      "Iteration 208, loss = 0.86840363\n",
      "Iteration 209, loss = 0.86325869\n",
      "Iteration 210, loss = 0.86413152\n",
      "Iteration 211, loss = 0.86239384\n",
      "Iteration 212, loss = 0.86226491\n",
      "Iteration 213, loss = 0.86194204\n",
      "Iteration 214, loss = 0.85832553\n",
      "Iteration 215, loss = 0.85924260\n",
      "Iteration 216, loss = 0.85747871\n",
      "Iteration 217, loss = 0.85555153\n",
      "Iteration 218, loss = 0.85229125\n",
      "Iteration 219, loss = 0.85350040\n",
      "Iteration 220, loss = 0.84906365\n",
      "Iteration 221, loss = 0.84993469\n",
      "Iteration 222, loss = 0.84798058\n",
      "Iteration 223, loss = 0.84825944\n",
      "Iteration 224, loss = 0.84663865\n",
      "Iteration 225, loss = 0.84579178\n",
      "Iteration 226, loss = 0.84323487\n",
      "Iteration 227, loss = 0.84096166\n",
      "Iteration 228, loss = 0.84170754\n",
      "Iteration 229, loss = 0.84238391\n",
      "Iteration 230, loss = 0.84129662\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 231, loss = 0.70890471\n",
      "Iteration 232, loss = 0.67368935\n",
      "Iteration 233, loss = 0.66897928\n",
      "Iteration 234, loss = 0.66657117\n",
      "Iteration 235, loss = 0.66521212\n",
      "Iteration 236, loss = 0.66441811\n",
      "Iteration 237, loss = 0.66312413\n",
      "Iteration 238, loss = 0.66321612\n",
      "Iteration 239, loss = 0.66224337\n",
      "Iteration 240, loss = 0.66161620\n",
      "Iteration 241, loss = 0.66076449\n",
      "Iteration 242, loss = 0.66055429\n",
      "Iteration 243, loss = 0.65950260\n",
      "Iteration 244, loss = 0.65937800\n",
      "Iteration 245, loss = 0.65914881\n",
      "Iteration 246, loss = 0.65801233\n",
      "Iteration 247, loss = 0.65813434\n",
      "Iteration 248, loss = 0.65780553\n",
      "Iteration 249, loss = 0.65730843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 250, loss = 0.65671040\n",
      "Iteration 251, loss = 0.65620464\n",
      "Iteration 252, loss = 0.65610680\n",
      "Iteration 253, loss = 0.65518637\n",
      "Iteration 254, loss = 0.65517112\n",
      "Iteration 255, loss = 0.65459013\n",
      "Iteration 256, loss = 0.65442498\n",
      "Iteration 257, loss = 0.65405056\n",
      "Iteration 258, loss = 0.65336530\n",
      "Iteration 259, loss = 0.65311559\n",
      "Iteration 260, loss = 0.65274954\n",
      "Iteration 261, loss = 0.65266240\n",
      "Iteration 262, loss = 0.65192985\n",
      "Iteration 263, loss = 0.65166187\n",
      "Iteration 264, loss = 0.65146958\n",
      "Iteration 265, loss = 0.65093889\n",
      "Iteration 266, loss = 0.65083410\n",
      "Iteration 267, loss = 0.65068655\n",
      "Iteration 268, loss = 0.65007845\n",
      "Iteration 269, loss = 0.64967837\n",
      "Iteration 270, loss = 0.64923262\n",
      "Iteration 271, loss = 0.64908747\n",
      "Iteration 272, loss = 0.64859397\n",
      "Iteration 273, loss = 0.64843882\n",
      "Iteration 274, loss = 0.64784642\n",
      "Iteration 275, loss = 0.64802620\n",
      "Iteration 276, loss = 0.64735420\n",
      "Iteration 277, loss = 0.64685557\n",
      "Iteration 278, loss = 0.64690595\n",
      "Iteration 279, loss = 0.64613857\n",
      "Iteration 280, loss = 0.64616193\n",
      "Iteration 281, loss = 0.64557652\n",
      "Iteration 282, loss = 0.64565081\n",
      "Iteration 283, loss = 0.64529777\n",
      "Iteration 284, loss = 0.64493939\n",
      "Iteration 285, loss = 0.64420143\n",
      "Iteration 286, loss = 0.64406157\n",
      "Iteration 287, loss = 0.64401694\n",
      "Iteration 288, loss = 0.64362224\n",
      "Iteration 289, loss = 0.64327711\n",
      "Iteration 290, loss = 0.64328819\n",
      "Iteration 291, loss = 0.64284347\n",
      "Iteration 292, loss = 0.64239119\n",
      "Iteration 293, loss = 0.64215150\n",
      "Iteration 294, loss = 0.64188716\n",
      "Iteration 295, loss = 0.64157976\n",
      "Iteration 296, loss = 0.64156600\n",
      "Iteration 297, loss = 0.64102805\n",
      "Iteration 298, loss = 0.64046906\n",
      "Iteration 299, loss = 0.64018950\n",
      "Iteration 300, loss = 0.64036254\n",
      "Iteration 301, loss = 0.63982359\n",
      "Iteration 302, loss = 0.63969651\n",
      "Iteration 303, loss = 0.63894148\n",
      "Iteration 304, loss = 0.63916100\n",
      "Iteration 305, loss = 0.63849579\n",
      "Iteration 306, loss = 0.63815282\n",
      "Iteration 307, loss = 0.63831385\n",
      "Iteration 308, loss = 0.63771737\n",
      "Iteration 309, loss = 0.63722241\n",
      "Iteration 310, loss = 0.63758228\n",
      "Iteration 311, loss = 0.63691327\n",
      "Iteration 312, loss = 0.63659895\n",
      "Iteration 313, loss = 0.63625089\n",
      "Iteration 314, loss = 0.63617686\n",
      "Iteration 315, loss = 0.63562705\n",
      "Iteration 316, loss = 0.63560291\n",
      "Iteration 317, loss = 0.63531438\n",
      "Iteration 318, loss = 0.63517638\n",
      "Iteration 319, loss = 0.63483684\n",
      "Iteration 320, loss = 0.63417743\n",
      "Iteration 321, loss = 0.63411873\n",
      "Iteration 322, loss = 0.63383258\n",
      "Iteration 323, loss = 0.63384565\n",
      "Iteration 324, loss = 0.63311932\n",
      "Iteration 325, loss = 0.63322639\n",
      "Iteration 326, loss = 0.63279765\n",
      "Iteration 327, loss = 0.63249533\n",
      "Iteration 328, loss = 0.63219112\n",
      "Iteration 329, loss = 0.63205969\n",
      "Iteration 330, loss = 0.63184369\n",
      "Iteration 331, loss = 0.63130838\n",
      "Iteration 332, loss = 0.63122098\n",
      "Iteration 333, loss = 0.63106304\n",
      "Iteration 334, loss = 0.63047112\n",
      "Iteration 335, loss = 0.62987201\n",
      "Iteration 336, loss = 0.63003122\n",
      "Iteration 337, loss = 0.62971448\n",
      "Iteration 338, loss = 0.62943056\n",
      "Iteration 339, loss = 0.62945574\n",
      "Iteration 340, loss = 0.62898199\n",
      "Iteration 341, loss = 0.62877756\n",
      "Iteration 342, loss = 0.62849862\n",
      "Iteration 343, loss = 0.62844871\n",
      "Iteration 344, loss = 0.62778648\n",
      "Iteration 345, loss = 0.62788946\n",
      "Iteration 346, loss = 0.62758079\n",
      "Iteration 347, loss = 0.62742707\n",
      "Iteration 348, loss = 0.62668496\n",
      "Iteration 349, loss = 0.62675195\n",
      "Iteration 350, loss = 0.62669084\n",
      "Iteration 351, loss = 0.62641982\n",
      "Iteration 352, loss = 0.62585839\n",
      "Iteration 353, loss = 0.62557598\n",
      "Iteration 354, loss = 0.62531084\n",
      "Iteration 355, loss = 0.62519645\n",
      "Iteration 356, loss = 0.62506794\n",
      "Iteration 357, loss = 0.62481484\n",
      "Iteration 358, loss = 0.62442840\n",
      "Iteration 359, loss = 0.62419972\n",
      "Iteration 360, loss = 0.62390168\n",
      "Iteration 361, loss = 0.62373173\n",
      "Iteration 362, loss = 0.62357151\n",
      "Iteration 363, loss = 0.62317457\n",
      "Iteration 364, loss = 0.62295779\n",
      "Iteration 365, loss = 0.62274501\n",
      "Iteration 366, loss = 0.62209792\n",
      "Iteration 367, loss = 0.62244765\n",
      "Iteration 368, loss = 0.62172854\n",
      "Iteration 369, loss = 0.62153412\n",
      "Iteration 370, loss = 0.62141012\n",
      "Iteration 371, loss = 0.62092837\n",
      "Iteration 372, loss = 0.62128246\n",
      "Iteration 373, loss = 0.62042036\n",
      "Iteration 374, loss = 0.62088469\n",
      "Iteration 375, loss = 0.62016338\n",
      "Iteration 376, loss = 0.62006348\n",
      "Iteration 377, loss = 0.61994133\n",
      "Iteration 378, loss = 0.61941630\n",
      "Iteration 379, loss = 0.61926285\n",
      "Iteration 380, loss = 0.61864325\n",
      "Iteration 381, loss = 0.61893790\n",
      "Iteration 382, loss = 0.61831127\n",
      "Iteration 383, loss = 0.61800903\n",
      "Iteration 384, loss = 0.61788313\n",
      "Iteration 385, loss = 0.61778744\n",
      "Iteration 386, loss = 0.61776128\n",
      "Iteration 387, loss = 0.61735202\n",
      "Iteration 388, loss = 0.61711338\n",
      "Iteration 389, loss = 0.61698997\n",
      "Iteration 390, loss = 0.61653346\n",
      "Iteration 391, loss = 0.61622759\n",
      "Iteration 392, loss = 0.61603640\n",
      "Iteration 393, loss = 0.61607123\n",
      "Iteration 394, loss = 0.61563556\n",
      "Iteration 395, loss = 0.61532509\n",
      "Iteration 396, loss = 0.61511635\n",
      "Iteration 397, loss = 0.61491012\n",
      "Iteration 398, loss = 0.61448698\n",
      "Iteration 399, loss = 0.61415498\n",
      "Iteration 400, loss = 0.61407389\n",
      "Iteration 401, loss = 0.61369103\n",
      "Iteration 402, loss = 0.61379230\n",
      "Iteration 403, loss = 0.61359029\n",
      "Iteration 404, loss = 0.61335343\n",
      "Iteration 405, loss = 0.61306555\n",
      "Iteration 406, loss = 0.61255660\n",
      "Iteration 407, loss = 0.61240671\n",
      "Iteration 408, loss = 0.61207830\n",
      "Iteration 409, loss = 0.61232622\n",
      "Iteration 410, loss = 0.61209127\n",
      "Iteration 411, loss = 0.61136727\n",
      "Iteration 412, loss = 0.61157310\n",
      "Iteration 413, loss = 0.61095407\n",
      "Iteration 414, loss = 0.61126381\n",
      "Iteration 415, loss = 0.61074263\n",
      "Iteration 416, loss = 0.61047294\n",
      "Iteration 417, loss = 0.61032334\n",
      "Iteration 418, loss = 0.61002363\n",
      "Iteration 419, loss = 0.60954251\n",
      "Iteration 420, loss = 0.60943721\n",
      "Iteration 421, loss = 0.60952863\n",
      "Iteration 422, loss = 0.60906921\n",
      "Iteration 423, loss = 0.60878355\n",
      "Iteration 424, loss = 0.60843814\n",
      "Iteration 425, loss = 0.60852296\n",
      "Iteration 426, loss = 0.60811120\n",
      "Iteration 427, loss = 0.60786523\n",
      "Iteration 428, loss = 0.60778485\n",
      "Iteration 429, loss = 0.60759731\n",
      "Iteration 430, loss = 0.60725991\n",
      "Iteration 431, loss = 0.60721233\n",
      "Iteration 432, loss = 0.60628880\n",
      "Iteration 433, loss = 0.60671725\n",
      "Iteration 434, loss = 0.60627755\n",
      "Iteration 435, loss = 0.60610798\n",
      "Iteration 436, loss = 0.60603965\n",
      "Iteration 437, loss = 0.60584764\n",
      "Iteration 438, loss = 0.60541659\n",
      "Iteration 439, loss = 0.60536992\n",
      "Iteration 440, loss = 0.60486425\n",
      "Iteration 441, loss = 0.60454520\n",
      "Iteration 442, loss = 0.60424172\n",
      "Iteration 443, loss = 0.60452035\n",
      "Iteration 444, loss = 0.60378863\n",
      "Iteration 445, loss = 0.60412597\n",
      "Iteration 446, loss = 0.60373876\n",
      "Iteration 447, loss = 0.60358543\n",
      "Iteration 448, loss = 0.60296600\n",
      "Iteration 449, loss = 0.60292881\n",
      "Iteration 450, loss = 0.60286972\n",
      "Iteration 451, loss = 0.60244533\n",
      "Iteration 452, loss = 0.60221434\n",
      "Iteration 453, loss = 0.60215359\n",
      "Iteration 454, loss = 0.60173891\n",
      "Iteration 455, loss = 0.60149280\n",
      "Iteration 456, loss = 0.60133874\n",
      "Iteration 457, loss = 0.60094106\n",
      "Iteration 458, loss = 0.60080261\n",
      "Iteration 459, loss = 0.60094094\n",
      "Iteration 460, loss = 0.60074858\n",
      "Iteration 461, loss = 0.60058134\n",
      "Iteration 462, loss = 0.60024685\n",
      "Iteration 463, loss = 0.60029091\n",
      "Iteration 464, loss = 0.59905828\n",
      "Iteration 465, loss = 0.59941046\n",
      "Iteration 466, loss = 0.59935828\n",
      "Iteration 467, loss = 0.59935302\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 468, loss = 0.57084745\n",
      "Iteration 469, loss = 0.56713552\n",
      "Iteration 470, loss = 0.56628204\n",
      "Iteration 471, loss = 0.56615374\n",
      "Iteration 472, loss = 0.56592966\n",
      "Iteration 473, loss = 0.56577975\n",
      "Iteration 474, loss = 0.56564220\n",
      "Iteration 475, loss = 0.56554706\n",
      "Iteration 476, loss = 0.56551786\n",
      "Iteration 477, loss = 0.56540864\n",
      "Iteration 478, loss = 0.56544778\n",
      "Iteration 479, loss = 0.56525424\n",
      "Iteration 480, loss = 0.56519038\n",
      "Iteration 481, loss = 0.56531319\n",
      "Iteration 482, loss = 0.56513820\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 483, loss = 0.55844130\n",
      "Iteration 484, loss = 0.55812745\n",
      "Iteration 485, loss = 0.55802100\n",
      "Iteration 486, loss = 0.55796550\n",
      "Iteration 487, loss = 0.55793788\n",
      "Iteration 488, loss = 0.55792663\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 489, loss = 0.55638106\n",
      "Iteration 490, loss = 0.55637297\n",
      "Iteration 491, loss = 0.55636947\n",
      "Iteration 492, loss = 0.55636361\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 493, loss = 0.55604520\n",
      "Iteration 494, loss = 0.55604365\n",
      "Iteration 495, loss = 0.55604310\n",
      "Iteration 496, loss = 0.55604209\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 497, loss = 0.55597817\n",
      "Iteration 498, loss = 0.55597789\n",
      "Iteration 499, loss = 0.55597776\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.861235\n",
      "Test set score: 0.296744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000, alpha=1e-3,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.01, learning_rate='adaptive',\n",
    "                    warm_start=True)\n",
    "\n",
    "mlp.fit(X_train, Y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, Y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.84823433\n",
      "Iteration 2, loss = 3.88486541\n",
      "Iteration 3, loss = 3.43537199\n",
      "Iteration 4, loss = 3.16622684\n",
      "Iteration 5, loss = 2.96778319\n",
      "Iteration 6, loss = 2.81027816\n",
      "Iteration 7, loss = 2.68026579\n",
      "Iteration 8, loss = 2.57124372\n",
      "Iteration 9, loss = 2.47866258\n",
      "Iteration 10, loss = 2.39561010\n",
      "Iteration 11, loss = 2.32379340\n",
      "Iteration 12, loss = 2.25790240\n",
      "Iteration 13, loss = 2.20109330\n",
      "Iteration 14, loss = 2.15043786\n",
      "Iteration 15, loss = 2.10227564\n",
      "Iteration 16, loss = 2.05900617\n",
      "Iteration 17, loss = 2.01907173\n",
      "Iteration 18, loss = 1.98198393\n",
      "Iteration 19, loss = 1.94891217\n",
      "Iteration 20, loss = 1.91944626\n",
      "Iteration 21, loss = 1.89009110\n",
      "Iteration 22, loss = 1.86280710\n",
      "Iteration 23, loss = 1.83646230\n",
      "Iteration 24, loss = 1.81316396\n",
      "Iteration 25, loss = 1.79029128\n",
      "Iteration 26, loss = 1.76962118\n",
      "Iteration 27, loss = 1.74829853\n",
      "Iteration 28, loss = 1.72977725\n",
      "Iteration 29, loss = 1.71211608\n",
      "Iteration 30, loss = 1.69473925\n",
      "Iteration 31, loss = 1.68031551\n",
      "Iteration 32, loss = 1.66377720\n",
      "Iteration 33, loss = 1.64832857\n",
      "Iteration 34, loss = 1.63557633\n",
      "Iteration 35, loss = 1.62104907\n",
      "Iteration 36, loss = 1.60700738\n",
      "Iteration 37, loss = 1.59523658\n",
      "Iteration 38, loss = 1.58293457\n",
      "Iteration 39, loss = 1.56952074\n",
      "Iteration 40, loss = 1.56013495\n",
      "Iteration 41, loss = 1.54853612\n",
      "Iteration 42, loss = 1.53915092\n",
      "Iteration 43, loss = 1.52825512\n",
      "Iteration 44, loss = 1.51926725\n",
      "Iteration 45, loss = 1.50935522\n",
      "Iteration 46, loss = 1.49894088\n",
      "Iteration 47, loss = 1.49201417\n",
      "Iteration 48, loss = 1.48115526\n",
      "Iteration 49, loss = 1.47408221\n",
      "Iteration 50, loss = 1.46517824\n",
      "Iteration 51, loss = 1.45761425\n",
      "Iteration 52, loss = 1.45039411\n",
      "Iteration 53, loss = 1.44145171\n",
      "Iteration 54, loss = 1.43525364\n",
      "Iteration 55, loss = 1.42680191\n",
      "Iteration 56, loss = 1.42111964\n",
      "Iteration 57, loss = 1.41512844\n",
      "Iteration 58, loss = 1.40742912\n",
      "Iteration 59, loss = 1.40181776\n",
      "Iteration 60, loss = 1.39482976\n",
      "Iteration 61, loss = 1.38781398\n",
      "Iteration 62, loss = 1.38154048\n",
      "Iteration 63, loss = 1.37841361\n",
      "Iteration 64, loss = 1.37144927\n",
      "Iteration 65, loss = 1.36491724\n",
      "Iteration 66, loss = 1.36031572\n",
      "Iteration 67, loss = 1.35401653\n",
      "Iteration 68, loss = 1.35088700\n",
      "Iteration 69, loss = 1.34595845\n",
      "Iteration 70, loss = 1.33930528\n",
      "Iteration 71, loss = 1.33608452\n",
      "Iteration 72, loss = 1.33118173\n",
      "Iteration 73, loss = 1.32602653\n",
      "Iteration 74, loss = 1.32194921\n",
      "Iteration 75, loss = 1.31753053\n",
      "Iteration 76, loss = 1.31323503\n",
      "Iteration 77, loss = 1.30703842\n",
      "Iteration 78, loss = 1.30427202\n",
      "Iteration 79, loss = 1.30039396\n",
      "Iteration 80, loss = 1.29649947\n",
      "Iteration 81, loss = 1.29271020\n",
      "Iteration 82, loss = 1.28802194\n",
      "Iteration 83, loss = 1.28435425\n",
      "Iteration 84, loss = 1.28035669\n",
      "Iteration 85, loss = 1.27689087\n",
      "Iteration 86, loss = 1.27253141\n",
      "Iteration 87, loss = 1.26909809\n",
      "Iteration 88, loss = 1.26619145\n",
      "Iteration 89, loss = 1.26178170\n",
      "Iteration 90, loss = 1.25882610\n",
      "Iteration 91, loss = 1.25811053\n",
      "Iteration 92, loss = 1.25162978\n",
      "Iteration 93, loss = 1.24944702\n",
      "Iteration 94, loss = 1.24684397\n",
      "Iteration 95, loss = 1.24331266\n",
      "Iteration 96, loss = 1.24022117\n",
      "Iteration 97, loss = 1.23897550\n",
      "Iteration 98, loss = 1.23372805\n",
      "Iteration 99, loss = 1.23251277\n",
      "Iteration 100, loss = 1.22904634\n",
      "Iteration 101, loss = 1.22730688\n",
      "Iteration 102, loss = 1.22416240\n",
      "Iteration 103, loss = 1.22097134\n",
      "Iteration 104, loss = 1.21864751\n",
      "Iteration 105, loss = 1.21382421\n",
      "Iteration 106, loss = 1.21313795\n",
      "Iteration 107, loss = 1.21059699\n",
      "Iteration 108, loss = 1.20711600\n",
      "Iteration 109, loss = 1.20353577\n",
      "Iteration 110, loss = 1.20237248\n",
      "Iteration 111, loss = 1.20050048\n",
      "Iteration 112, loss = 1.19737486\n",
      "Iteration 113, loss = 1.19549553\n",
      "Iteration 114, loss = 1.19216966\n",
      "Iteration 115, loss = 1.19086356\n",
      "Iteration 116, loss = 1.18651434\n",
      "Iteration 117, loss = 1.18474175\n",
      "Iteration 118, loss = 1.18267020\n",
      "Iteration 119, loss = 1.18101116\n",
      "Iteration 120, loss = 1.17849121\n",
      "Iteration 121, loss = 1.17656153\n",
      "Iteration 122, loss = 1.17461522\n",
      "Iteration 123, loss = 1.17321818\n",
      "Iteration 124, loss = 1.16957022\n",
      "Iteration 125, loss = 1.16877098\n",
      "Iteration 126, loss = 1.16667609\n",
      "Iteration 127, loss = 1.16404226\n",
      "Iteration 128, loss = 1.16242087\n",
      "Iteration 129, loss = 1.16104833\n",
      "Iteration 130, loss = 1.15951468\n",
      "Iteration 131, loss = 1.15541399\n",
      "Iteration 132, loss = 1.15298538\n",
      "Iteration 133, loss = 1.15326414\n",
      "Iteration 134, loss = 1.15061959\n",
      "Iteration 135, loss = 1.14819689\n",
      "Iteration 136, loss = 1.14761946\n",
      "Iteration 137, loss = 1.14580453\n",
      "Iteration 138, loss = 1.14214195\n",
      "Iteration 139, loss = 1.14134699\n",
      "Iteration 140, loss = 1.13971935\n",
      "Iteration 141, loss = 1.13888152\n",
      "Iteration 142, loss = 1.13810160\n",
      "Iteration 143, loss = 1.13574174\n",
      "Iteration 144, loss = 1.13376195\n",
      "Iteration 145, loss = 1.13074339\n",
      "Iteration 146, loss = 1.13140266\n",
      "Iteration 147, loss = 1.12801640\n",
      "Iteration 148, loss = 1.12713118\n",
      "Iteration 149, loss = 1.12405769\n",
      "Iteration 150, loss = 1.12471191\n",
      "Iteration 151, loss = 1.12240671\n",
      "Iteration 152, loss = 1.12095622\n",
      "Iteration 153, loss = 1.12032813\n",
      "Iteration 154, loss = 1.11792620\n",
      "Iteration 155, loss = 1.11665805\n",
      "Iteration 156, loss = 1.11428272\n",
      "Iteration 157, loss = 1.11252201\n",
      "Iteration 158, loss = 1.11188598\n",
      "Iteration 159, loss = 1.11063139\n",
      "Iteration 160, loss = 1.10869807\n",
      "Iteration 161, loss = 1.10812849\n",
      "Iteration 162, loss = 1.10440194\n",
      "Iteration 163, loss = 1.10513448\n",
      "Iteration 164, loss = 1.10325019\n",
      "Iteration 165, loss = 1.10145332\n",
      "Iteration 166, loss = 1.10078142\n",
      "Iteration 167, loss = 1.09895195\n",
      "Iteration 168, loss = 1.09737550\n",
      "Iteration 169, loss = 1.09661171\n",
      "Iteration 170, loss = 1.09545099\n",
      "Iteration 171, loss = 1.09491230\n",
      "Iteration 172, loss = 1.09268537\n",
      "Iteration 173, loss = 1.09303572\n",
      "Iteration 174, loss = 1.09112648\n",
      "Iteration 175, loss = 1.08971326\n",
      "Iteration 176, loss = 1.08729097\n",
      "Iteration 177, loss = 1.08699257\n",
      "Iteration 178, loss = 1.08706251\n",
      "Iteration 179, loss = 1.08434352\n",
      "Iteration 180, loss = 1.08469937\n",
      "Iteration 181, loss = 1.08384781\n",
      "Iteration 182, loss = 1.08286532\n",
      "Iteration 183, loss = 1.07971385\n",
      "Iteration 184, loss = 1.07840582\n",
      "Iteration 185, loss = 1.07708163\n",
      "Iteration 186, loss = 1.07644818\n",
      "Iteration 187, loss = 1.07510290\n",
      "Iteration 188, loss = 1.07418616\n",
      "Iteration 189, loss = 1.07318144\n",
      "Iteration 190, loss = 1.07180386\n",
      "Iteration 191, loss = 1.07195438\n",
      "Iteration 192, loss = 1.07114535\n",
      "Iteration 193, loss = 1.06908158\n",
      "Iteration 194, loss = 1.06745631\n",
      "Iteration 195, loss = 1.06582361\n",
      "Iteration 196, loss = 1.06661633\n",
      "Iteration 197, loss = 1.06768777\n",
      "Iteration 198, loss = 1.06459822\n",
      "Iteration 199, loss = 1.06261280\n",
      "Iteration 200, loss = 1.06142601\n",
      "Iteration 201, loss = 1.06346088\n",
      "Iteration 202, loss = 1.05932905\n",
      "Iteration 203, loss = 1.05854250\n",
      "Iteration 204, loss = 1.05792145\n",
      "Iteration 205, loss = 1.05648522\n",
      "Iteration 206, loss = 1.05668208\n",
      "Iteration 207, loss = 1.05526315\n",
      "Iteration 208, loss = 1.05681587\n",
      "Iteration 209, loss = 1.05264404\n",
      "Iteration 210, loss = 1.05223802\n",
      "Iteration 211, loss = 1.05239959\n",
      "Iteration 212, loss = 1.05228477\n",
      "Iteration 213, loss = 1.05128108\n",
      "Iteration 214, loss = 1.04932033\n",
      "Iteration 215, loss = 1.04959661\n",
      "Iteration 216, loss = 1.04747953\n",
      "Iteration 217, loss = 1.04862827\n",
      "Iteration 218, loss = 1.04610385\n",
      "Iteration 219, loss = 1.04555405\n",
      "Iteration 220, loss = 1.04358437\n",
      "Iteration 221, loss = 1.04415033\n",
      "Iteration 222, loss = 1.04172567\n",
      "Iteration 223, loss = 1.04244522\n",
      "Iteration 224, loss = 1.04092081\n",
      "Iteration 225, loss = 1.04075473\n",
      "Iteration 226, loss = 1.04042099\n",
      "Iteration 227, loss = 1.03733082\n",
      "Iteration 228, loss = 1.03898730\n",
      "Iteration 229, loss = 1.03996388\n",
      "Iteration 230, loss = 1.03826286\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 231, loss = 0.92022343\n",
      "Iteration 232, loss = 0.89050674\n",
      "Iteration 233, loss = 0.88667654\n",
      "Iteration 234, loss = 0.88486884\n",
      "Iteration 235, loss = 0.88360543\n",
      "Iteration 236, loss = 0.88317443\n",
      "Iteration 237, loss = 0.88234824\n",
      "Iteration 238, loss = 0.88217537\n",
      "Iteration 239, loss = 0.88166510\n",
      "Iteration 240, loss = 0.88112454\n",
      "Iteration 241, loss = 0.88056137\n",
      "Iteration 242, loss = 0.88069261\n",
      "Iteration 243, loss = 0.87997134\n",
      "Iteration 244, loss = 0.87948892\n",
      "Iteration 245, loss = 0.87957996\n",
      "Iteration 246, loss = 0.87885309\n",
      "Iteration 247, loss = 0.87895702\n",
      "Iteration 248, loss = 0.87885909\n",
      "Iteration 249, loss = 0.87826330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 250, loss = 0.87796072\n",
      "Iteration 251, loss = 0.87777781\n",
      "Iteration 252, loss = 0.87757092\n",
      "Iteration 253, loss = 0.87689829\n",
      "Iteration 254, loss = 0.87694859\n",
      "Iteration 255, loss = 0.87639168\n",
      "Iteration 256, loss = 0.87643136\n",
      "Iteration 257, loss = 0.87602615\n",
      "Iteration 258, loss = 0.87559616\n",
      "Iteration 259, loss = 0.87553722\n",
      "Iteration 260, loss = 0.87527260\n",
      "Iteration 261, loss = 0.87533279\n",
      "Iteration 262, loss = 0.87481660\n",
      "Iteration 263, loss = 0.87456826\n",
      "Iteration 264, loss = 0.87447884\n",
      "Iteration 265, loss = 0.87423245\n",
      "Iteration 266, loss = 0.87402250\n",
      "Iteration 267, loss = 0.87395879\n",
      "Iteration 268, loss = 0.87360645\n",
      "Iteration 269, loss = 0.87305361\n",
      "Iteration 270, loss = 0.87299354\n",
      "Iteration 271, loss = 0.87304045\n",
      "Iteration 272, loss = 0.87276438\n",
      "Iteration 273, loss = 0.87271891\n",
      "Iteration 274, loss = 0.87227468\n",
      "Iteration 275, loss = 0.87239638\n",
      "Iteration 276, loss = 0.87221381\n",
      "Iteration 277, loss = 0.87154065\n",
      "Iteration 278, loss = 0.87147650\n",
      "Iteration 279, loss = 0.87138408\n",
      "Iteration 280, loss = 0.87105798\n",
      "Iteration 281, loss = 0.87086591\n",
      "Iteration 282, loss = 0.87070557\n",
      "Iteration 283, loss = 0.87043325\n",
      "Iteration 284, loss = 0.87058737\n",
      "Iteration 285, loss = 0.86984214\n",
      "Iteration 286, loss = 0.86970405\n",
      "Iteration 287, loss = 0.86958321\n",
      "Iteration 288, loss = 0.86946547\n",
      "Iteration 289, loss = 0.86927588\n",
      "Iteration 290, loss = 0.86944872\n",
      "Iteration 291, loss = 0.86907612\n",
      "Iteration 292, loss = 0.86871314\n",
      "Iteration 293, loss = 0.86869860\n",
      "Iteration 294, loss = 0.86863795\n",
      "Iteration 295, loss = 0.86819367\n",
      "Iteration 296, loss = 0.86860012\n",
      "Iteration 297, loss = 0.86771930\n",
      "Iteration 298, loss = 0.86746164\n",
      "Iteration 299, loss = 0.86753803\n",
      "Iteration 300, loss = 0.86778303\n",
      "Iteration 301, loss = 0.86758529\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 302, loss = 0.84170644\n",
      "Iteration 303, loss = 0.83856830\n",
      "Iteration 304, loss = 0.83798612\n",
      "Iteration 305, loss = 0.83773060\n",
      "Iteration 306, loss = 0.83759876\n",
      "Iteration 307, loss = 0.83751463\n",
      "Iteration 308, loss = 0.83737361\n",
      "Iteration 309, loss = 0.83729018\n",
      "Iteration 310, loss = 0.83728565\n",
      "Iteration 311, loss = 0.83721601\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 312, loss = 0.83098186\n",
      "Iteration 313, loss = 0.83074586\n",
      "Iteration 314, loss = 0.83066201\n",
      "Iteration 315, loss = 0.83061677\n",
      "Iteration 316, loss = 0.83059759\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 317, loss = 0.82918334\n",
      "Iteration 318, loss = 0.82917867\n",
      "Iteration 319, loss = 0.82917169\n",
      "Iteration 320, loss = 0.82916671\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 321, loss = 0.82887557\n",
      "Iteration 322, loss = 0.82887477\n",
      "Iteration 323, loss = 0.82887395\n",
      "Iteration 324, loss = 0.82887340\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 325, loss = 0.82881473\n",
      "Iteration 326, loss = 0.82881455\n",
      "Iteration 327, loss = 0.82881428\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.835366\n",
      "Test set score: 0.309380\n"
     ]
    }
   ],
   "source": [
    "# with alpha = 1e-2 TODO: make tol to 1e-3\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000, alpha=1e-2,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.01, learning_rate='adaptive',\n",
    "                    warm_start=True)\n",
    "\n",
    "mlp2.fit(X_train, Y_train)\n",
    "print(\"Training set score: %f\" % mlp2.score(X_train, Y_train))\n",
    "print(\"Test set score: %f\" % mlp2.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.08553676\n",
      "Iteration 2, loss = 3.18011014\n",
      "Iteration 3, loss = 2.90430351\n",
      "Iteration 4, loss = 2.75712901\n",
      "Iteration 5, loss = 2.65627727\n",
      "Iteration 6, loss = 2.59416119\n",
      "Iteration 7, loss = 2.54163814\n",
      "Iteration 8, loss = 2.50128167\n",
      "Iteration 9, loss = 2.47490265\n",
      "Iteration 10, loss = 2.44778385\n",
      "Iteration 11, loss = 2.43149896\n",
      "Iteration 12, loss = 2.41310202\n",
      "Iteration 13, loss = 2.39924455\n",
      "Iteration 14, loss = 2.38673252\n",
      "Iteration 15, loss = 2.37358086\n",
      "Iteration 16, loss = 2.36544009\n",
      "Iteration 17, loss = 2.35252032\n",
      "Iteration 18, loss = 2.34581672\n",
      "Iteration 19, loss = 2.33684322\n",
      "Iteration 20, loss = 2.33023752\n",
      "Iteration 21, loss = 2.32699194\n",
      "Iteration 22, loss = 2.32210172\n",
      "Iteration 23, loss = 2.31623994\n",
      "Iteration 24, loss = 2.30878230\n",
      "Iteration 25, loss = 2.30635006\n",
      "Iteration 26, loss = 2.29959521\n",
      "Iteration 27, loss = 2.29808571\n",
      "Iteration 28, loss = 2.29334613\n",
      "Iteration 29, loss = 2.28971027\n",
      "Iteration 30, loss = 2.28563740\n",
      "Iteration 31, loss = 2.28440675\n",
      "Iteration 32, loss = 2.28130900\n",
      "Iteration 33, loss = 2.27791422\n",
      "Iteration 34, loss = 2.27758445\n",
      "Iteration 35, loss = 2.27357275\n",
      "Iteration 36, loss = 2.26892709\n",
      "Iteration 37, loss = 2.26594054\n",
      "Iteration 38, loss = 2.26820482\n",
      "Iteration 39, loss = 2.26347599\n",
      "Iteration 40, loss = 2.25811595\n",
      "Iteration 41, loss = 2.25887928\n",
      "Iteration 42, loss = 2.26140619\n",
      "Iteration 43, loss = 2.25512603\n",
      "Iteration 44, loss = 2.25367490\n",
      "Iteration 45, loss = 2.25277340\n",
      "Iteration 46, loss = 2.24942681\n",
      "Iteration 47, loss = 2.25058073\n",
      "Iteration 48, loss = 2.24737281\n",
      "Iteration 49, loss = 2.24567991\n",
      "Iteration 50, loss = 2.24448596\n",
      "Iteration 51, loss = 2.24799785\n",
      "Iteration 52, loss = 2.24258085\n",
      "Iteration 53, loss = 2.24228972\n",
      "Iteration 54, loss = 2.24039325\n",
      "Iteration 55, loss = 2.23789004\n",
      "Iteration 56, loss = 2.23527913\n",
      "Iteration 57, loss = 2.23707116\n",
      "Iteration 58, loss = 2.23427525\n",
      "Iteration 59, loss = 2.23712015\n",
      "Iteration 60, loss = 2.23333574\n",
      "Iteration 61, loss = 2.23353074\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.010000\n",
      "Iteration 62, loss = 1.94019198\n",
      "Iteration 63, loss = 1.85855725\n",
      "Iteration 64, loss = 1.84415668\n",
      "Iteration 65, loss = 1.83627398\n",
      "Iteration 66, loss = 1.83127811\n",
      "Iteration 67, loss = 1.82672714\n",
      "Iteration 68, loss = 1.82462799\n",
      "Iteration 69, loss = 1.82155021\n",
      "Iteration 70, loss = 1.81876751\n",
      "Iteration 71, loss = 1.81719385\n",
      "Iteration 72, loss = 1.81486686\n",
      "Iteration 73, loss = 1.81335189\n",
      "Iteration 74, loss = 1.81222916\n",
      "Iteration 75, loss = 1.81010521\n",
      "Iteration 76, loss = 1.80903330\n",
      "Iteration 77, loss = 1.80830643\n",
      "Iteration 78, loss = 1.80711327\n",
      "Iteration 79, loss = 1.80517341\n",
      "Iteration 80, loss = 1.80503062\n",
      "Iteration 81, loss = 1.80374923\n",
      "Iteration 82, loss = 1.80248030\n",
      "Iteration 83, loss = 1.80135990\n",
      "Iteration 84, loss = 1.80106611\n",
      "Iteration 85, loss = 1.80015652\n",
      "Iteration 86, loss = 1.79962438\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 87, loss = 1.72874522\n",
      "Iteration 88, loss = 1.71568703\n",
      "Iteration 89, loss = 1.71351259\n",
      "Iteration 90, loss = 1.71249293\n",
      "Iteration 91, loss = 1.71217319\n",
      "Iteration 92, loss = 1.71135052\n",
      "Iteration 93, loss = 1.71081084\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 94, loss = 1.69360335\n",
      "Iteration 95, loss = 1.69188309\n",
      "Iteration 96, loss = 1.69155946\n",
      "Iteration 97, loss = 1.69144057\n",
      "Iteration 98, loss = 1.69123867\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 99, loss = 1.68696526\n",
      "Iteration 100, loss = 1.68688473\n",
      "Iteration 101, loss = 1.68683047\n",
      "Iteration 102, loss = 1.68679306\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 103, loss = 1.68585903\n",
      "Iteration 104, loss = 1.68585499\n",
      "Iteration 105, loss = 1.68584926\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 106, loss = 1.68565936\n",
      "Iteration 107, loss = 1.68565811\n",
      "Iteration 108, loss = 1.68565753\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 109, loss = 1.68561908\n",
      "Iteration 110, loss = 1.68561884\n",
      "Iteration 111, loss = 1.68561857\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.733652\n",
      "Test set score: 0.349113\n"
     ]
    }
   ],
   "source": [
    "# with alpha = 1e-1, tol to 1e-3, eta=0.05\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp3 = MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000, alpha=1e-1,\n",
    "                    solver='sgd', verbose=10, tol=1e-3, random_state=1,\n",
    "                    learning_rate_init=.05, learning_rate='adaptive',\n",
    "                    warm_start=True)\n",
    "\n",
    "mlp3.fit(X_train, Y_train)\n",
    "print(\"Training set score: %f\" % mlp3.score(X_train, Y_train))\n",
    "print(\"Test set score: %f\" % mlp3.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.48688165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pratik varshney\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.233417\n",
      "Test set score: 0.180162\n",
      "\n",
      "Iteration 2, loss = 3.66833779\n",
      "Training set score: 0.297808\n",
      "Test set score: 0.222191\n",
      "\n",
      "Iteration 3, loss = 3.52018472\n",
      "Training set score: 0.326963\n",
      "Test set score: 0.236928\n",
      "\n",
      "Iteration 4, loss = 3.46453306\n",
      "Training set score: 0.345082\n",
      "Test set score: 0.243583\n",
      "\n",
      "Iteration 5, loss = 3.43483316\n",
      "Training set score: 0.357454\n",
      "Test set score: 0.249129\n",
      "\n",
      "Iteration 6, loss = 3.41442941\n",
      "Training set score: 0.366007\n",
      "Test set score: 0.254754\n",
      "\n",
      "Iteration 7, loss = 3.39964328\n",
      "Training set score: 0.373851\n",
      "Test set score: 0.259507\n",
      "\n",
      "Iteration 8, loss = 3.38727146\n",
      "Training set score: 0.378858\n",
      "Test set score: 0.260933\n",
      "\n",
      "Iteration 9, loss = 3.37825359\n",
      "Training set score: 0.382353\n",
      "Test set score: 0.261448\n",
      "\n",
      "Iteration 10, loss = 3.37060689\n",
      "Training set score: 0.386609\n",
      "Test set score: 0.263548\n",
      "\n",
      "Iteration 11, loss = 3.36426833\n",
      "Training set score: 0.389654\n",
      "Test set score: 0.265212\n",
      "\n",
      "Iteration 12, loss = 3.35873280\n",
      "Training set score: 0.391803\n",
      "Test set score: 0.267549\n",
      "\n",
      "Iteration 13, loss = 3.35334819\n",
      "Training set score: 0.394015\n",
      "Test set score: 0.268183\n",
      "\n",
      "Iteration 14, loss = 3.34909363\n",
      "Training set score: 0.396695\n",
      "Test set score: 0.269926\n",
      "\n",
      "Iteration 15, loss = 3.34515217\n",
      "Training set score: 0.398072\n",
      "Test set score: 0.270361\n",
      "\n",
      "Iteration 16, loss = 3.34164927\n",
      "Training set score: 0.399334\n",
      "Test set score: 0.272025\n",
      "\n",
      "Iteration 17, loss = 3.33846643\n",
      "Training set score: 0.401734\n",
      "Test set score: 0.272144\n",
      "\n",
      "Iteration 18, loss = 3.33568634\n",
      "Training set score: 0.402818\n",
      "Test set score: 0.273055\n",
      "\n",
      "Iteration 19, loss = 3.33311931\n",
      "Training set score: 0.404154\n",
      "Test set score: 0.273174\n",
      "\n",
      "Iteration 20, loss = 3.33076025\n",
      "Training set score: 0.404654\n",
      "Test set score: 0.274164\n",
      "\n",
      "Iteration 21, loss = 3.32851359\n",
      "Training set score: 0.406271\n",
      "Test set score: 0.276145\n",
      "\n",
      "Iteration 22, loss = 3.32655606\n",
      "Training set score: 0.407241\n",
      "Test set score: 0.276620\n",
      "\n",
      "Iteration 23, loss = 3.32486126\n",
      "Training set score: 0.408472\n",
      "Test set score: 0.277294\n",
      "\n",
      "Iteration 24, loss = 3.32347808\n",
      "Training set score: 0.408910\n",
      "Test set score: 0.276462\n",
      "\n",
      "Iteration 25, loss = 3.32200715\n",
      "Training set score: 0.410006\n",
      "Test set score: 0.277214\n",
      "\n",
      "Iteration 26, loss = 3.32072172\n",
      "Training set score: 0.410725\n",
      "Test set score: 0.276818\n",
      "\n",
      "Iteration 27, loss = 3.31937159\n",
      "Training set score: 0.411341\n",
      "Test set score: 0.276977\n",
      "\n",
      "Iteration 28, loss = 3.31807975\n",
      "Training set score: 0.412050\n",
      "Test set score: 0.277096\n",
      "\n",
      "Iteration 29, loss = 3.31664570\n",
      "Training set score: 0.411925\n",
      "Test set score: 0.277175\n",
      "\n",
      "Iteration 30, loss = 3.31566801\n",
      "Training set score: 0.412592\n",
      "Test set score: 0.277492\n",
      "\n",
      "Iteration 31, loss = 3.31466761\n",
      "Training set score: 0.413802\n",
      "Test set score: 0.277927\n",
      "\n",
      "Iteration 32, loss = 3.31343898\n",
      "Training set score: 0.414303\n",
      "Test set score: 0.278125\n",
      "\n",
      "Iteration 33, loss = 3.31252905\n",
      "Training set score: 0.415252\n",
      "Test set score: 0.277848\n",
      "\n",
      "Iteration 34, loss = 3.31175113\n",
      "Training set score: 0.414762\n",
      "Test set score: 0.277373\n",
      "\n",
      "Iteration 35, loss = 3.31101761\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.010000\n",
      "Training set score: 0.415941\n",
      "Test set score: 0.277016\n",
      "\n",
      "Iteration 36, loss = 3.08472161\n",
      "Training set score: 0.496772\n",
      "Test set score: 0.324671\n",
      "\n",
      "Iteration 37, loss = 3.02418632\n",
      "Training set score: 0.503041\n",
      "Test set score: 0.327563\n",
      "\n",
      "Iteration 38, loss = 3.01129083\n",
      "Training set score: 0.506368\n",
      "Test set score: 0.328078\n",
      "\n",
      "Iteration 39, loss = 3.00422832\n",
      "Training set score: 0.508851\n",
      "Test set score: 0.328118\n",
      "\n",
      "Iteration 40, loss = 2.99953998\n",
      "Training set score: 0.510196\n",
      "Test set score: 0.328474\n",
      "\n",
      "Iteration 41, loss = 2.99611997\n",
      "Training set score: 0.511886\n",
      "Test set score: 0.328672\n",
      "\n",
      "Iteration 42, loss = 2.99349369\n",
      "Training set score: 0.512919\n",
      "Test set score: 0.328078\n",
      "\n",
      "Iteration 43, loss = 2.99139170\n",
      "Training set score: 0.513962\n",
      "Test set score: 0.328553\n",
      "\n",
      "Iteration 44, loss = 2.98966689\n",
      "Training set score: 0.514870\n",
      "Test set score: 0.327959\n",
      "\n",
      "Iteration 45, loss = 2.98821522\n",
      "Training set score: 0.515349\n",
      "Test set score: 0.327920\n",
      "\n",
      "Iteration 46, loss = 2.98696233\n",
      "Training set score: 0.516007\n",
      "Test set score: 0.328434\n",
      "\n",
      "Iteration 47, loss = 2.98590188\n",
      "Training set score: 0.516476\n",
      "Test set score: 0.328593\n",
      "\n",
      "Iteration 48, loss = 2.98488137\n",
      "Training set score: 0.516977\n",
      "Test set score: 0.328870\n",
      "\n",
      "Iteration 49, loss = 2.98394143\n",
      "Training set score: 0.517696\n",
      "Test set score: 0.328672\n",
      "\n",
      "Iteration 50, loss = 2.98311784\n",
      "Training set score: 0.517853\n",
      "Test set score: 0.328870\n",
      "\n",
      "Iteration 51, loss = 2.98239747\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.002000\n",
      "Training set score: 0.517936\n",
      "Test set score: 0.328870\n",
      "\n",
      "Iteration 52, loss = 2.91486243\n",
      "Training set score: 0.539696\n",
      "Test set score: 0.346419\n",
      "\n",
      "Iteration 53, loss = 2.90280940\n",
      "Training set score: 0.541365\n",
      "Test set score: 0.347528\n",
      "\n",
      "Iteration 54, loss = 2.90024993\n",
      "Training set score: 0.542084\n",
      "Test set score: 0.347489\n",
      "\n",
      "Iteration 55, loss = 2.89881326\n",
      "Training set score: 0.542533\n",
      "Test set score: 0.347766\n",
      "\n",
      "Iteration 56, loss = 2.89779780\n",
      "Training set score: 0.542908\n",
      "Test set score: 0.347766\n",
      "\n",
      "Iteration 57, loss = 2.89699807\n",
      "Training set score: 0.543169\n",
      "Test set score: 0.347687\n",
      "\n",
      "Iteration 58, loss = 2.89632602\n",
      "Training set score: 0.543628\n",
      "Test set score: 0.347449\n",
      "\n",
      "Iteration 59, loss = 2.89573398\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.000400\n",
      "Training set score: 0.543795\n",
      "Test set score: 0.347449\n",
      "\n",
      "Iteration 60, loss = 2.87860733\n",
      "Training set score: 0.546330\n",
      "Test set score: 0.349152\n",
      "\n",
      "Iteration 61, loss = 2.87632401\n",
      "Training set score: 0.546674\n",
      "Test set score: 0.348796\n",
      "\n",
      "Iteration 62, loss = 2.87581878\n",
      "Training set score: 0.546883\n",
      "Test set score: 0.349113\n",
      "\n",
      "Iteration 63, loss = 2.87550551\n",
      "Training set score: 0.547018\n",
      "Test set score: 0.349232\n",
      "\n",
      "Iteration 64, loss = 2.87526796\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.000080\n",
      "Training set score: 0.547206\n",
      "Test set score: 0.349311\n",
      "\n",
      "Iteration 65, loss = 2.87081256\n",
      "Training set score: 0.547206\n",
      "Test set score: 0.349033\n",
      "\n",
      "Iteration 66, loss = 2.87058375\n",
      "Training set score: 0.547216\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 67, loss = 2.87045791\n",
      "Training set score: 0.547227\n",
      "Test set score: 0.349509\n",
      "\n",
      "Iteration 68, loss = 2.87037438\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.000016\n",
      "Training set score: 0.547008\n",
      "Test set score: 0.349271\n",
      "\n",
      "Iteration 69, loss = 2.86933962\n",
      "Training set score: 0.547081\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 70, loss = 2.86932312\n",
      "Training set score: 0.546977\n",
      "Test set score: 0.349469\n",
      "\n",
      "Iteration 71, loss = 2.86930771\n",
      "Training set score: 0.546904\n",
      "Test set score: 0.349390\n",
      "\n",
      "Iteration 72, loss = 2.86929334\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.000003\n",
      "Training set score: 0.546841\n",
      "Test set score: 0.349311\n",
      "\n",
      "Iteration 73, loss = 2.86908034\n",
      "Training set score: 0.546872\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 74, loss = 2.86907755\n",
      "Training set score: 0.546883\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 75, loss = 2.86907476\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Training set score: 0.546924\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 76, loss = 2.86903192\n",
      "Training set score: 0.546935\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 77, loss = 2.86903137\n",
      "Training set score: 0.546935\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 78, loss = 2.86903082\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546935\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 79, loss = 2.86903027\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546924\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 80, loss = 2.86902972\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546914\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 81, loss = 2.86902917\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.546924\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 82, loss = 2.86902863\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546924\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 83, loss = 2.86902808\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546935\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 84, loss = 2.86902754\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546924\n",
      "Test set score: 0.349390\n",
      "\n",
      "Iteration 85, loss = 2.86902699\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546914\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 86, loss = 2.86902645\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546914\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 87, loss = 2.86902591\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546914\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 88, loss = 2.86902537\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546914\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 89, loss = 2.86902484\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546914\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 90, loss = 2.86902430\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546914\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 91, loss = 2.86902376\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546914\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 92, loss = 2.86902323\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546914\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 93, loss = 2.86902270\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546904\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 94, loss = 2.86902217\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546893\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 95, loss = 2.86902164\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546893\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 96, loss = 2.86902111\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546904\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 97, loss = 2.86902059\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546904\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 98, loss = 2.86902006\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546904\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 99, loss = 2.86901954\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546904\n",
      "Test set score: 0.349430\n",
      "\n",
      "Iteration 100, loss = 2.86901901\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.546904\n",
      "Test set score: 0.349390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with alpha = 1e-1, tol to 1e-3, eta=0.05\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp4 = MLPClassifier(hidden_layer_sizes=(200,), max_iter=1, alpha=0.5,\n",
    "                    solver='sgd', verbose=10, tol=1e-3, random_state=1,\n",
    "                    learning_rate_init=.05, learning_rate='adaptive',\n",
    "                    warm_start=True)\n",
    "\n",
    "max_iter = 100\n",
    "for i in range(max_iter):\n",
    "    mlp4.fit(X_train, Y_train)\n",
    "    print(\"Training set score: %f\" % mlp4.score(X_train, Y_train))\n",
    "    print(\"Test set score: %f\" % mlp4.score(X_test, Y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766331658291\n",
      "maxlen 381\n"
     ]
    }
   ],
   "source": [
    "# segment acc\n",
    "from scipy import stats\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "# test\n",
    "maxlen = 0\n",
    "for speaker_id, feature_list in test.items():\n",
    "    speaker_id = idx[speaker_id]\n",
    "    for features in feature_list:\n",
    "        maxlen = max(maxlen, features.shape[1])\n",
    "        x = []\n",
    "        # y = []\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            x.append(frame)\n",
    "            # y.append(speaker_id)\n",
    "        x = scaler.transform(x)\n",
    "        pred = stats.mode(mlp4.predict(x)).mode[0]\n",
    "        y_true.append(speaker_id)\n",
    "        y_pred.append(pred)\n",
    "print(sum(np.array(y_true) == np.array(y_pred))/len(y_true))\n",
    "print('maxlen', maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925\n",
      "maxlen 381\n"
     ]
    }
   ],
   "source": [
    "# all segment acc ~ file\n",
    "from scipy import stats\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "# test\n",
    "maxlen = 0\n",
    "for speaker_id, feature_list in test.items():\n",
    "    speaker_id = idx[speaker_id]\n",
    "    x = []\n",
    "    for features in feature_list:\n",
    "        maxlen = max(maxlen, features.shape[1])\n",
    "        # y = []\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            x.append(frame)\n",
    "            # y.append(speaker_id)\n",
    "    x = scaler.transform(x)\n",
    "    pred = stats.mode(mlp.predict(x)).mode[0]\n",
    "    y_true.append(speaker_id)\n",
    "    y_pred.append(pred)\n",
    "print(sum(np.array(y_true) == np.array(y_pred))/len(y_true))\n",
    "print('maxlen', maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
