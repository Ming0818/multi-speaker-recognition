{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2289 DR1-MCPM0-SA1-00.wav\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "indir = 'chunks/' # already VAD\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = 'chunks/'\n",
    "flist = [f for f in listdir(indir) if isfile(join(indir, f))]\n",
    "print(len(flist), flist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(fname):\n",
    "    attr = fname.split('.')[0].split('-')\n",
    "    dialect = attr[0]\n",
    "    gender = attr[1][0]\n",
    "    speaker_id = attr[1]\n",
    "    sentence_type = attr[2][:2]\n",
    "    return dialect, gender, speaker_id, sentence_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "\n",
    "for fname in flist:\n",
    "    input_path = indir + fname\n",
    "    y, sr = librosa.load(input_path, sr=None) # set sr=None for orig file sr otherwise it is converted to ~22K\n",
    "\n",
    "    # scaling the maximum of absolute amplitude to 1\n",
    "    processed_data = y/max(abs(y))\n",
    "    \n",
    "    # TODO: calc VAD (already done)\n",
    "    \n",
    "    # https://groups.google.com/forum/#!topic/librosa/V4Z1HpTKn8Q\n",
    "    mfcc = librosa.feature.mfcc(y=processed_data, sr=sr, n_mfcc=13, n_fft=(25*sr)//1000, hop_length=(10*sr)//1000)\n",
    "    mfcc[0] = librosa.feature.rmse(processed_data, hop_length=int(0.010*sr), n_fft=int(0.025*sr)) \n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    features = np.vstack([mfcc, mfcc_delta, mfcc_delta2]) \n",
    "    \n",
    "    # split train test\n",
    "    dialect, gender, speaker_id, sentence_type = get_attributes(fname)\n",
    "    if sentence_type == 'SA':\n",
    "        test.setdefault(speaker_id, []).append(features)\n",
    "    else:\n",
    "        train.setdefault(speaker_id, []).append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MADC0', 'MAEB0', 'MAKB0', 'MAKR0', 'MAPV0', 'MARC0', 'MARW0', 'MBEF0', 'MBGT0', 'MBJV0', 'MBMA0', 'MBWP0', 'MCAL0', 'MCDC0', 'MCDD0', 'MCDR0', 'MCEF0', 'MCEW0', 'MCHL0', 'MCLM0', 'MCPM0', 'MCSS0', 'MCTM0', 'MDAC0', 'MDAS0', 'MDBB1', 'MDBP0', 'MDCD0', 'MDDC0', 'MDEF0', 'MDEM0', 'MDHL0', 'MDHS0', 'MDJM0', 'MDLB0', 'MDLC0', 'MDLC2', 'MDLH0', 'MDMA0', 'MDMT0', 'MDNS0', 'MDPK0', 'MDPS0', 'MDSJ0', 'MDSS0', 'MDSS1', 'MDTB0', 'MDWD0', 'MDWH0', 'MDWM0', 'MEDR0', 'MEFG0', 'MEGJ0', 'MESG0', 'MEWM0', 'MFER0', 'MFMC0', 'MFRM0', 'MFWK0', 'MGAF0', 'MGAG0', 'MGES0', 'MGJC0', 'MGRL0', 'MGRP0', 'MGSH0', 'MGXP0', 'MHIT0', 'MHJB0', 'MHMG0', 'MHMR0', 'MHRM0', 'MILB0', 'MJAC0', 'MJAE0', 'MJBG0', 'MJDA0', 'MJDC0', 'MJDE0', 'MJEB0', 'MJEB1', 'MJEE0', 'MJHI0', 'MJJB0', 'MJJJ0', 'MJKR0', 'MJLB0', 'MJLG1', 'MJLS0', 'MJMA0', 'MJMD0', 'MJMM0', 'MJPM0', 'MJPM1', 'MJRH0', 'MJRH1', 'MJRP0', 'MJSR0', 'MJWS0', 'MJWT0', 'MJXL0', 'MKAH0', 'MKAJ0', 'MKAM0', 'MKDT0', 'MKJO0', 'MKLS0', 'MKLS1', 'MKLW0', 'MKXL0', 'MLBC0', 'MLEL0', 'MLJC0', 'MLJH0', 'MLNS0', 'MLSH0', 'MMAA0', 'MMAG0', 'MMAM0', 'MMAR0', 'MMBS0', 'MMDM0', 'MMDS0', 'MMEB0', 'MMGC0', 'MMGG0', 'MMGK0', 'MMJB1', 'MMRP0', 'MMSM0', 'MMXS0', 'MNET0', 'MPEB0', 'MPGH0', 'MPGR0', 'MPPC0', 'MPRB0', 'MPRD0', 'MPRK0', 'MPRT0', 'MPSW0', 'MRAB0', 'MRAB1', 'MRAI0', 'MRBC0', 'MRCG0', 'MRCW0', 'MRDD0', 'MRDS0', 'MREE0', 'MREH1', 'MRFK0', 'MRFL0', 'MRGM0', 'MRGS0', 'MRHL0', 'MRJB1', 'MRJH0', 'MRJM0', 'MRJM1', 'MRJT0', 'MRLJ0', 'MRLR0', 'MRMS0', 'MRSO0', 'MRSP0', 'MRTC0', 'MRTJ0', 'MRWA0', 'MRWS0', 'MSAT0', 'MSFH0', 'MSFV0', 'MSMC0', 'MSMS0', 'MSRG0', 'MSTF0', 'MTAS0', 'MTAT1', 'MTBC0', 'MTDB0', 'MTJG0', 'MTJM0', 'MTJS0', 'MTKP0', 'MTLB0', 'MTPF0', 'MTPG0', 'MTPP0', 'MTQC0', 'MTRC0', 'MTRR0', 'MTRT0', 'MVJH0', 'MWAD0', 'MWAR0', 'MWDK0', 'MWGR0', 'MWSB0', 'MZMB0']\n",
      "{'MKAJ0': 102, 'MRHL0': 155, 'MREH1': 150, 'MGES0': 61, 'MLBC0': 110, 'MPPC0': 135, 'MJWS0': 98, 'MJWT0': 99, 'MRTC0': 166, 'MFER0': 55, 'MDHS0': 32, 'MTRR0': 191, 'MJXL0': 100, 'MBWP0': 11, 'MTDB0': 180, 'MTRC0': 190, 'MRCW0': 146, 'MJDE0': 78, 'MTJS0': 183, 'MDSS0': 44, 'MDMT0': 39, 'MBMA0': 10, 'MCTM0': 22, 'MDBB1': 25, 'MMRP0': 128, 'MDLH0': 37, 'MCDC0': 13, 'MDJM0': 33, 'MJLB0': 86, 'MEGJ0': 52, 'MMXS0': 130, 'MMEB0': 123, 'MPRT0': 139, 'MRJT0': 160, 'MDHL0': 31, 'MRFK0': 151, 'MWGR0': 197, 'MGRP0': 64, 'MLSH0': 115, 'MGRL0': 63, 'MSTF0': 176, 'MVJH0': 193, 'MBGT0': 8, 'MCAL0': 12, 'MCPM0': 20, 'MDDC0': 28, 'MGAG0': 60, 'MRJM0': 158, 'MREE0': 149, 'MTPF0': 186, 'MKAH0': 101, 'MJRH1': 95, 'MDPK0': 41, 'MJAC0': 73, 'MCDR0': 15, 'MFWK0': 58, 'MKAM0': 103, 'MWAD0': 194, 'MAEB0': 1, 'MDSJ0': 43, 'MSMC0': 173, 'MDWM0': 49, 'MTRT0': 192, 'MJHI0': 82, 'MJMD0': 90, 'MGSH0': 65, 'MJEE0': 81, 'MDNS0': 40, 'MRGS0': 154, 'MRAB0': 141, 'MHMG0': 69, 'MPRD0': 137, 'MKJO0': 105, 'MMDM0': 121, 'MRJM1': 159, 'MRMS0': 163, 'MKLW0': 108, 'MMGG0': 125, 'MTAS0': 177, 'MGAF0': 59, 'MLEL0': 111, 'MDMA0': 38, 'MSAT0': 170, 'MFRM0': 57, 'MEFG0': 51, 'MJMA0': 89, 'MPGH0': 133, 'MARC0': 5, 'MGJC0': 62, 'MDEM0': 30, 'MEWM0': 54, 'MTJG0': 181, 'MJMM0': 91, 'MTBC0': 179, 'MTPG0': 187, 'MJLG1': 87, 'MRSO0': 164, 'MTKP0': 184, 'MMAG0': 117, 'MPSW0': 140, 'MJRP0': 96, 'MJPM0': 92, 'MCEW0': 17, 'MRLJ0': 161, 'MHJB0': 68, 'MSRG0': 175, 'MAKR0': 3, 'MARW0': 6, 'MAPV0': 4, 'MRFL0': 152, 'MHMR0': 70, 'MDEF0': 29, 'MPRK0': 138, 'MRGM0': 153, 'MDSS1': 45, 'MMGK0': 126, 'MPGR0': 134, 'MDTB0': 46, 'MRLR0': 162, 'MZMB0': 199, 'MTJM0': 182, 'MDAS0': 24, 'MTPP0': 188, 'MPRB0': 136, 'MLJC0': 112, 'MJBG0': 75, 'MRWA0': 168, 'MMDS0': 122, 'MMAA0': 116, 'MMJB1': 127, 'MJDA0': 76, 'MJKR0': 85, 'MADC0': 0, 'MJPM1': 93, 'MDPS0': 42, 'MWAR0': 195, 'MDCD0': 27, 'MMSM0': 129, 'MLNS0': 114, 'MRDS0': 148, 'MTAT1': 178, 'MCLM0': 19, 'MRDD0': 147, 'MRWS0': 169, 'MFMC0': 56, 'MJEB1': 80, 'MILB0': 72, 'MDWH0': 48, 'MESG0': 53, 'MCSS0': 21, 'MDAC0': 23, 'MDLB0': 34, 'MCHL0': 18, 'MRAI0': 143, 'MJRH0': 94, 'MJAE0': 74, 'MMBS0': 120, 'MJSR0': 97, 'MWSB0': 198, 'MJLS0': 88, 'MJEB0': 79, 'MBJV0': 9, 'MRCG0': 145, 'MEDR0': 50, 'MRJH0': 157, 'MBEF0': 7, 'MRJB1': 156, 'MMAR0': 119, 'MAKB0': 2, 'MNET0': 131, 'MRSP0': 165, 'MRAB1': 142, 'MMGC0': 124, 'MKLS0': 106, 'MTQC0': 189, 'MCEF0': 16, 'MTLB0': 185, 'MKXL0': 109, 'MDBP0': 26, 'MRBC0': 144, 'MJJB0': 83, 'MSFV0': 172, 'MHRM0': 71, 'MHIT0': 67, 'MJDC0': 77, 'MKLS1': 107, 'MGXP0': 66, 'MWDK0': 196, 'MJJJ0': 84, 'MPEB0': 132, 'MDLC2': 36, 'MLJH0': 113, 'MRTJ0': 167, 'MSFH0': 171, 'MCDD0': 14, 'MSMS0': 174, 'MDWD0': 47, 'MKDT0': 104, 'MMAM0': 118, 'MDLC0': 35}\n"
     ]
    }
   ],
   "source": [
    "ids = list(test.keys())\n",
    "ids.sort()\n",
    "print(ids)\n",
    "\n",
    "idx = {}\n",
    "for i in range(len(ids)):\n",
    "    idx[ids[i]] = i # TODO: for MATLAB set i+1 (i.e 1 to 200)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(x, win_size=10, hop_size=3):\n",
    "    r, c = x.shape\n",
    "    y = []\n",
    "    for i in range(0, c, hop_size):\n",
    "        if i + win_size > c:\n",
    "            break\n",
    "        y.append(x[:, i:i + win_size].T.flatten())\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "# smvn of train + test\n",
    "for speaker_id_str, feature_list in train.items():\n",
    "    speaker_id = idx[speaker_id_str]\n",
    "    \n",
    "    # calc speaker level mean and std\n",
    "    data = []\n",
    "    for features in feature_list:\n",
    "        frames = features.T\n",
    "        for frame in frames:\n",
    "            data.append(frame)\n",
    "    # test data\n",
    "    for features in test[speaker_id_str]:\n",
    "        frames = features.T\n",
    "        for frame in frames:\n",
    "            data.append(frame)\n",
    "    \n",
    "    data = np.array(data)\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    \n",
    "    # speaker level normalize\n",
    "    for features in feature_list:\n",
    "        features = (features - mean)/std\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            X_train.append(frame)\n",
    "            Y_train.append(speaker_id)\n",
    "            \n",
    "    # test\n",
    "    for features in test[speaker_id_str]:\n",
    "        features = (features - mean)/std\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            X_test.append(frame)\n",
    "            Y_test.append(speaker_id)\n",
    "            \n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137520, 390) (36091, 390)\n",
      "(137520,) (36091,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, Y_train = shuffle(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.79987856\n",
      "Iteration 2, loss = 3.97951609\n",
      "Iteration 3, loss = 3.49010571\n",
      "Iteration 4, loss = 3.19037954\n",
      "Iteration 5, loss = 2.97333647\n",
      "Iteration 6, loss = 2.79801911\n",
      "Iteration 7, loss = 2.65466405\n",
      "Iteration 8, loss = 2.52934152\n",
      "Iteration 9, loss = 2.42031341\n",
      "Iteration 10, loss = 2.32323049\n",
      "Iteration 11, loss = 2.23694064\n",
      "Iteration 12, loss = 2.15878638\n",
      "Iteration 13, loss = 2.08918504\n",
      "Iteration 14, loss = 2.02254103\n",
      "Iteration 15, loss = 1.96315661\n",
      "Iteration 16, loss = 1.90854563\n",
      "Iteration 17, loss = 1.85891213\n",
      "Iteration 18, loss = 1.81251427\n",
      "Iteration 19, loss = 1.76697304\n",
      "Iteration 20, loss = 1.72508257\n",
      "Iteration 21, loss = 1.68809449\n",
      "Iteration 22, loss = 1.65125448\n",
      "Iteration 23, loss = 1.61834834\n",
      "Iteration 24, loss = 1.58774619\n",
      "Iteration 25, loss = 1.55654125\n",
      "Iteration 26, loss = 1.52949976\n",
      "Iteration 27, loss = 1.50327693\n",
      "Iteration 28, loss = 1.47636920\n",
      "Iteration 29, loss = 1.45327254\n",
      "Iteration 30, loss = 1.42730093\n",
      "Iteration 31, loss = 1.40852136\n",
      "Iteration 32, loss = 1.38717284\n",
      "Iteration 33, loss = 1.36729617\n",
      "Iteration 34, loss = 1.34835351\n",
      "Iteration 35, loss = 1.32994653\n",
      "Iteration 36, loss = 1.31161694\n",
      "Iteration 37, loss = 1.29460269\n",
      "Iteration 38, loss = 1.27992064\n",
      "Iteration 39, loss = 1.26343949\n",
      "Iteration 40, loss = 1.24721878\n",
      "Iteration 41, loss = 1.23456266\n",
      "Iteration 42, loss = 1.22123561\n",
      "Iteration 43, loss = 1.20552659\n",
      "Iteration 44, loss = 1.19445373\n",
      "Iteration 45, loss = 1.18114595\n",
      "Iteration 46, loss = 1.16858386\n",
      "Iteration 47, loss = 1.15774782\n",
      "Iteration 48, loss = 1.14734040\n",
      "Iteration 49, loss = 1.13469891\n",
      "Iteration 50, loss = 1.12531943\n",
      "Iteration 51, loss = 1.11305415\n",
      "Iteration 52, loss = 1.10381368\n",
      "Iteration 53, loss = 1.09373212\n",
      "Iteration 54, loss = 1.08284080\n",
      "Iteration 55, loss = 1.07485829\n",
      "Iteration 56, loss = 1.06665193\n",
      "Iteration 57, loss = 1.05553682\n",
      "Iteration 58, loss = 1.04672668\n",
      "Iteration 59, loss = 1.04077402\n",
      "Iteration 60, loss = 1.03121507\n",
      "Iteration 61, loss = 1.02524501\n",
      "Iteration 62, loss = 1.01750378\n",
      "Iteration 63, loss = 1.00723117\n",
      "Iteration 64, loss = 1.00154373\n",
      "Iteration 65, loss = 0.99371315\n",
      "Iteration 66, loss = 0.98679227\n",
      "Iteration 67, loss = 0.97986077\n",
      "Iteration 68, loss = 0.97118643\n",
      "Iteration 69, loss = 0.96404576\n",
      "Iteration 70, loss = 0.95927445\n",
      "Iteration 71, loss = 0.95240363\n",
      "Iteration 72, loss = 0.94658719\n",
      "Iteration 73, loss = 0.94063204\n",
      "Iteration 74, loss = 0.93307530\n",
      "Iteration 75, loss = 0.92662908\n",
      "Iteration 76, loss = 0.92206341\n",
      "Iteration 77, loss = 0.91610401\n",
      "Iteration 78, loss = 0.91080792\n",
      "Iteration 79, loss = 0.90385791\n",
      "Iteration 80, loss = 0.89917072\n",
      "Iteration 81, loss = 0.89263774\n",
      "Iteration 82, loss = 0.88977938\n",
      "Iteration 83, loss = 0.88362840\n",
      "Iteration 84, loss = 0.87754006\n",
      "Iteration 85, loss = 0.87252723\n",
      "Iteration 86, loss = 0.86793118\n",
      "Iteration 87, loss = 0.86335603\n",
      "Iteration 88, loss = 0.85841736\n",
      "Iteration 89, loss = 0.85384950\n",
      "Iteration 90, loss = 0.84839590\n",
      "Iteration 91, loss = 0.84407121\n",
      "Iteration 92, loss = 0.83901595\n",
      "Iteration 93, loss = 0.83564306\n",
      "Iteration 94, loss = 0.83063092\n",
      "Iteration 95, loss = 0.82668250\n",
      "Iteration 96, loss = 0.82137008\n",
      "Iteration 97, loss = 0.81801779\n",
      "Iteration 98, loss = 0.81371420\n",
      "Iteration 99, loss = 0.81120090\n",
      "Iteration 100, loss = 0.80447838\n",
      "Iteration 101, loss = 0.80005046\n",
      "Iteration 102, loss = 0.79592827\n",
      "Iteration 103, loss = 0.79369547\n",
      "Iteration 104, loss = 0.78964449\n",
      "Iteration 105, loss = 0.78621616\n",
      "Iteration 106, loss = 0.77949975\n",
      "Iteration 107, loss = 0.77970231\n",
      "Iteration 108, loss = 0.77453753\n",
      "Iteration 109, loss = 0.77083334\n",
      "Iteration 110, loss = 0.76834460\n",
      "Iteration 111, loss = 0.76330647\n",
      "Iteration 112, loss = 0.76021327\n",
      "Iteration 113, loss = 0.75583018\n",
      "Iteration 114, loss = 0.75191761\n",
      "Iteration 115, loss = 0.74990232\n",
      "Iteration 116, loss = 0.74663434\n",
      "Iteration 117, loss = 0.74362321\n",
      "Iteration 118, loss = 0.74036068\n",
      "Iteration 119, loss = 0.73614409\n",
      "Iteration 120, loss = 0.73519731\n",
      "Iteration 121, loss = 0.72956655\n",
      "Iteration 122, loss = 0.72775111\n",
      "Iteration 123, loss = 0.72412563\n",
      "Iteration 124, loss = 0.72116352\n",
      "Iteration 125, loss = 0.71981921\n",
      "Iteration 126, loss = 0.71481410\n",
      "Iteration 127, loss = 0.71293787\n",
      "Iteration 128, loss = 0.70979573\n",
      "Iteration 129, loss = 0.70769778\n",
      "Iteration 130, loss = 0.70526021\n",
      "Iteration 131, loss = 0.69996265\n",
      "Iteration 132, loss = 0.69737629\n",
      "Iteration 133, loss = 0.69766704\n",
      "Iteration 134, loss = 0.69130172\n",
      "Iteration 135, loss = 0.68892987\n",
      "Iteration 136, loss = 0.68733992\n",
      "Iteration 137, loss = 0.68662041\n",
      "Iteration 138, loss = 0.68169971\n",
      "Iteration 139, loss = 0.67905722\n",
      "Iteration 140, loss = 0.67709474\n",
      "Iteration 141, loss = 0.67494114\n",
      "Iteration 142, loss = 0.67117863\n",
      "Iteration 143, loss = 0.66979882\n",
      "Iteration 144, loss = 0.66765324\n",
      "Iteration 145, loss = 0.66273612\n",
      "Iteration 146, loss = 0.66046640\n",
      "Iteration 147, loss = 0.66025502\n",
      "Iteration 148, loss = 0.65761762\n",
      "Iteration 149, loss = 0.65516466\n",
      "Iteration 150, loss = 0.65090504\n",
      "Iteration 151, loss = 0.64871189\n",
      "Iteration 152, loss = 0.64803186\n",
      "Iteration 153, loss = 0.64580923\n",
      "Iteration 154, loss = 0.64377438\n",
      "Iteration 155, loss = 0.64094097\n",
      "Iteration 156, loss = 0.63878499\n",
      "Iteration 157, loss = 0.63638521\n",
      "Iteration 158, loss = 0.63364834\n",
      "Iteration 159, loss = 0.63380265\n",
      "Iteration 160, loss = 0.63194267\n",
      "Iteration 161, loss = 0.62566028\n",
      "Iteration 162, loss = 0.62396235\n",
      "Iteration 163, loss = 0.62390541\n",
      "Iteration 164, loss = 0.62213943\n",
      "Iteration 165, loss = 0.62103992\n",
      "Iteration 166, loss = 0.61740865\n",
      "Iteration 167, loss = 0.61678469\n",
      "Iteration 168, loss = 0.61286972\n",
      "Iteration 169, loss = 0.61168271\n",
      "Iteration 170, loss = 0.61006989\n",
      "Iteration 171, loss = 0.60770380\n",
      "Iteration 172, loss = 0.60652225\n",
      "Iteration 173, loss = 0.60341086\n",
      "Iteration 174, loss = 0.60101176\n",
      "Iteration 175, loss = 0.59846626\n",
      "Iteration 176, loss = 0.59826353\n",
      "Iteration 177, loss = 0.59535733\n",
      "Iteration 178, loss = 0.59490448\n",
      "Iteration 179, loss = 0.59209330\n",
      "Iteration 180, loss = 0.59039955\n",
      "Iteration 181, loss = 0.58908768\n",
      "Iteration 182, loss = 0.58651370\n",
      "Iteration 183, loss = 0.58598328\n",
      "Iteration 184, loss = 0.58321062\n",
      "Iteration 185, loss = 0.58205670\n",
      "Iteration 186, loss = 0.57920224\n",
      "Iteration 187, loss = 0.57913689\n",
      "Iteration 188, loss = 0.57469558\n",
      "Iteration 189, loss = 0.57459069\n",
      "Iteration 190, loss = 0.57387121\n",
      "Iteration 191, loss = 0.57045892\n",
      "Iteration 192, loss = 0.56914385\n",
      "Iteration 193, loss = 0.56647293\n",
      "Iteration 194, loss = 0.56642178\n",
      "Iteration 195, loss = 0.56578291\n",
      "Iteration 196, loss = 0.56210442\n",
      "Iteration 197, loss = 0.56067240\n",
      "Iteration 198, loss = 0.55762361\n",
      "Iteration 199, loss = 0.55705285\n",
      "Iteration 200, loss = 0.55440521\n",
      "Iteration 201, loss = 0.55416080\n",
      "Iteration 202, loss = 0.55264376\n",
      "Iteration 203, loss = 0.55188023\n",
      "Iteration 204, loss = 0.55043817\n",
      "Iteration 205, loss = 0.54852845\n",
      "Iteration 206, loss = 0.54947989\n",
      "Iteration 207, loss = 0.54470524\n",
      "Iteration 208, loss = 0.54363216\n",
      "Iteration 209, loss = 0.54324686\n",
      "Iteration 210, loss = 0.54070102\n",
      "Iteration 211, loss = 0.53925943\n",
      "Iteration 212, loss = 0.53572850\n",
      "Iteration 213, loss = 0.53643053\n",
      "Iteration 214, loss = 0.53531248\n",
      "Iteration 215, loss = 0.53286582\n",
      "Iteration 216, loss = 0.53039098\n",
      "Iteration 217, loss = 0.53000615\n",
      "Iteration 218, loss = 0.52877470\n",
      "Iteration 219, loss = 0.52727850\n",
      "Iteration 220, loss = 0.52597158\n",
      "Iteration 221, loss = 0.52458944\n",
      "Iteration 222, loss = 0.52382846\n",
      "Iteration 223, loss = 0.52203384\n",
      "Iteration 224, loss = 0.51960265\n",
      "Iteration 225, loss = 0.51915510\n",
      "Iteration 226, loss = 0.51774900\n",
      "Iteration 227, loss = 0.51713641\n",
      "Iteration 228, loss = 0.51538879\n",
      "Iteration 229, loss = 0.51405401\n",
      "Iteration 230, loss = 0.51090857\n",
      "Iteration 231, loss = 0.50933974\n",
      "Iteration 232, loss = 0.50817663\n",
      "Iteration 233, loss = 0.50768296\n",
      "Iteration 234, loss = 0.50649943\n",
      "Iteration 235, loss = 0.50481059\n",
      "Iteration 236, loss = 0.50401769\n",
      "Iteration 237, loss = 0.50224053\n",
      "Iteration 238, loss = 0.50280361\n",
      "Iteration 239, loss = 0.50203983\n",
      "Iteration 240, loss = 0.49961308\n",
      "Iteration 241, loss = 0.49875570\n",
      "Iteration 242, loss = 0.49663795\n",
      "Iteration 243, loss = 0.49647884\n",
      "Iteration 244, loss = 0.49405736\n",
      "Iteration 245, loss = 0.49411414\n",
      "Iteration 246, loss = 0.49120301\n",
      "Iteration 247, loss = 0.49162707\n",
      "Iteration 248, loss = 0.49056074\n",
      "Iteration 249, loss = 0.48687923\n",
      "Iteration 250, loss = 0.48765741\n",
      "Iteration 251, loss = 0.48699303\n",
      "Iteration 252, loss = 0.48343539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.48323043\n",
      "Iteration 254, loss = 0.48205790\n",
      "Iteration 255, loss = 0.48144245\n",
      "Iteration 256, loss = 0.48064412\n",
      "Iteration 257, loss = 0.47887409\n",
      "Iteration 258, loss = 0.47836350\n",
      "Iteration 259, loss = 0.47660211\n",
      "Iteration 260, loss = 0.47503518\n",
      "Iteration 261, loss = 0.47466223\n",
      "Iteration 262, loss = 0.47414510\n",
      "Iteration 263, loss = 0.47354456\n",
      "Iteration 264, loss = 0.47033050\n",
      "Iteration 265, loss = 0.47184144\n",
      "Iteration 266, loss = 0.46841501\n",
      "Iteration 267, loss = 0.46784961\n",
      "Iteration 268, loss = 0.46650926\n",
      "Iteration 269, loss = 0.46481784\n",
      "Iteration 270, loss = 0.46439410\n",
      "Iteration 271, loss = 0.46371653\n",
      "Iteration 272, loss = 0.46222881\n",
      "Iteration 273, loss = 0.46092767\n",
      "Iteration 274, loss = 0.46068179\n",
      "Iteration 275, loss = 0.45951787\n",
      "Iteration 276, loss = 0.45850728\n",
      "Iteration 277, loss = 0.45678454\n",
      "Iteration 278, loss = 0.45499009\n",
      "Iteration 279, loss = 0.45622813\n",
      "Iteration 280, loss = 0.45689919\n",
      "Iteration 281, loss = 0.45274231\n",
      "Iteration 282, loss = 0.45036986\n",
      "Iteration 283, loss = 0.45174204\n",
      "Iteration 284, loss = 0.44975509\n",
      "Iteration 285, loss = 0.44785830\n",
      "Iteration 286, loss = 0.44672601\n",
      "Iteration 287, loss = 0.44748606\n",
      "Iteration 288, loss = 0.44610967\n",
      "Iteration 289, loss = 0.44484519\n",
      "Iteration 290, loss = 0.44435385\n",
      "Iteration 291, loss = 0.44453959\n",
      "Iteration 292, loss = 0.44260962\n",
      "Iteration 293, loss = 0.43960256\n",
      "Iteration 294, loss = 0.44092449\n",
      "Iteration 295, loss = 0.43825382\n",
      "Iteration 296, loss = 0.43870762\n",
      "Iteration 297, loss = 0.43826124\n",
      "Iteration 298, loss = 0.43584602\n",
      "Iteration 299, loss = 0.43498788\n",
      "Iteration 300, loss = 0.43387397\n",
      "Iteration 301, loss = 0.43502985\n",
      "Iteration 302, loss = 0.43296617\n",
      "Iteration 303, loss = 0.43213133\n",
      "Iteration 304, loss = 0.43136873\n",
      "Iteration 305, loss = 0.43097528\n",
      "Iteration 306, loss = 0.42986597\n",
      "Iteration 307, loss = 0.42689228\n",
      "Iteration 308, loss = 0.42536267\n",
      "Iteration 309, loss = 0.42597764\n",
      "Iteration 310, loss = 0.42499877\n",
      "Iteration 311, loss = 0.42471323\n",
      "Iteration 312, loss = 0.42442333\n",
      "Iteration 313, loss = 0.42167894\n",
      "Iteration 314, loss = 0.42209004\n",
      "Iteration 315, loss = 0.42321842\n",
      "Iteration 316, loss = 0.42017059\n",
      "Iteration 317, loss = 0.41778964\n",
      "Iteration 318, loss = 0.41879762\n",
      "Iteration 319, loss = 0.41772304\n",
      "Iteration 320, loss = 0.41428481\n",
      "Iteration 321, loss = 0.41743845\n",
      "Iteration 322, loss = 0.41508507\n",
      "Iteration 323, loss = 0.41279012\n",
      "Iteration 324, loss = 0.41272108\n",
      "Iteration 325, loss = 0.41371611\n",
      "Iteration 326, loss = 0.41078945\n",
      "Iteration 327, loss = 0.41105874\n",
      "Iteration 328, loss = 0.40858885\n",
      "Iteration 329, loss = 0.40903539\n",
      "Iteration 330, loss = 0.40563386\n",
      "Iteration 331, loss = 0.40516851\n",
      "Iteration 332, loss = 0.40494164\n",
      "Iteration 333, loss = 0.40604592\n",
      "Iteration 334, loss = 0.40336077\n",
      "Iteration 335, loss = 0.40369821\n",
      "Iteration 336, loss = 0.40393154\n",
      "Iteration 337, loss = 0.40181025\n",
      "Iteration 338, loss = 0.40082534\n",
      "Iteration 339, loss = 0.39972441\n",
      "Iteration 340, loss = 0.40114149\n",
      "Iteration 341, loss = 0.40000415\n",
      "Iteration 342, loss = 0.39880623\n",
      "Iteration 343, loss = 0.39789272\n",
      "Iteration 344, loss = 0.39828783\n",
      "Iteration 345, loss = 0.39492558\n",
      "Iteration 346, loss = 0.39521595\n",
      "Iteration 347, loss = 0.39395775\n",
      "Iteration 348, loss = 0.39489537\n",
      "Iteration 349, loss = 0.39308550\n",
      "Iteration 350, loss = 0.39196746\n",
      "Iteration 351, loss = 0.39047526\n",
      "Iteration 352, loss = 0.39231957\n",
      "Iteration 353, loss = 0.38906430\n",
      "Iteration 354, loss = 0.39032033\n",
      "Iteration 355, loss = 0.39015234\n",
      "Iteration 356, loss = 0.38773009\n",
      "Iteration 357, loss = 0.38787053\n",
      "Iteration 358, loss = 0.38795377\n",
      "Iteration 359, loss = 0.38708871\n",
      "Iteration 360, loss = 0.38505048\n",
      "Iteration 361, loss = 0.38537221\n",
      "Iteration 362, loss = 0.38328759\n",
      "Iteration 363, loss = 0.38223869\n",
      "Iteration 364, loss = 0.38070101\n",
      "Iteration 365, loss = 0.38088819\n",
      "Iteration 366, loss = 0.37862624\n",
      "Iteration 367, loss = 0.37863544\n",
      "Iteration 368, loss = 0.37916449\n",
      "Iteration 369, loss = 0.37763287\n",
      "Iteration 370, loss = 0.37846200\n",
      "Iteration 371, loss = 0.37595896\n",
      "Iteration 372, loss = 0.37681049\n",
      "Iteration 373, loss = 0.37608135\n",
      "Iteration 374, loss = 0.37695849\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 375, loss = 0.29502638\n",
      "Iteration 376, loss = 0.28499207\n",
      "Iteration 377, loss = 0.28374079\n",
      "Iteration 378, loss = 0.28282098\n",
      "Iteration 379, loss = 0.28255725\n",
      "Iteration 380, loss = 0.28174265\n",
      "Iteration 381, loss = 0.28152540\n",
      "Iteration 382, loss = 0.28108619\n",
      "Iteration 383, loss = 0.28080776\n",
      "Iteration 384, loss = 0.28022094\n",
      "Iteration 385, loss = 0.27997752\n",
      "Iteration 386, loss = 0.28009121\n",
      "Iteration 387, loss = 0.27972372\n",
      "Iteration 388, loss = 0.27962775\n",
      "Iteration 389, loss = 0.27894414\n",
      "Iteration 390, loss = 0.27906999\n",
      "Iteration 391, loss = 0.27883681\n",
      "Iteration 392, loss = 0.27819516\n",
      "Iteration 393, loss = 0.27818134\n",
      "Iteration 394, loss = 0.27837085\n",
      "Iteration 395, loss = 0.27780216\n",
      "Iteration 396, loss = 0.27796052\n",
      "Iteration 397, loss = 0.27793846\n",
      "Iteration 398, loss = 0.27736377\n",
      "Iteration 399, loss = 0.27747336\n",
      "Iteration 400, loss = 0.27742203\n",
      "Iteration 401, loss = 0.27655525\n",
      "Iteration 402, loss = 0.27675255\n",
      "Iteration 403, loss = 0.27656263\n",
      "Iteration 404, loss = 0.27609080\n",
      "Iteration 405, loss = 0.27618359\n",
      "Iteration 406, loss = 0.27650171\n",
      "Iteration 407, loss = 0.27604592\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 408, loss = 0.26293108\n",
      "Iteration 409, loss = 0.26110363\n",
      "Iteration 410, loss = 0.26099172\n",
      "Iteration 411, loss = 0.26095337\n",
      "Iteration 412, loss = 0.26094575\n",
      "Iteration 413, loss = 0.26088815\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 414, loss = 0.25803877\n",
      "Iteration 415, loss = 0.25764806\n",
      "Iteration 416, loss = 0.25763617\n",
      "Iteration 417, loss = 0.25759857\n",
      "Iteration 418, loss = 0.25755795\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 419, loss = 0.25693959\n",
      "Iteration 420, loss = 0.25685380\n",
      "Iteration 421, loss = 0.25683419\n",
      "Iteration 422, loss = 0.25682756\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 423, loss = 0.25665465\n",
      "Iteration 424, loss = 0.25665292\n",
      "Iteration 425, loss = 0.25665142\n",
      "Iteration 426, loss = 0.25665060\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 427, loss = 0.25661311\n",
      "Iteration 428, loss = 0.25661298\n",
      "Iteration 429, loss = 0.25661261\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training set score: 0.941369\n",
      "Test set score: 0.612563\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.01, learning_rate='adaptive',\n",
    "                    warm_start=True)\n",
    "\n",
    "mlp.fit(X_train, Y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, Y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('coefs_smvn_train_test', mlp.coefs_)\n",
    "np.save('intercepts_smvn_train_test', mlp.intercepts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = X_test\n",
    "Y_test2 = Y_test\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# mvn of test\n",
    "for speaker_id, feature_list in test.items():\n",
    "    speaker_id = idx[speaker_id]\n",
    "    for features in feature_list:\n",
    "        features = preprocessing.scale(features.T).T\n",
    "        frames = concat(features)\n",
    "        for frame in frames:\n",
    "            X_test.append(frame)\n",
    "            Y_test.append(speaker_id)\n",
    "            \n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.037572\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set score: %f\" % mlp.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
